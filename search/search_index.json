{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"resource-tracker","text":"<p>A lightweight, zero-dependency Python package for monitoring resource usage across processes and at the system level. Designed with batch jobs in mind (like Python or R scripts, or Metaflow steps), it provides simple tools to track CPU, memory, GPU, network, and disk utilization with minimal setup -- e.g. using a step decorator in Metaflow to automatically track resource usage and generate a card with data visualizations on historical resource usage and cloud server recommendations for future runs.</p>"},{"location":"#installation","title":"Installation","text":"<p>You can install the stable version of the package from PyPI: </p> <pre><code>pip install resource-tracker\n</code></pre> <p>Development version can be installed directly from the git repository:</p> <pre><code>pip install git+https://github.com/sparecores/resource-tracker.git\n</code></pre> <p>Note that depending on your operating system, you might need to also install <code>psutil</code> (e.g. on MacOS and Windows). For more details, see the OS support section.</p>"},{"location":"#integrations","title":"Integrations","text":"<p>The <code>resource-tracker</code> Python package is designed to be used in a variety of ways, even outside of Python. Find more details about how to use it directly from Python, R, or via our framework integrations, such as Metaflow, in the integrations section of the documentation.</p>"},{"location":"#operating-system-support","title":"Operating System Support","text":"<p>The package was originally created to work on Linux systems (as the most commonly used operating system on cloud servers) using <code>procfs</code> directly and without requiring any further Python dependencies, but to support other operating systems as well, now it can also use <code>psutil</code> when available.</p> <p>To make sure the resource tracker works on non-Linux systems, install via:</p> <pre><code>pip install resource-tracker[psutil]\n</code></pre> <p>Minor inconsistencies between operating systems are expected, e.g. using PSS (Proportional Set Size) instead of RSS (Resident Set Size) as the process-level memory usage metric on Linux, as it is evenly divides the shared memory usage between the processes using it, making it more representative of the memory usage of the monitored applications. Mac OS X and Windows use USS (Unique Set Size) instead.</p> <p>CI/CD is set up to run tests on the below operating systems:</p> <ul> <li>Ubuntu latest LTS (24.04)</li> <li>MacOS latest (13)</li> <li>Windows latest (Windows Server 2022)</li> </ul> <p></p>"},{"location":"#python-version-support","title":"Python Version Support","text":"<p>The package supports Python 3.9 and above.</p> <p>CI/CD is set up to run tests on the below Python versions on Ubuntu latest LTS, Windows Server 2022 and MacOS latest:</p> <ul> <li>3.9</li> <li>3.10</li> <li>3.11</li> <li>3.12</li> <li>3.13</li> </ul> <p></p>"},{"location":"#performance","title":"Performance","text":"<p>The performance of the <code>procfs</code> and the <code>psutil</code> implementations is similar, see e.g. benchmark.py for a comparison of the two implementations when looking at process-level stats:</p> <pre><code>PSUtil implementation: 0.082130s avg (min: 0.067612s, max: 0.114606s)\nProcFS implementation: 0.084533s avg (min: 0.081533s, max: 0.111782s)\nSpeedup factor: 0.97x (psutil faster)\n</code></pre> <p>On a heavy application with many descendants (such as Google Chrome with hundreds of processes and open tabs):</p> <pre><code>PSUtil implementation: 0.201849s avg (min: 0.193392s, max: 0.214061s)\nProcFS implementation: 0.182557s avg (min: 0.174610s, max: 0.192760s)\nSpeedup factor: 1.11x (procfs faster)\n</code></pre> <p>The system-level stats are much cheaper to collect, and there is no effective difference in performance between the two implementations.</p> <p>Why have both implementations then? The <code>psutil</code> implementation works on all operating systems at the cost of the extra dependency, while the <code>procfs</code> implementation works without any additional dependencies, but only on Linux. This latter can be useful when deploying cloud applications in limited environments without easy control over the dependencies (e.g. Metaflow step decorator without explicit <code>@pypi</code> config).</p>"},{"location":"#references","title":"References","text":"<ul> <li>PyPI: https://pypi.org/project/resource-tracker</li> <li>Documentation: https://sparecores.github.io/resource-tracker</li> <li>Source code: https://github.com/SpareCores/resource-tracker</li> <li>Project roadmap and feedback form: https://sparecores.com/feedback/metaflow-resource-tracker</li> </ul>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#v042-august-8-2025","title":"v0.4.2 (August 8, 2025)","text":"<ul> <li>Add <code>cleanup</code> method to <code>ResourceTracker</code> to cleanup temp files and background processes.</li> </ul>"},{"location":"CHANGELOG/#v041-august-8-2025","title":"v0.4.1 (August 8, 2025)","text":"<p>Introduce native R support. Example usage:</p> <pre><code>library(resource_tracker)\n\ntracker &lt;- ResourceTracker$new()\ntracker$wait_for_samples(1)\ntracker$stats()\ntracker$recommend_resources()\ntracker$recommend_server()\ntracker$report()$browse()\n</code></pre> <p>Find more details in the R integration docs.</p> <p>Additional changes: - Split documentation and extend into multiple pages with more details on the integrations.</p>"},{"location":"CHANGELOG/#v040-august-6-2025","title":"v0.4.0 (August 6, 2025)","text":"<p>Refactoring and cleanup release with the main focus on extracting the reporting features from the Metaflow extension and support it in standalone use as well.</p> <p>Main changes:</p> <ul> <li>Much better support for standalone use of <code>ResourceTracker</code> via the <code>stats</code>, <code>recommend_resources</code>,  <code>recommend_server</code> and <code>report</code> methods</li> </ul> <p>More details:</p> <ul> <li>Rename <code>PidTracker</code> to <code>ProcessTracker</code> to better reflect its purpose. <code>PidTracker</code> is still available as an alias that is to be deprecated in the future.</li> <li>Rename <code>get_pid_stats</code> to <code>get_process_stats</code> and related references in <code>ResourceTracker</code> and <code>ProcessTracker</code>.</li> <li>Rename the <code>pid_tracker</code> and <code>system_tracker</code> properties of <code>ResourceTracker</code> to <code>process_metrics</code> and <code>system_metrics</code> respectively. All related references were also updated, e.g. in the Metaflow extension and docs.</li> <li>Rename process-related helpers in the ProcFS implementation from <code>pid</code> prefix to <code>process</code> prefix.</li> <li>Fix <code>SystemTracker</code> and <code>ProcessTracker</code> to not print dummy stats on start when header is disabled</li> <li>Add optional <code>start_time</code> parameter to <code>SystemTracker</code> and <code>ProcessTracker</code></li> <li>Update <code>ResourceTracker</code> to start tracking at the nearest interval in the future, syncing <code>SystemTracker</code> and <code>ProcessTracker</code></li> <li>Fix <code>SystemTracker</code> and <code>ProcessTracker</code> to not drift by a few nanoseconds in every interval</li> <li>Move cloud and server discovery along with the server allocation check to the <code>ResourceTracker</code> class from the Metaflow-specific decorators</li> <li>Extract serialization and deserialization of <code>ResourceTracker</code> from the Metaflow extension into the <code>ResourceTracker</code> class with <code>snapshot</code> and <code>dump(s)</code>/<code>load(s)</code> methods</li> <li>Round timestamp and user/system time to reasonable (6/4) decimal places</li> <li>Rework internal data structure of <code>TinyDataFrame</code> to use a list of lists instead of a dictionary of lists to support more efficient slicing and column renaming</li> <li>Add <code>get_combined_metrics</code> method to <code>ResourceTracker</code> to combine <code>process_metrics</code> and <code>system_metrics</code> into a single data frame, optionally with all metrics converted to bytes, and columns renamed to use human-friendly names</li> <li>Add <code>stats</code> method to <code>TinyDataFrame</code> to compute on-demand statistics on columns</li> <li>Add <code>stats</code> method to <code>ResourceTracker</code> to compute statistics on the combined metrics</li> <li>Add minimal support for Handlebars-like templates in the <code>render_template</code> function</li> <li>Add <code>report</code> method to <code>ResourceTracker</code> to generate a report in HTML format, and use that from the Metaflow extension</li> <li>Add <code>recommend_resources</code> and <code>recommend_server</code> methods to <code>ResourceTracker</code></li> <li>Windows-specific reliability improvements</li> </ul> <p>Related breaking changes:</p> <ul> <li>Historical data collected before v0.4.0 is not compatible with the new <code>ResourceTracker</code> class, and will be discarded</li> <li><code>TinyDataFrame</code> is no longer made available from <code>resource_tracker</code> directly, but from the <code>resource_tracker.tiny_data_frame</code> submodule</li> </ul>"},{"location":"CHANGELOG/#v031-may-30-2025","title":"v0.3.1 (May 30, 2025)","text":"<ul> <li>Generate card for failed step</li> <li>Note failed step status in card</li> <li>Standardize timestamp format and timezone</li> </ul>"},{"location":"CHANGELOG/#v030-march-27-2025","title":"v0.3.0 (March 27, 2025)","text":"<ul> <li>Extract background process management and related complexities from the <code>track_resources</code> decorator into the <code>ResourceTracker</code> class to track resource usage of a process and/or the system in a non-blocking way</li> <li>Add unit tests for the <code>ResourceTracker</code> class, including checks for deadlocks and partially started trackers</li> <li>Keep test HTML card as GHA artifacts for manual inspection</li> <li>Improve documentation</li> </ul>"},{"location":"CHANGELOG/#v021-march-21-2025","title":"v0.2.1 (March 21, 2025)","text":"<ul> <li>Fix don't always round up CPU/GPU recommendations</li> <li>Improve error message on missing historical data</li> <li>Improve documentation</li> </ul>"},{"location":"CHANGELOG/#v020-march-21-2025","title":"v0.2.0 (March 21, 2025)","text":"<p>Relatively major package rewrite to support alternative tracker implementations (other than directly reading from <code>/proc</code>). No breaking changes in the public API on Linux.</p> <ul> <li>Add tracker implementation using <code>psutil</code> to support MacOS and Windows</li> <li>Fix data issues with the <code>/proc</code> implementation after validating with the <code>psutil</code> version (e.g. number of processes reported)</li> <li>Refactor code for better maintainability</li> <li>Add additional unit tests:<ul> <li>Tracker implementation using <code>procfs</code></li> <li>Tracker implementation using <code>psutil</code></li> <li>Consistency between tracker implementations</li> <li>Metaflow decorators</li> </ul> </li> <li>Extend CI/CD pipeline:<ul> <li>Test on Linux, MacOS, and Windows</li> <li>Test multiple Python versions (3.9, 3.10, 3.11, 3.12, 3.13)</li> </ul> </li> <li>Improve documentation</li> </ul>"},{"location":"CHANGELOG/#v012-march-18-2025","title":"v0.1.2 (March 18, 2025)","text":"<ul> <li>Add experimental psutil support</li> <li>Add server info card for operating system</li> </ul>"},{"location":"CHANGELOG/#v011-march-17-2025","title":"v0.1.1 (March 17, 2025)","text":"<ul> <li>Fix rounding down recommended vCPUs with &lt;0.5 load</li> <li>Add info popups with more details and disclaimers for recommendations</li> <li>Add detection for shared server environments</li> <li>Add potential cost savings card</li> <li>Improve documentation</li> </ul>"},{"location":"CHANGELOG/#v010-march-12-2025","title":"v0.1.0 (March 12, 2025)","text":"<p>Initial PyPI release of <code>resource-tracker</code> with the following features:</p> <ul> <li>Detect if the system is running on a cloud provider, and if so, detect the provider, region, and instance type</li> <li>Detect main server hardware (CPU count, memory amount, disk space, GPU count and VRAM amount)</li> <li>Track system-wide resource usage:<ul> <li>Process count</li> <li>CPU usage (user + system time, relative vCPU percentage)</li> <li>Memory usage (total, free, used, buffers, cached, active anon, inactive anon pages)</li> <li>Disk I/O (read and write bytes)</li> <li>Disk space usage (total, used, free)</li> <li>Network I/O (receive and transmit bytes)</li> <li>GPU and VRAM usage (using <code>nvidia-smi</code>)</li> </ul> </li> <li>Track resource usage of a process and its descendant processes:<ul> <li>Descendant process count</li> <li>CPU usage (user + system time, relative vCPU percentage)</li> <li>Memory usage (based on proportional set sizes)</li> <li>Disk I/O (read and write bytes)</li> <li>GPU and VRAM usage (using <code>nvidia-smi pmon</code>)</li> </ul> </li> <li>Add Metaflow plugin for tracking resource usage of a step:<ul> <li>Track process and system resource usage for the duration of the step</li> <li>Generate a card with the resource usage data</li> <li>Suggest <code>@resources</code> decorator for future runs</li> <li>Find cheapest cloud instance type for a step</li> </ul> </li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>The <code>resource-tracker</code> Python package can be used in a variety of ways, even outside of Python.</p>"},{"location":"integrations/#standalone","title":"Standalone","text":"<p>After installing the zero-dependency <code>resource-tracker</code> Python package, you can start using it right away by initializing a <code>ResourceTracker</code> object and later calling its methods to summarize the resource usage.</p> <p>   \ud83d\udc49 Click here to learn more! </p>"},{"location":"integrations/#metaflow","title":"Metaflow","text":"<p>The <code>resource-tracker</code> Python package is designed to be used in a Metaflow flow in a seamless way via the <code>resource_tracker</code> Metaflow extension and <code>@track_resources</code> step decorator. With a single line of code, you can track the resource usage of your Metaflow steps and get recommendations on the best server to run your steps on, including automated HTML cards attached to your steps.</p> <p>   \ud83d\udc49 Click here to learn more! </p>"},{"location":"integrations/#r","title":"R","text":"<p>The <code>resource.tracker</code> R package provides a convenient R6 wrapper for the <code>resource-tracker</code> Python package. To get started, install the package and initialize a <code>ResourceTracker</code> object, then call its methods to summarize the resource usage.</p> <p>   \ud83d\udc49 Click here to learn more! </p>"},{"location":"integrations/metaflow/","title":"Resource Tracker for Metaflow","text":"<p>The <code>resource-tracker</code> Python package comes with a Metaflow extension for tracking resource usage of Metaflow steps, including the visualization of the collected data in a card with recommended <code>@resources</code> and cheapest cloud server type for future runs, along with basic cost estimates.</p> <p>To get started, import the <code>track_resources</code> decorator from <code>metaflow</code> (note that no need to import the <code>resource_tracker</code> at all) and use it to decorate your Metaflow steps:</p> <pre><code>from metaflow import Flow, FlowSpec, step, track_resources\n\nclass ResourceTrackingFlow(FlowSpec):\n    @step\n    def start(self):\n        print(\"Starting step\")\n        self.next(self.my_sleeping_data)\n\n    @track_resources\n    @step\n    def my_sleeping_data(self):\n        data = bytearray(500 * 1024 * 1024)  # 500MB\n        sleep(3)\n        self.next(self.end)\n\n    @step\n    def end(self):\n        print(\"Step finished\")\n        pass\n\nif __name__ == \"__main__\":\n    ResourceTrackingFlow()\n</code></pre> <p>Note that if you are using <code>--environment=pypi</code> or other means of custom package management on a non-Linux system, you might need to install <code>psutil</code> there, as the zero-dependency <code>procfs</code> implementation doesn't work on Windows or MacOS.</p>"},{"location":"integrations/metaflow/#html-card","title":"HTML Card","text":"<p>Example output of an auto-generated Metaflow card:</p> <p></p> <p>For a live view of the HTML card, see this Metaflow card.</p>"},{"location":"integrations/metaflow/#artifacts","title":"Artifacts","text":"<p>Example data collected and then stored as an artifact of the step:</p> <pre><code>from metaflow import Flow\nfrom rich import print as pp\n\nartifact = Flow(\"ResourceTrackingFlow\").latest_run.data.resource_tracker_data\npp(artifact)\n# {\n#     'process_metrics': TinyDataFrame with 9 rows and 12 columns. First row as a dict: {'timestamp': 1741732803.3076203, 'pid': \n# 777691.0, 'children': 3.0, 'utime': 95.0, 'stime': 13.0, 'cpu_usage': 1.0796, 'memory': 563273.0, 'read_bytes': 52260.0, \n# 'write_bytes': 0.0, 'gpu_usage': 0.0, 'gpu_vram': 0.0, 'gpu_utilized': 0.0},\n#     'system_metrics': TinyDataFrame with 9 rows and 21 columns. First row as a dict: {'timestamp': 1741732803.2471318, \n# 'processes': 777773.0, 'utime': 225.0, 'stime': 53.0, 'cpu_usage': 2.7797, 'memory_free': 38480700.0, 'memory_used': \n# 24338580.0, 'memory_buffers': 4792.0, 'memory_cached': 2727720.0, 'memory_active': 15931396.0, 'memory_inactive': \n# 0.0, 'disk_read_bytes': 380928.0, 'disk_write_bytes': 10088448.0, 'disk_space_total_gb': 5635.25, 'disk_space_used_gb': \n# 3405.11, 'disk_space_free_gb': 2230.14, 'net_recv_bytes': 8066.0, 'net_sent_bytes': 8593.0, 'gpu_usage': 0.29, 'gpu_vram': \n# 998.0, 'gpu_utilized': 1.0},\n#     'cloud_info': {\n#         'vendor': 'unknown',\n#         'instance_type': 'unknown',\n#         'region': 'unknown',\n#         'discovery_time': 1.0617177486419678\n#     },\n#     'server_info': {\n#         'vcpus': 12,\n#         'memory_mb': 64015.42,\n#         'gpu_count': 1,\n#         'gpu_names': ['Quadro T1000'],\n#         'gpu_memory_mb': 4096.0\n#     },\n#     'stats': {\n#         'cpu_usage': {'mean': 1.42, 'max': 6.11},\n#         'memory_usage': {'mean': 342509.0, 'max': 591621.0},\n#         'gpu_usage': {'mean': 0.0, 'max': 0.0},\n#         'gpu_vram': {'mean': 0.0, 'max': 0.0},\n#         'gpu_utilized': {'mean': 0.0, 'max': 0.0},\n#         'disk_usage': {'max': 3405.11},\n#         'traffic': {'inbound': 77383.0, 'outbound': 58481.0},\n#         'duration': 9.89\n#     },\n#     'historical_stats': {\n#         'available': True,\n#         'runs_analyzed': 5,\n#         'avg_cpu_mean': 1.52,\n#         'max_memory_max': 597372.0,\n#         'avg_gpu_mean': 0.0,\n#         'max_vram_max': 0.0,\n#         'max_gpu_count': 0.0,\n#         'avg_duration': 10.2\n#     }\n# }\n</code></pre> <p>Find more examples in the examples directory, including multiple Metaflow flows with different resource usage patterns, e.g. GPU jobs as well.</p>"},{"location":"integrations/r/","title":"Resource Tracker for R","text":"<p>This R package provides raw access to the <code>resource-tracker</code> Python module via the <code>reticulate</code> R package along with a convenience <code>R6</code> wrapper for the <code>ResourceTracker</code> class with the most common methods, such as starting and stopping the tracker, collecting samples, and generating reports.</p>"},{"location":"integrations/r/#installation","title":"Installation","text":"<p>This package requires Python to be installed along with the <code>resource-tracker</code> Python module, which can be installed from R via:</p> <pre><code>reticulate::py_install('resource-tracker')\n</code></pre> <p>Note that on MacOS and Windows, you also need to install <code>psutil</code> that's used under the hood to collect metrics. On Linux, it's optional, and can rely on the <code>/proc</code> filesystem when using a modern kernel. For more details, see the installation instructions for the Python package.</p> <p>Once the Python dependencies are resolved, you can either install the R package from CRAN:</p> <pre><code>install.packages('resource.tracker')\n</code></pre> <p>Or the most recent (development version) from GitHub:</p> <pre><code>remotes::install_github('SpareCores/resource-tracker', subdir = 'R')\n</code></pre>"},{"location":"integrations/r/#getting-started","title":"Getting started","text":"<p>Load the package and start the resource tracker:</p> <pre><code>library(resource.tracker)\ntracker &lt;- ResourceTracker$new()\n</code></pre> <p>This now runs in the background, not blocking your R session.</p> <p>Let's simulate some work for some seconds with the below, inefficient rolling mean implementation:</p> <pre><code>numbers &lt;- 1:1e6\nwindow &lt;- 3\nrollavg &lt;- sapply(\n  seq_len(length(numbers) - window + 1),\n  function(i) mean(numbers[i:(i + window - 1)]))\n</code></pre> <p>It's time to check on the aggregated statistics of the resources used:</p> <pre><code>tracker$stats()\n# List of 9\n#  $ process_cpu_usage        :List of 2\n#   ..$ mean: num 1.12\n#   ..$ max : num 1.14\n#  $ process_memory           :List of 2\n#   ..$ mean: num 728783\n#   ..$ max : num 798130\n#  $ process_gpu_usage        :List of 2\n#   ..$ mean: num 0\n#   ..$ max : num 0\n#  $ process_gpu_vram         :List of 2\n#   ..$ mean: num 0\n#   ..$ max : num 0\n#  $ process_gpu_utilized     :List of 2\n#   ..$ mean: num 0\n#   ..$ max : num 0\n#  $ system_disk_space_used_gb:List of 1\n#   ..$ max: num 2560\n#  $ system_net_recv_bytes    :List of 1\n#   ..$ sum: num 1840217\n#  $ system_net_sent_bytes    :List of 1\n#   ..$ sum: num 1843725\n#  $ timestamp                :List of 1\n#   ..$ duration: num 7\n</code></pre> <p>Or get recommendations on hardware requirements for the next run:</p> <pre><code>tracker$recommend_resources()\n# List of 4\n#  $ cpu   : int 1\n#  $ memory: int 1024\n#  $ gpu   : int 0\n#  $ vram  : int 0\n\ntracker$recommend_server()\n# List of 50\n#  $ vendor_id            : chr \"upcloud\"\n#  $ server_id            : chr \"DEV-1xCPU-1GB-10GB\"\n#  $ description          : chr \"Developer 1 vCPUs, 1 GB RAM\"\n#  $ family               : chr \"Developer\"\n#  $ vcpus                : int 1\n#  $ hypervisor           : chr \"KVM\"\n#  $ cpu_allocation       : chr \"Shared\"\n#  $ cpu_cores            : int 1\n#  $ cpu_architecture     : chr \"x86_64\"\n#  $ cpu_manufacturer     : chr \"AMD\"\n#  $ cpu_family           : chr \"EPYC\"\n#  $ cpu_model            : chr \"7542\"\n#  $ cpu_l1_cache         : int 131072\n#  $ cpu_l2_cache         : int 524288\n#  $ cpu_l3_cache         : int 16777216\n#  $ cpu_flags            : chr [1:88] \"fpu\" \"vme\" \"de\" \"pse\" ...\n#  $ memory_amount        : int 1024\n#  $ storage_size         : int 10\n#  $ inbound_traffic      : num 0\n#  $ outbound_traffic     : num 1024\n#  $ ipv4                 : int 1\n#  $ price                : num 0.0052\n#  ...\n</code></pre> <p>Or look at the collected samples:</p> <pre><code>tracker$system_metrics\n# 'data.frame':   7 obs. of  21 variables:\n#  $ timestamp          : POSIXct, format: \"2025-08-08 00:02:28\" \"2025-08-08 00:02:29\" ...\n#  $ processes          : num  697 696 694 693 694 694 693\n#  $ utime              : num  3.01 2.25 2.3 2.15 2.9 1.44 1.36\n#  $ stime              : num  0.51 0.57 0.55 0.24 0.89 0.83 0.36\n#  $ cpu_usage          : num  3.52 2.82 2.85 2.39 3.79 ...\n#  $ memory_free        : num  8279604 8254872 8298804 8247688 8200296 ...\n#  $ memory_used        : num  23920048 23946512 23901264 23951016 23993224 ...\n#  $ memory_buffers     : num  2992 2992 2992 2992 2992 ...\n#  $ memory_cached      : num  33344236 33342504 33343820 33345184 33350368 ...\n#  $ memory_active      : num  47841676 47860856 47855900 47877072 47989240 ...\n#  $ memory_inactive    : num  106028 106028 106028 106028 106028 ...\n#  $ disk_read_bytes    : num  1138688 819200 24576 126976 36352000 ...\n#  $ disk_write_bytes   : num  942080 0 8585216 61440 20480 ...\n#  $ disk_space_total_gb: num  3700 3700 3700 3700 3700 ...\n#  $ disk_space_used_gb : num  2560 2560 2560 2560 2560 ...\n#  $ disk_space_free_gb : num  1140 1140 1140 1140 1140 ...\n#  $ net_recv_bytes     : num  710628 1204 7685 4553 1094516 ...\n#  $ net_sent_bytes     : num  710504 1463 7042 8261 1085653 ...\n#  $ gpu_usage          : num  0.06 0.13 0.11 0.05 0.03 0.02 0.02\n#  $ gpu_vram           : num  571 562 558 558 558 558 558\n#  $ gpu_utilized       : num  1 1 1 1 1 1 1\n\ntracker$process_metrics\n# 'data.frame':   7 obs. of  12 variables:\n#  $ timestamp   : POSIXct, format: \"2025-08-08 00:02:28\" \"2025-08-08 00:02:29\" ...\n#  $ pid         : num  941247 941247 941247 941247 941247 ...\n#  $ children    : num  4 4 4 4 4 4 4\n#  $ utime       : num  1.01 1.02 1.01 1 0.99 1.04 1.01\n#  $ stime       : num  0.1 0.12 0.07 0.13 0.14 0.09 0.09\n#  $ cpu_usage   : num  1.11 1.14 1.08 1.13 1.13 1.13 1.1\n#  $ memory      : num  646973 668091 680918 724281 798130 ...\n#  $ read_bytes  : num  0 0 0 0 0 0 0\n#  $ write_bytes : num  0 0 0 0 0 0 0\n#  $ gpu_usage   : num  0 0 0 0 0 0 0\n#  $ gpu_vram    : num  0 0 0 0 0 0 0\n#  $ gpu_utilized: num  0 0 0 0 0 0 0\n\ntracker$get_combined_metrics()\n# 'data.frame':   7 obs. of  32 variables:\n#  $ timestamp                 : POSIXct, format: \"2025-08-08 00:02:28\" \"2025-08-08 00:02:29\" ...\n#  $ system_processes          : num  697 696 694 693 694 694 693\n#  $ system_utime              : num  3.01 2.25 2.3 2.15 2.9 1.44 1.36\n#  $ system_stime              : num  0.51 0.57 0.55 0.24 0.89 0.83 0.36\n#  $ system_cpu_usage          : num  3.52 2.82 2.85 2.39 3.79 ...\n#  $ system_memory_free        : num  8279604 8254872 8298804 8247688 8200296 ...\n#  $ system_memory_used        : num  23920048 23946512 23901264 23951016 23993224 ...\n#  $ system_memory_buffers     : num  2992 2992 2992 2992 2992 ...\n#  $ system_memory_cached      : num  33344236 33342504 33343820 33345184 33350368 ...\n#  $ system_memory_active      : num  47841676 47860856 47855900 47877072 47989240 ...\n#  $ system_memory_inactive    : num  106028 106028 106028 106028 106028 ...\n#  $ system_disk_read_bytes    : num  1138688 819200 24576 126976 36352000 ...\n#  $ system_disk_write_bytes   : num  942080 0 8585216 61440 20480 ...\n#  $ system_disk_space_total_gb: num  3700 3700 3700 3700 3700 ...\n#  $ system_disk_space_used_gb : num  2560 2560 2560 2560 2560 ...\n#  $ system_disk_space_free_gb : num  1140 1140 1140 1140 1140 ...\n#  $ system_net_recv_bytes     : num  710628 1204 7685 4553 1094516 ...\n#  $ system_net_sent_bytes     : num  710504 1463 7042 8261 1085653 ...\n#  $ system_gpu_usage          : num  0.06 0.13 0.11 0.05 0.03 0.02 0.02\n#  $ system_gpu_vram           : num  571 562 558 558 558 558 558\n#  $ system_gpu_utilized       : num  1 1 1 1 1 1 1\n#  $ process_pid               : num  941247 941247 941247 941247 941247 ...\n#  $ process_children          : num  4 4 4 4 4 4 4\n#  $ process_utime             : num  1.01 1.02 1.01 1 0.99 1.04 1.01\n#  $ process_stime             : num  0.1 0.12 0.07 0.13 0.14 0.09 0.09\n#  $ process_cpu_usage         : num  1.11 1.14 1.08 1.13 1.13 1.13 1.1\n#  $ process_memory            : num  646973 668091 680918 724281 798130 ...\n#  $ process_read_bytes        : num  0 0 0 0 0 0 0\n#  $ process_write_bytes       : num  0 0 0 0 0 0 0\n#  $ process_gpu_usage         : num  0 0 0 0 0 0 0\n#  $ process_gpu_vram          : num  0 0 0 0 0 0 0\n#  $ process_gpu_utilized      : num  0 0 0 0 0 0 0\n</code></pre> <p>Note that the <code>system_metrics</code> and <code>process_metrics</code> are properties of the <code>ResourceTracker</code> object, while <code>get_combined_metrics</code> is a method that takes optional parameters (e.g. to render more human-friendly names or return all values in bytes) and returns a <code>data.frame</code>.</p> <p>And finally, let's generate a report and open it in your browser:</p> <pre><code>report &lt;- tracker$report()\nreport$browse()\n</code></pre> <p>For an example HTML report, see this Metaflow card.</p>"},{"location":"integrations/r/#advanced-usage","title":"Advanced usage","text":"<p>Not all features are exposed via the <code>ResourceTracker</code> class. For example, you could snapshot a <code>ResourceTracker</code> object to a string or compressed file and later restore it. To access these features, open a GitHub issue or you could use the Python API directly via the <code>resource.tracker::resource_tracker</code> Python module reference.</p>"},{"location":"integrations/standalone/","title":"Resource Tracker in Python","text":"<p>After installing the zero-dependency <code>resource-tracker</code> Python package, you can start using it right away by initializing a <code>ResourceTracker</code> object and later calling its methods to summarize the resource usage.</p> <p>Quick example featuring the most common use cases:</p> <pre><code>from resource_tracker import ResourceTracker\n\ntracker = ResourceTracker()\n# your compute-heavy code\ntracker.stop()\n\n# your analytics code utilizing the collected data\ntracker.process_metrics\ntracker.system_metrics\ntracker.get_combined_metrics()\n\n# or more conveniently get combined statistics\ntracker.stats()\n\n# get recommendations for resource allocation and cloud server type\ntracker.recommend_resources()\ntracker.recommend_server()\n\n# generate a HTML report on resource usage and recommendations\nreport = tracker.report()\nreport.save(\"report.html\")\nreport.browse()\n</code></pre>"},{"location":"integrations/standalone/#background-details","title":"Background Details","text":"<p>The <code>ResourceTracker</code> class runs trackers in the background. The underlying <code>ProcessTracker</code> and <code>SystemTracker</code> classes log resource usage to a temporary file, both using either <code>procfs</code> or <code>psutil</code> under the hood -- depending on which is available, with a preference for <code>psutil</code> when both are present.</p> <p>The <code>ResourceTracker</code> instance gives you access to the collected data in real-time, or after stopping the trackers via the <code>process_metrics</code> and <code>system_metrics</code> properties, or the <code>get_combined_metrics</code> method. Each of them is a <code>TinyDataFrame</code> object, which is essentially a dictionary of lists, with additional methods for e.g. printing and saving to a CSV file. See the standalone.py for a more detailed actual usage example.</p> <p>It's possible to track only the system-wide or process resource usage by the related init parameters of <code>ResourceTracker</code>, just like controlling the sampling interval, or how to start (e.g. spawn or fork) the subprocesses of the trackers.</p> <p>For even more control, you can use the underlying <code>ProcessTracker</code> and <code>SystemTracker</code> classes directly, which are not starting and handling new processes in the background, but simply log resource usage to the standard output or a file. For example, to track only the system-wide resource usage, you can use the <code>SystemTracker</code> class:</p> <pre><code>from resource_tracker import SystemTracker\ntracker = SystemTracker()\n</code></pre> <p><code>SystemTracker</code> tracks system-wide resource usage, including CPU, memory, GPU, network traffic, disk I/O and space usage every 1 second, and write CSV to the standard output stream by default. Example output:</p> <pre><code># \"timestamp\",\"processes\",\"utime\",\"stime\",\"cpu_usage\",\"memory_free\",\"memory_used\",\"memory_buffers\",\"memory_cached\",\"memory_active\",\"memory_inactive\",\"disk_read_bytes\",\"disk_write_bytes\",\"disk_space_total_gb\",\"disk_space_used_gb\",\"disk_space_free_gb\",\"net_recv_bytes\",\"net_sent_bytes\",\"gpu_usage\",\"gpu_vram\",\"gpu_utilized\"\n# 1741785685.6762981,1147955,40,31,0.7098,37828072,26322980,16,1400724,13080320,1009284,86016,401408,5635.25,3405.81,2229.44,10382,13140,0.24,1034.0,1\n# 1741785686.676473,1147984,23,49,0.7199,37836696,26316404,16,1398676,13071060,1009284,86016,7000064,5635.25,3405.81,2229.44,1369,1824,0.15,1033.0,1\n# 1741785687.6766264,1148012,38,34,0.7199,37850036,26301016,16,1400724,13043036,1009284,40960,49152,5635.25,3405.81,2229.44,10602,9682,0.26,1029.0,1\n</code></pre> <p>The default stream can be redirected to a file by passing a path to the <code>csv_file_path</code> argument, and can use different intervals for sampling via the <code>interval</code> argument.</p> <p>The <code>ProcessTracker</code> class tracks resource usage of a running process (defaults to the current process) and optionally all its children (recursively), in a similar manner, although somewhat limited in functionality, as e.g. <code>nvidia-smi pmon</code> can only track up-to 4 GPUs, and network traffic monitoring is not available.</p> <p>Helper functions are also provided, e.g. <code>get_process_stats</code> and <code>get_system_stats</code> from both the <code>tracker_procfs</code> and <code>tracker_psutil</code> modules, which are used internally by the above classes after diffing values between subsequent calls.</p> <p>See more details in the API References.</p>"},{"location":"integrations/standalone/#discovery-helpers","title":"Discovery Helpers","text":"<p>The packages also comes with helpers for discovering the cloud environment and basic server hardware specs. Quick example on an AWS EC2 instance:</p> <pre><code>from resource_tracker import get_cloud_info, get_server_info\nget_cloud_info()\n# {'vendor': 'aws', 'instance_type': 'g4dn.xlarge', 'region': 'us-west-2', 'discovery_time': 0.1330404281616211}\nget_server_info()\n# {'vcpus': 4, 'memory_mb': 15788.21, 'gpu_count': 1, 'gpu_names': ['Tesla T4'], 'gpu_memory_mb': 15360.0}\n</code></pre> <p>Spare Cores integration can do further lookups for the current server type, e.g. to calculate the cost of running the current job and recommend cheaper cloud server types for future runs.</p>"},{"location":"reference/resource_tracker/","title":"resource_tracker","text":""},{"location":"reference/resource_tracker/#resource_tracker","title":"resource_tracker","text":"<p>Resource Tracker package for monitoring resource usage, detecting cloud environments, and more.</p> <p>Modules:</p> Name Description <code>cloud_info</code> <p>Detect cloud environment (provider, region, instance type) via VM metadata services.</p> <code>dummy_workloads</code> <code>helpers</code> <p>General helpers.</p> <code>keeper</code> <code>nvidia</code> <p>Helpers to monitor NVIDIA GPUs.</p> <code>report</code> <code>server_info</code> <p>Detect server hardware (CPU count, memory amount, disk space, GPU count and VRAM amount) via <code>procfs</code> or <code>psutil</code>, and <code>nvidia-smi</code>.</p> <code>tiny_bars</code> <p>A tiny, partial, and opinionated implementation of the Handlebars template engine.</p> <code>tiny_data_frame</code> <p>A very inefficient data-frame implementation for manipulating resource usage data.</p> <code>tracker</code> <p>Track resource usage of a process and/or the system.</p> <code>tracker_procfs</code> <p>Helpers to track resource usage via <code>procfs</code>.</p> <code>tracker_psutil</code> <p>Helpers to track resource usage via <code>psutil</code>.</p> <p>Classes:</p> Name Description <code>ProcessTracker</code> <p>Track resource usage of a process and optionally its children.</p> <code>ResourceTracker</code> <p>Track resource usage of processes and the system in a non-blocking way.</p> <code>SystemTracker</code> <p>Track system-wide resource usage.</p> <p>Functions:</p> Name Description <code>get_cloud_info</code> <p>Detect cloud environment and return standardized information on provider, region, and instance type.</p> <code>get_server_info</code> <p>Collects important information about the Linux server.</p>"},{"location":"reference/resource_tracker/#resource_tracker.get_cloud_info","title":"get_cloud_info  <code>cached</code>","text":"<pre><code>get_cloud_info()\n</code></pre> <p>Detect cloud environment and return standardized information on provider, region, and instance type.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing standardized cloud information:</p> <ul> <li><code>vendor</code>: The cloud provider (aws, gcp, azure, hcloud, upcloud), or \"unknown\"</li> <li><code>instance_type</code>: The instance type/size/flavor, or \"unknown\"</li> <li><code>region</code>: The region/zone where the instance is running, or \"unknown\"</li> <li><code>discovery_time</code>: The time taken to discover the cloud environment, in seconds</li> </ul> Source code in <code>resource_tracker/cloud_info.py</code> <pre><code>@cache\ndef get_cloud_info() -&gt; dict:\n    \"\"\"\n    Detect cloud environment and return standardized information on provider, region, and instance type.\n\n    Returns:\n        A dictionary containing standardized cloud information:\n\n            - `vendor`: The cloud provider (aws, gcp, azure, hcloud, upcloud), or \"unknown\"\n            - `instance_type`: The instance type/size/flavor, or \"unknown\"\n            - `region`: The region/zone where the instance is running, or \"unknown\"\n            - `discovery_time`: The time taken to discover the cloud environment, in seconds\n    \"\"\"\n    start_time = time()\n    check_functions = [\n        _check_aws,\n        _check_gcp,\n        _check_azure,\n        _check_hetzner,\n        _check_upcloud,\n    ]\n\n    # run checks in parallel, return early if any check succeeds\n    with ThreadPoolExecutor(max_workers=len(check_functions)) as executor:\n        futures = {executor.submit(check_fn): check_fn for check_fn in check_functions}\n        pending = set(futures.keys())\n        while pending:\n            done, pending = wait(pending, return_when=FIRST_COMPLETED)\n            for future in done:\n                with suppress(Exception):\n                    info = future.result()\n                    if info:\n                        # stop all remaining checks early\n                        for f in pending:\n                            f.cancel()\n                        return info | {\"discovery_time\": time() - start_time}\n\n    return {\n        \"vendor\": \"unknown\",\n        \"instance_type\": \"unknown\",\n        \"region\": \"unknown\",\n        \"discovery_time\": time() - start_time,\n    }\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.get_server_info","title":"get_server_info","text":"<pre><code>get_server_info()\n</code></pre> <p>Collects important information about the Linux server.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing server information:</p> <ul> <li><code>os</code>: Operating system</li> <li><code>vcpus</code>: Number of virtual CPUs</li> <li><code>memory_mb</code>: Total memory in MB</li> <li><code>gpu_count</code>: Number of GPUs (<code>0</code> if not available)</li> <li><code>gpu_names</code>: List of GPU names (<code>[]</code> if not available)</li> <li><code>gpu_name</code>: Most common GPU name (<code>\"\"</code> if not available)</li> <li><code>gpu_memory_mb</code>: Total VRAM in MB (<code>0</code> if not available)</li> </ul> Source code in <code>resource_tracker/server_info.py</code> <pre><code>def get_server_info() -&gt; dict:\n    \"\"\"\n    Collects important information about the Linux server.\n\n    Returns:\n        A dictionary containing server information:\n\n            - `os`: Operating system\n            - `vcpus`: Number of virtual CPUs\n            - `memory_mb`: Total memory in MB\n            - `gpu_count`: Number of GPUs (`0` if not available)\n            - `gpu_names`: List of GPU names (`[]` if not available)\n            - `gpu_name`: Most common GPU name (`\"\"` if not available)\n            - `gpu_memory_mb`: Total VRAM in MB (`0` if not available)\n    \"\"\"\n    gpu_info = get_gpu_info()\n    info = {\n        \"os\": system(),\n        \"vcpus\": cpu_count(),\n        \"memory_mb\": get_total_memory_mb(),\n        \"gpu_count\": gpu_info[\"count\"],\n        \"gpu_names\": gpu_info[\"gpu_names\"],\n        \"gpu_name\": (\n            Counter(gpu_info[\"gpu_names\"]).most_common(1)[0][0]\n            if gpu_info[\"gpu_names\"]\n            else \"\"\n        ),\n        \"gpu_memory_mb\": gpu_info[\"memory_mb\"],\n    }\n    return info\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ProcessTracker","title":"ProcessTracker","text":"<p>Track resource usage of a process and optionally its children.</p> <p>This class monitors system resources like CPU times and usage, memory usage, GPU and VRAM utilization, I/O operations for a given process ID and optionally its child processes.</p> <p>Data is collected every <code>interval</code> seconds and written to the stdout or <code>output_file</code> (if provided) as CSV. Currently, the following columns are tracked:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>pid (int): The monitored process ID.</li> <li>children (int | None): The current number of child processes.</li> <li>utime (int): The total user+nice mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>cpu_usage (float): The current CPU usage between 0 and number of CPUs.</li> <li>memory (int): The current memory usage in kB. Implementation depends on the   operating system, and it is preferably PSS (Proportional Set Size) on Linux,   USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on   Windows.</li> <li>read_bytes (int): The total number of bytes read from disk.</li> <li>write_bytes (int): The total number of bytes written to disk.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>Process ID to track. Defaults to current process ID.</p> <code>getpid()</code> <code>start_time</code> <code>float</code> <p>Time when to start tracking. Defaults to current time.</p> <code>time()</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>children</code> <code>bool</code> <p>Whether to track child processes. Defaults to True.</p> <code>True</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>File to write the output to. Defaults to None, print to stdout.</p> <code>None</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Dummy method to make this class callable.</p> <code>diff_stats</code> <p>Calculate stats since last call.</p> <code>start_tracking</code> <p>Start an infinite loop tracking resource usage of the process until it exits.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class ProcessTracker:\n    \"\"\"Track resource usage of a process and optionally its children.\n\n    This class monitors system resources like CPU times and usage, memory usage,\n    GPU and VRAM utilization, I/O operations for a given process ID and\n    optionally its child processes.\n\n    Data is collected every `interval` seconds and written to the stdout or\n    `output_file` (if provided) as CSV. Currently, the following columns are\n    tracked:\n\n    - timestamp (float): The current timestamp.\n    - pid (int): The monitored process ID.\n    - children (int | None): The current number of child processes.\n    - utime (int): The total user+nice mode CPU time in seconds.\n    - stime (int): The total system mode CPU time in seconds.\n    - cpu_usage (float): The current CPU usage between 0 and number of CPUs.\n    - memory (int): The current memory usage in kB. Implementation depends on the\n      operating system, and it is preferably PSS (Proportional Set Size) on Linux,\n      USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on\n      Windows.\n    - read_bytes (int): The total number of bytes read from disk.\n    - write_bytes (int): The total number of bytes written to disk.\n    - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n    - gpu_vram (float): The current GPU memory used in MiB.\n    - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n\n    Args:\n        pid (int, optional): Process ID to track. Defaults to current process ID.\n        start_time: Time when to start tracking. Defaults to current time.\n        interval (float, optional): Sampling interval in seconds. Defaults to 1.\n        children (bool, optional): Whether to track child processes. Defaults to True.\n        autostart (bool, optional): Whether to start tracking immediately. Defaults to True.\n        output_file (str, optional): File to write the output to. Defaults to None, print to stdout.\n    \"\"\"\n\n    def __init__(\n        self,\n        pid: int = getpid(),\n        start_time: float = time(),\n        interval: float = 1,\n        children: bool = True,\n        autostart: bool = True,\n        output_file: str = None,\n    ):\n        self.get_process_stats, _ = get_tracker_implementation()\n\n        self.pid = pid\n        self.status = \"running\"\n        self.interval = interval\n        self.cycle = 0\n        self.children = children\n        self.start_time = start_time\n\n        # initial data collection so that we can use that as a baseline when diffing after the first interval\n        self.stats = self.get_process_stats(pid, children)\n\n        if autostart:\n            # wait for the start time to be reached\n            if start_time &gt; time():\n                sleep(start_time - time())\n            # we can now start. 1st interval used to collect baseline\n            self.start_tracking(output_file)\n\n    def __call__(self):\n        \"\"\"Dummy method to make this class callable.\"\"\"\n        pass\n\n    def diff_stats(self):\n        \"\"\"Calculate stats since last call.\"\"\"\n        last_stats = self.stats\n        self.stats = self.get_process_stats(self.pid, self.children)\n        self.cycle += 1\n\n        return {\n            \"timestamp\": round(self.stats[\"timestamp\"], 3),\n            \"pid\": self.pid,\n            \"children\": self.stats[\"children\"],\n            \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n            \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n            \"cpu_usage\": round(\n                max(\n                    0,\n                    (\n                        (self.stats[\"utime\"] + self.stats[\"stime\"])\n                        - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                    )\n                    / (self.stats[\"timestamp\"] - last_stats[\"timestamp\"]),\n                ),\n                4,\n            ),\n            \"memory\": self.stats[\"memory\"],\n            \"read_bytes\": max(0, self.stats[\"read_bytes\"] - last_stats[\"read_bytes\"]),\n            \"write_bytes\": max(\n                0, self.stats[\"write_bytes\"] - last_stats[\"write_bytes\"]\n            ),\n            \"gpu_usage\": self.stats[\"gpu_usage\"],\n            \"gpu_vram\": self.stats[\"gpu_vram\"],\n            \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n        }\n\n    def start_tracking(\n        self, output_file: Optional[str] = None, print_header: bool = True\n    ):\n        \"\"\"Start an infinite loop tracking resource usage of the process until it exits.\n\n        A CSV line is written every `interval` seconds.\n\n        Args:\n            output_file: File to write the output to. Defaults to None, printing to stdout.\n            print_header: Whether to print the header of the CSV. Defaults to True.\n        \"\"\"\n        file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n        try:\n            while True:\n                current_stats = self.diff_stats()\n                if current_stats[\"memory\"] == 0:\n                    # the process has exited\n                    self.status = \"exited\"\n                    break\n                # don't print values yet, we collect data for the 1st baseline\n                if self.cycle == 1:\n                    if print_header:\n                        file_handle.write(\n                            render_csv_row(\n                                current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                            )\n                        )\n                else:\n                    file_handle.write(\n                        render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                    )\n                if output_file:\n                    file_handle.flush()\n                # sleep until the next interval\n                sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n        finally:\n            if output_file and not file_handle.closed:\n                file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ProcessTracker.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Dummy method to make this class callable.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def __call__(self):\n    \"\"\"Dummy method to make this class callable.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ProcessTracker.diff_stats","title":"diff_stats","text":"<pre><code>diff_stats()\n</code></pre> <p>Calculate stats since last call.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def diff_stats(self):\n    \"\"\"Calculate stats since last call.\"\"\"\n    last_stats = self.stats\n    self.stats = self.get_process_stats(self.pid, self.children)\n    self.cycle += 1\n\n    return {\n        \"timestamp\": round(self.stats[\"timestamp\"], 3),\n        \"pid\": self.pid,\n        \"children\": self.stats[\"children\"],\n        \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n        \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n        \"cpu_usage\": round(\n            max(\n                0,\n                (\n                    (self.stats[\"utime\"] + self.stats[\"stime\"])\n                    - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                )\n                / (self.stats[\"timestamp\"] - last_stats[\"timestamp\"]),\n            ),\n            4,\n        ),\n        \"memory\": self.stats[\"memory\"],\n        \"read_bytes\": max(0, self.stats[\"read_bytes\"] - last_stats[\"read_bytes\"]),\n        \"write_bytes\": max(\n            0, self.stats[\"write_bytes\"] - last_stats[\"write_bytes\"]\n        ),\n        \"gpu_usage\": self.stats[\"gpu_usage\"],\n        \"gpu_vram\": self.stats[\"gpu_vram\"],\n        \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n    }\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ProcessTracker.start_tracking","title":"start_tracking","text":"<pre><code>start_tracking(output_file=None, print_header=True)\n</code></pre> <p>Start an infinite loop tracking resource usage of the process until it exits.</p> <p>A CSV line is written every <code>interval</code> seconds.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>Optional[str]</code> <p>File to write the output to. Defaults to None, printing to stdout.</p> <code>None</code> <code>print_header</code> <code>bool</code> <p>Whether to print the header of the CSV. Defaults to True.</p> <code>True</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start_tracking(\n    self, output_file: Optional[str] = None, print_header: bool = True\n):\n    \"\"\"Start an infinite loop tracking resource usage of the process until it exits.\n\n    A CSV line is written every `interval` seconds.\n\n    Args:\n        output_file: File to write the output to. Defaults to None, printing to stdout.\n        print_header: Whether to print the header of the CSV. Defaults to True.\n    \"\"\"\n    file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n    try:\n        while True:\n            current_stats = self.diff_stats()\n            if current_stats[\"memory\"] == 0:\n                # the process has exited\n                self.status = \"exited\"\n                break\n            # don't print values yet, we collect data for the 1st baseline\n            if self.cycle == 1:\n                if print_header:\n                    file_handle.write(\n                        render_csv_row(\n                            current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                        )\n                    )\n            else:\n                file_handle.write(\n                    render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                )\n            if output_file:\n                file_handle.flush()\n            # sleep until the next interval\n            sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n    finally:\n        if output_file and not file_handle.closed:\n            file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker","title":"ResourceTracker","text":"<p>Track resource usage of processes and the system in a non-blocking way.</p> <p>Start a resource_tracker.ProcessTracker and/or a resource_tracker.SystemTracker in the background as spawned or forked process(es), and make the collected data available easily in the main process via the <code>process_metrics</code> and <code>system_metrics</code> properties.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>Process ID to track. Defaults to current process ID.</p> <code>getpid()</code> <code>children</code> <code>bool</code> <p>Whether to track child processes. Defaults to True.</p> <code>True</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>method</code> <code>Optional[str]</code> <p>Multiprocessing method. Defaults to None, which tries to fork on Linux and macOS, and spawn on Windows.</p> <code>None</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>track_processes</code> <code>bool</code> <p>Whether to track resource usage at the process level. Defaults to True.</p> <code>True</code> <code>track_system</code> <code>bool</code> <p>Whether to track system-wide resource usage. Defaults to True.</p> <code>True</code> <code>discover_server</code> <code>bool</code> <p>Whether to discover the server specs in the background at startup. Defaults to True.</p> <code>True</code> <code>discover_cloud</code> <code>bool</code> <p>Whether to discover the cloud environment in the background at startup. Defaults to True.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; from resource_tracker.dummy_workloads import cpu_single, cpu_multi\n&gt;&gt;&gt; tracker = ResourceTracker()\n&gt;&gt;&gt; cpu_single()\n&gt;&gt;&gt; tracker.recommend_resources()  # doctest: +SKIP\n{'cpu': 1, 'memory': 128, 'gpu': 0, 'vram': 0}\n&gt;&gt;&gt; tracker = ResourceTracker()\n&gt;&gt;&gt; while tracker.n_samples == 0:\n...     cpu_multi(duration=0.25, ncores=2)\n&gt;&gt;&gt; tracker.recommend_resources()  # multiprocessing is not enough efficient on Windows/macOS  # doctest: +SKIP\n{'cpu': 2, 'memory': 128, 'gpu': 0, 'vram': 0}\n</code></pre> <p>Methods:</p> Name Description <code>start</code> <p>Start the selected resource trackers in the background as subprocess(es).</p> <code>cleanup</code> <p>Cleanup temp files and background processes.</p> <code>stop</code> <p>Stop the previously started resource trackers' background processes.</p> <code>snapshot</code> <p>Collect the current state of the resource tracker.</p> <code>from_snapshot</code> <p>Create a ResourceTracker from a snapshot.</p> <code>dumps</code> <p>Serialize the resource tracker to a JSON string.</p> <code>loads</code> <p>Deserialize the resource tracker from a JSON string.</p> <code>dump</code> <p>Serialize the resource tracker to a gzipped JSON file.</p> <code>load</code> <p>Deserialize the resource tracker from a gzipped JSON file.</p> <code>get_combined_metrics</code> <p>Collected data both from the resource_tracker.ProcessTracker and resource_tracker.SystemTracker.</p> <code>stats</code> <p>Collect statistics from the resource tracker.</p> <code>wait_for_samples</code> <p>Wait for at least one sample to be collected.</p> <code>recommend_resources</code> <p>Recommend optimal resource allocation based on the measured resource tracker data.</p> <code>recommend_server</code> <p>Recommend the cheapest cloud server matching the recommended resources.</p> <p>Attributes:</p> Name Type Description <code>n_samples</code> <code>int</code> <p>Number of samples collected by the resource tracker.</p> <code>server_info</code> <code>dict</code> <p>High-level server info.</p> <code>cloud_info</code> <code>dict</code> <p>High-level cloud info.</p> <code>process_metrics</code> <code>TinyDataFrame</code> <p>Collected data from resource_tracker.ProcessTracker.</p> <code>system_metrics</code> <code>TinyDataFrame</code> <p>Collected data from resource_tracker.SystemTracker.</p> <code>running</code> <code>bool</code> <p>Check if the resource tracker is running.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class ResourceTracker:\n    \"\"\"Track resource usage of processes and the system in a non-blocking way.\n\n    Start a [resource_tracker.ProcessTracker][] and/or a [resource_tracker.SystemTracker][] in the background as spawned\n    or forked process(es), and make the collected data available easily in the\n    main process via the `process_metrics` and `system_metrics` properties.\n\n    Args:\n        pid: Process ID to track. Defaults to current process ID.\n        children: Whether to track child processes. Defaults to True.\n        interval: Sampling interval in seconds. Defaults to 1.\n        method: Multiprocessing method. Defaults to None, which tries to fork on\n            Linux and macOS, and spawn on Windows.\n        autostart: Whether to start tracking immediately. Defaults to True.\n        track_processes: Whether to track resource usage at the process level.\n            Defaults to True.\n        track_system: Whether to track system-wide resource usage. Defaults to True.\n        discover_server: Whether to discover the server specs in the background at\n            startup. Defaults to True.\n        discover_cloud: Whether to discover the cloud environment in the background\n            at startup. Defaults to True.\n\n    Example:\n\n        &gt;&gt;&gt; from resource_tracker.dummy_workloads import cpu_single, cpu_multi\n        &gt;&gt;&gt; tracker = ResourceTracker()\n        &gt;&gt;&gt; cpu_single()\n        &gt;&gt;&gt; tracker.recommend_resources()  # doctest: +SKIP\n        {'cpu': 1, 'memory': 128, 'gpu': 0, 'vram': 0}\n        &gt;&gt;&gt; tracker = ResourceTracker()\n        &gt;&gt;&gt; while tracker.n_samples == 0:\n        ...     cpu_multi(duration=0.25, ncores=2)\n        &gt;&gt;&gt; tracker.recommend_resources()  # multiprocessing is not enough efficient on Windows/macOS  # doctest: +SKIP\n        {'cpu': 2, 'memory': 128, 'gpu': 0, 'vram': 0}\n    \"\"\"\n\n    _server_info: Optional[dict] = None\n    _cloud_info: Optional[dict] = None\n\n    def __init__(\n        self,\n        pid: int = getpid(),\n        children: bool = True,\n        interval: float = 1,\n        method: Optional[str] = None,\n        autostart: bool = True,\n        track_processes: bool = True,\n        track_system: bool = True,\n        discover_server: bool = True,\n        discover_cloud: bool = True,\n    ):\n        self.pid = pid\n        self.children = children\n        self.interval = interval\n        self.method = method\n        self.autostart = autostart\n        self.trackers = []\n        if track_processes:\n            self.trackers.append(\"process_tracker\")\n        if track_system:\n            self.trackers.append(\"system_tracker\")\n        self.discover_server = discover_server\n        self.discover_cloud = discover_cloud\n\n        if platform != \"linux\" and not is_psutil_available():\n            raise ImportError(\n                \"psutil is required for resource tracking on non-Linux platforms\"\n            )\n\n        if method is None:\n            # try to fork when possible due to leaked semaphores on older Python versions\n            # see e.g. https://github.com/python/cpython/issues/90549\n            if platform in [\"linux\", \"darwin\"]:\n                self.mpc = get_context(\"fork\")\n            else:\n                self.mpc = get_context(\"spawn\")\n        else:\n            self.mpc = get_context(method)\n\n        # error details from subprocesses\n        self.error_queue = self.mpc.SimpleQueue()\n\n        # create temporary CSV file(s) for the tracker(s), and record only the file path(s)\n        # to be passed later to subprocess(es) avoiding pickling the file object(s)\n        for tracker_name in self.trackers:\n            temp_file = NamedTemporaryFile(delete=False)\n            setattr(self, f\"{tracker_name}_filepath\", temp_file.name)\n            temp_file.close()\n        # make sure to cleanup the temp file(s)\n        finalize(\n            self,\n            cleanup_files,\n            [\n                getattr(self, f\"{tracker_name}_filepath\")\n                for tracker_name in self.trackers\n            ],\n        )\n\n        if autostart:\n            self.start()\n\n    def start(self):\n        \"\"\"Start the selected resource trackers in the background as subprocess(es).\"\"\"\n        if self.running:\n            raise RuntimeError(\"Resource tracker already running, cannot start again.\")\n        if hasattr(self, \"stop_time\"):\n            raise RuntimeError(\n                \"Resource tracker already stopped. Create a new instance instead of trying to restart it.\"\n            )\n\n        self.start_time = time()\n        self.stop_time = None\n        # round to the nearest interval in the future\n        self.start_time = ceil(self.start_time / self.interval) * self.interval\n        # leave at least 50 ms for trackers to start\n        if self.start_time - time() &lt; 0.05:\n            self.start_time += self.interval\n\n        if \"process_tracker\" in self.trackers:\n            self.process_tracker_process = self.mpc.Process(\n                target=_run_tracker,\n                args=(\"process\", self.error_queue),\n                kwargs={\n                    \"pid\": self.pid,\n                    \"start_time\": self.start_time,\n                    \"interval\": self.interval,\n                    \"children\": self.children,\n                    \"output_file\": self.process_tracker_filepath,\n                },\n                daemon=True,\n            )\n            self.process_tracker_process.start()\n\n        if \"system_tracker\" in self.trackers:\n            self.system_tracker_process = self.mpc.Process(\n                target=_run_tracker,\n                args=(\"system\", self.error_queue),\n                kwargs={\n                    \"start_time\": self.start_time,\n                    \"interval\": self.interval,\n                    \"output_file\": self.system_tracker_filepath,\n                },\n                daemon=True,\n            )\n            self.system_tracker_process.start()\n\n        def collect_server_info():\n            \"\"\"Collect server info to be run in a background thread.\"\"\"\n            try:\n                self._server_info = get_server_info()\n            except Exception as e:\n                logger.warning(f\"Error fetching server info: {e}\")\n\n        def collect_cloud_info():\n            \"\"\"Collect cloud info to be run in a background thread.\"\"\"\n            try:\n                self._cloud_info = get_cloud_info()\n            except Exception as e:\n                logger.warning(f\"Error fetching cloud info: {e}\")\n\n        if self.discover_server:\n            server_thread = Thread(target=collect_server_info, daemon=True)\n            server_thread.start()\n        if self.discover_cloud:\n            cloud_thread = Thread(target=collect_cloud_info, daemon=True)\n            cloud_thread.start()\n\n        # make sure to cleanup the started subprocess(es)\n        finalize(\n            self,\n            cleanup_processes,\n            [\n                getattr(self, f\"{tracker_name}_process\")\n                for tracker_name in self.trackers\n            ],\n        )\n\n    def cleanup(self):\n        \"\"\"Cleanup temp files and background processes.\n\n        Note that there is no need to call this method manually, as it is\n        automatically handled by the garbage collector, but in some cases it\n        might be useful to call it manually to avoid waiting for the garbage\n        collector to run.\n        \"\"\"\n        with suppress(Exception):\n            self.stop()\n        with suppress(Exception):\n            cleanup_files(\n                [\n                    getattr(self, f\"{tracker_name}_filepath\")\n                    for tracker_name in self.trackers\n                ]\n            )\n        with suppress(Exception):\n            cleanup_processes(\n                [\n                    getattr(self, f\"{tracker_name}_process\")\n                    for tracker_name in self.trackers\n                ]\n            )\n\n    def stop(self):\n        \"\"\"Stop the previously started resource trackers' background processes.\"\"\"\n        self.stop_time = time()\n        # check for errors in the subprocesses\n        if not self.error_queue.empty():\n            error_data = self.error_queue.get()\n            logger.warning(\n                \"Resource tracker subprocess failed!\\n\"\n                f\"Error type: {error_data['name']} (from module {error_data['module']})\\n\"\n                f\"Error message: {error_data['message']}\\n\"\n                f\"Original traceback:\\n{error_data['traceback']}\"\n            )\n        # terminate tracker processes\n        for tracker_name in self.trackers:\n            process_attr = f\"{tracker_name}_process\"\n            if hasattr(self, process_attr):\n                cleanup_processes([getattr(self, process_attr)])\n        self.error_queue.close()\n        logger.debug(\n            \"Resource tracker stopped after %s seconds, logging %d process-level and %d system-wide records\",\n            self.stop_time - self.start_time,\n            len(self.process_metrics),\n            len(self.system_metrics),\n        )\n\n    @property\n    def n_samples(self) -&gt; int:\n        \"\"\"Number of samples collected by the resource tracker.\"\"\"\n        return min(len(self.process_metrics), len(self.system_metrics))\n\n    @property\n    def server_info(self) -&gt; dict:\n        \"\"\"High-level server info.\n\n        Collected data from [resource_tracker.get_server_info][] plus a guess\n        for the allocation type of the server: if it's dedicated to the tracked\n        process(es) or shared with other processes. The guess is based on the\n        [resource_tracker.column_maps.SERVER_ALLOCATION_CHECKS][] checks.\n        \"\"\"\n        server_info = self._server_info\n        if server_info:\n            server_info[\"allocation\"] = None\n        if self.n_samples &gt; 0:\n            for check in SERVER_ALLOCATION_CHECKS:\n                try:\n                    system_val = mean(self.system_metrics[check[\"system_column\"]])\n                    task_val = mean(self.process_metrics[check[\"process_column\"]])\n                    if (system_val &gt; task_val * check[\"percent\"]) or (\n                        system_val &gt; task_val + check[\"absolute\"]\n                    ):\n                        server_info[\"allocation\"] = \"shared\"\n                        break\n                except Exception as e:\n                    logger.warning(\n                        f\"Error calculating server allocation based on {check['system_column']} and {check['process_column']}: {e}\"\n                    )\n                    server_info[\"allocation\"] = \"unknown\"\n                    break\n            server_info[\"allocation\"] = server_info.get(\"allocation\", \"dedicated\")\n        return server_info\n\n    @property\n    def cloud_info(self) -&gt; dict:\n        \"\"\"High-level cloud info.\n\n        Collected data from [resource_tracker.get_cloud_info][].\n        \"\"\"\n        return self._cloud_info\n\n    @property\n    def process_metrics(self) -&gt; TinyDataFrame:\n        \"\"\"Collected data from [resource_tracker.ProcessTracker][].\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the collected data or an empty list if the [resource_tracker.ProcessTracker][] is not running.\n        \"\"\"\n        try:\n            return TinyDataFrame(\n                csv_file_path=self.process_tracker_filepath,\n                retries=2,\n                retry_delay=min(0.05, self.interval / 10),\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to read process metrics: {e}\")\n            return TinyDataFrame(data=[])\n\n    @property\n    def system_metrics(self) -&gt; TinyDataFrame:\n        \"\"\"Collected data from [resource_tracker.SystemTracker][].\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the collected data or an empty list if the [resource_tracker.SystemTracker][] is not running.\n        \"\"\"\n        try:\n            return TinyDataFrame(\n                csv_file_path=self.system_tracker_filepath,\n                retries=2,\n                retry_delay=min(0.05, self.interval / 10),\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to read system metrics: {e}\")\n            return TinyDataFrame(data=[])\n\n    def snapshot(self) -&gt; dict:\n        \"\"\"Collect the current state of the resource tracker.\n\n        Returns:\n            A dictionary containing the current state of the resource tracker.\n        \"\"\"\n        return {\n            \"metadata\": {\n                \"version\": 1,\n                \"resource_tracker\": {\n                    \"version\": __version__,\n                    \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n                },\n                \"pid\": self.pid,\n                \"children\": self.children,\n                \"interval\": self.interval,\n                \"method\": self.method,\n                \"autostart\": self.autostart,\n                \"track_processes\": \"process_tracker\" in self.trackers,\n                \"track_system\": \"system_tracker\" in self.trackers,\n                \"discover_server\": self.discover_server,\n                \"discover_cloud\": self.discover_cloud,\n                \"start_time\": self.start_time,\n                \"stop_time\": self.stop_time or time(),\n                \"duration\": round(\n                    (self.stop_time or time()) - self.start_time + self.interval, 2\n                ),\n            },\n            \"server_info\": self.server_info,\n            \"cloud_info\": self.cloud_info,\n            \"process_metrics\": self.process_metrics.to_dict(),\n            \"system_metrics\": self.system_metrics.to_dict(),\n        }\n\n    @classmethod\n    def from_snapshot(cls, snapshot: dict):\n        \"\"\"Create a ResourceTracker from a snapshot.\n\n        Args:\n            snapshot: A dictionary containing the current state of the resource tracker, created by [resource_tracker.ResourceTracker.snapshot][].\n        \"\"\"\n        tracker = cls(\n            pid=snapshot[\"metadata\"][\"pid\"],\n            children=snapshot[\"metadata\"][\"children\"],\n            interval=snapshot[\"metadata\"][\"interval\"],\n            method=snapshot[\"metadata\"][\"method\"],\n            autostart=False,\n            track_processes=snapshot[\"metadata\"][\"track_processes\"],\n            track_system=snapshot[\"metadata\"][\"track_system\"],\n            discover_server=snapshot[\"metadata\"][\"discover_server\"],\n            discover_cloud=snapshot[\"metadata\"][\"discover_cloud\"],\n        )\n        tracker.start_time = snapshot[\"metadata\"][\"start_time\"]\n        tracker.stop_time = snapshot[\"metadata\"][\"stop_time\"]\n        tracker._server_info = snapshot[\"server_info\"]\n        tracker._cloud_info = snapshot[\"cloud_info\"]\n        TinyDataFrame(data=snapshot[\"process_metrics\"]).to_csv(\n            tracker.process_tracker_filepath\n        )\n        TinyDataFrame(data=snapshot[\"system_metrics\"]).to_csv(\n            tracker.system_tracker_filepath\n        )\n        return tracker\n\n    def dumps(self) -&gt; str:\n        \"\"\"Serialize the resource tracker to a JSON string.\n\n        Returns:\n            A JSON string containing the current state of the resource tracker.\n        \"\"\"\n        return json_dumps(self.snapshot())\n\n    @classmethod\n    def loads(cls, s: str):\n        \"\"\"Deserialize the resource tracker from a JSON string.\n\n        Args:\n            s: The JSON string to deserialize the resource tracker from.\n        \"\"\"\n        return cls.from_snapshot(json_loads(s))\n\n    def dump(self, file: str):\n        \"\"\"Serialize the resource tracker to a gzipped JSON file.\n\n        Args:\n            file: The path to the file to write the serialized resource tracker to.\n        \"\"\"\n        with gzip_open(file, \"wb\") as f:\n            f.write(self.dumps().encode())\n\n    @classmethod\n    def load(cls, file: str):\n        \"\"\"Deserialize the resource tracker from a gzipped JSON file.\n\n        Args:\n            file: The path to the file to read the serialized resource tracker from.\n        \"\"\"\n        with gzip_open(file, \"rb\") as f:\n            return cls.loads(f.read().decode())\n\n    def get_combined_metrics(\n        self,\n        bytes: bool = False,\n        human_names: bool = False,\n        system_prefix: Optional[str] = None,\n        process_prefix: Optional[str] = None,\n    ) -&gt; TinyDataFrame:\n        \"\"\"Collected data both from the [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][].\n\n        This is effectively binding the two dataframes together by timestamp,\n        and adding a prefix to the column names to distinguish between the system and process metrics.\n\n        Args:\n            bytes: Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] (kB, MiB, or GiB).\n            human_names: Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] with prefixes.\n            system_prefix: Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of `human_names`.\n            process_prefix: Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of `human_names`.\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the combined data or an empty list if tracker(s) not running.\n        \"\"\"\n        try:\n            process_metrics = self.process_metrics\n            system_metrics = self.system_metrics\n\n            # ensure both have the same length\n            if len(process_metrics) &gt; len(system_metrics):\n                process_metrics = process_metrics[: len(system_metrics)]\n            elif len(system_metrics) &gt; len(process_metrics):\n                system_metrics = system_metrics[: len(process_metrics)]\n\n            # nothing to report on\n            if len(process_metrics) == 0:\n                return TinyDataFrame(data=[])\n\n            if bytes:\n                for col, factor in BYTE_MAPPING.items():\n                    for metrics in (system_metrics, process_metrics):\n                        if col in metrics.columns:\n                            metrics[col] = [v * factor for v in metrics[col]]\n\n            if system_prefix is None:\n                system_prefix = \"system_\" if not human_names else \"System \"\n            if process_prefix is None:\n                process_prefix = \"process_\" if not human_names else \"Process \"\n\n            # cbind the two dataframes with column name prefixes and optional human-friendly names\n            combined = system_metrics.rename(\n                columns={\n                    n: (\n                        (system_prefix if n != \"timestamp\" else \"\")\n                        + (n if not human_names else HUMAN_NAMES_MAPPING.get(n, n))\n                    )\n                    for n in system_metrics.columns\n                }\n            )\n            for col in process_metrics.columns[1:]:\n                combined[\n                    process_prefix\n                    + (col if not human_names else HUMAN_NAMES_MAPPING.get(col, col))\n                ] = process_metrics[col]\n\n            return combined\n        except Exception as e:\n            with suppress(Exception):\n                logger.warning(\n                    f\"Kept {len(process_metrics) if 'process_metrics' in locals() else 'unknown'} records of process metrics out of {len(self.process_metrics)} collected records ({self.process_metrics.columns}), \"\n                    f\"and {len(system_metrics) if 'system_metrics' in locals() else 'unknown'} records of system metrics out of {len(self.system_metrics)} collected records ({self.system_metrics.columns}), \"\n                    f\"but creating the combined metrics dataframe failed with error: {e}\"\n                )\n                logger.warning(f\"Process metrics: {self.process_metrics.to_dict()}\")\n                logger.warning(f\"System metrics: {self.system_metrics.to_dict()}\")\n            raise\n\n    def stats(\n        self,\n        specs: List[StatSpec] = [\n            StatSpec(column=\"process_cpu_usage\", agg=mean, round=2),\n            StatSpec(column=\"process_cpu_usage\", agg=max, round=2),\n            StatSpec(column=\"process_memory\", agg=mean, round=2),\n            StatSpec(column=\"process_memory\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_usage\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_usage\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_vram\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_vram\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_utilized\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_utilized\", agg=max, round=2),\n            StatSpec(column=\"system_disk_space_used_gb\", agg=max, round=2),\n            StatSpec(column=\"system_net_recv_bytes\", agg=sum),\n            StatSpec(column=\"system_net_sent_bytes\", agg=sum),\n            StatSpec(\n                column=\"timestamp\", agg=lambda x: max(x) - min(x), agg_name=\"duration\"\n            ),\n        ],\n    ) -&gt; dict:\n        \"\"\"Collect statistics from the resource tracker.\n\n        Args:\n            specs: A list of [resource_tracker.tiny_data_frame.StatSpec][] objects specifying the statistics to collect.\n\n        Returns:\n            A dictionary containing the collected statistics.\n        \"\"\"\n        if self.n_samples &gt; 0:\n            stats = self.get_combined_metrics().stats(specs)\n            stats[\"timestamp\"][\"duration\"] += self.interval\n            return stats\n        else:\n            raise RuntimeError(\"No metrics collected (yet)\")\n\n    @property\n    def running(self) -&gt; bool:\n        \"\"\"Check if the resource tracker is running.\n\n        Returns:\n            True if the resource tracker is running, False if already stopped.\n        \"\"\"\n        return hasattr(self, \"stop_time\") and self.stop_time is None\n\n    def wait_for_samples(self, n: int = 1, timeout: float = 5):\n        \"\"\"Wait for at least one sample to be collected.\n\n        Args:\n            n: The minimum number of samples to collect. Defaults to 1.\n            timeout: The maximum time to wait for a sample. Defaults to 5 seconds.\n        \"\"\"\n        if self.running:\n            while self.n_samples &lt; n:\n                sleep(self.interval / 10)\n                if time() - self.start_time &gt; timeout:\n                    raise RuntimeError(\n                        f\"Timed out waiting for resource tracker to collect {n} samples\"\n                    )\n        else:\n            if self.n_samples &lt; n:\n                raise RuntimeError(\n                    f\"Resource tracker has been already stopped with {self.n_samples} sample(s), \"\n                    f\"cannot wait to collect the requested {n} sample(s).\"\n                )\n\n    def recommend_resources(self, historical_stats: List[dict] = []) -&gt; dict:\n        \"\"\"Recommend optimal resource allocation based on the measured resource tracker data.\n\n        The recommended resources are based on the following rules:\n\n        - target average CPU usage of the process(es)\n        - target maximum memory usage of the process(es) with a 20% buffer\n        - target maximum number of GPUs used by the process(es)\n        - target maximum VRAM usage of the process(es) with a 20% buffer\n\n        Args:\n            historical_stats: Optional list of historical statistics (as returned by [resource_tracker.ResourceTracker.stats][])\n                              to consider when making recommendations. These will be combined with the current stats.\n\n        Returns:\n            A dictionary containing the recommended resources (cpu, memory, gpu, vram).\n        \"\"\"\n        self.wait_for_samples(n=1, timeout=self.interval * 5)\n\n        current_stats = self.stats()\n        if historical_stats:\n            stats = aggregate_stats([current_stats] + historical_stats)\n        else:\n            stats = current_stats\n\n        rec = {}\n        # target average CPU usage\n        rec[\"cpu\"] = max(1, round(stats[\"process_cpu_usage\"][\"mean\"]))\n        # target maximum memory usage (kB-&gt;MB) with a 20% buffer\n        rec[\"memory\"] = round_memory(mb=stats[\"process_memory\"][\"max\"] * 1.2 / 1024)\n        # target maximum GPU number of GPUs used\n        rec[\"gpu\"] = (\n            max(1, round(stats[\"process_gpu_usage\"][\"max\"]))\n            if stats[\"process_gpu_usage\"][\"mean\"] &gt; 0\n            else 0\n        )\n        # target maximum VRAM usage (MiB) with a 20% buffer\n        rec[\"vram\"] = (\n            round_memory(mb=stats[\"process_gpu_vram\"][\"max\"] * 1.2)\n            if stats[\"process_gpu_vram\"][\"max\"] &gt; 0\n            else 0\n        )\n        return rec\n\n    def recommend_server(self, **kwargs) -&gt; dict:\n        \"\"\"Recommend the cheapest cloud server matching the recommended resources.\n\n        Args:\n            **kwargs: Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.\n\n        Returns:\n            A dictionary containing the recommended cloud server. Response format is described at &lt;https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get&gt;.\n        \"\"\"\n        historical_stats = kwargs.pop(\"historical_stats\", [])\n        rec = self.recommend_resources(historical_stats=historical_stats)\n        return get_recommended_cloud_servers(**rec, **kwargs, n=1)[0]\n\n    def report(\n        self,\n        integration: Literal[\"standalone\", \"Metaflow\", \"R\"] = \"standalone\",\n        historical_stats: List[dict] = [],\n        status_failed: bool = False,\n        integration_version: Optional[str] = None,\n    ) -&gt; Report:\n        self.wait_for_samples(n=1, timeout=self.interval * 5)\n        duration = (self.stop_time or time()) - self.start_time + self.interval\n\n        current_stats = self.stats()\n        if historical_stats:\n            combined_stats = aggregate_stats([current_stats] + historical_stats)\n        else:\n            combined_stats = current_stats\n\n        ctx = {\n            \"files\": _read_report_template_files(),\n            \"server_info\": self.server_info,\n            \"cloud_info\": self.cloud_info,\n            \"process_metrics\": self.process_metrics,\n            \"system_metrics\": self.system_metrics,\n            \"stats\": current_stats,\n            \"historical_stats\": historical_stats,\n            \"combined_stats\": combined_stats,\n            \"recommended_resources\": self.recommend_resources(\n                historical_stats=historical_stats\n            ),\n            \"recommended_server\": self.recommend_server(\n                historical_stats=historical_stats\n            ),\n            \"resource_tracker\": {\n                \"version\": __version__,\n                \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n                \"integration\": integration,\n                \"integration_is\": {\n                    \"standalone\": integration == \"standalone\",\n                    \"not_standalone\": integration != \"standalone\",\n                    \"metaflow\": integration == \"Metaflow\",\n                    \"r\": integration == \"R\",\n                },\n                \"duration\": duration,\n                \"start_time\": self.start_time,\n                \"stop_time\": self.stop_time,\n                \"stopped\": self.stop_time is not None,\n                \"report_time\": time(),\n            },\n            \"status_failed\": status_failed,\n            \"csv\": {},\n        }\n\n        # comma-separated values\n        joined = self.get_combined_metrics(bytes=True, human_names=True)\n        for name, columns in REPORT_CSV_MAPPING.items():\n            csv_data = joined[columns]\n            # convert to JS milliseconds\n            csv_data[\"Timestamp\"] = [t * 1000 for t in csv_data[\"Timestamp\"]]\n            ctx[\"csv\"][name] = csv_data.to_csv(quote_strings=False)\n\n        # lookup instance prices\n        rec_server_cost = ctx[\"recommended_server\"][\"min_price_ondemand\"]\n        rec_run_cost = rec_server_cost / 60 / 60 * duration\n        ctx[\"recommended_server\"][\"best_ondemand_price_duration\"] = rec_run_cost\n        if ctx[\"cloud_info\"] and ctx[\"cloud_info\"][\"instance_type\"] != \"unknown\":\n            current_server_cost = get_instance_price(\n                ctx[\"cloud_info\"][\"vendor\"],\n                ctx[\"cloud_info\"][\"region\"],\n                ctx[\"cloud_info\"][\"instance_type\"],\n            )\n            if current_server_cost:\n                current_run_cost = round(current_server_cost / 60 / 60 * duration, 6)\n                ctx[\"cloud_info\"][\"run_costs\"] = current_run_cost\n                ctx[\"recommended_server\"][\"cost_savings\"] = {\n                    \"percent\": round(\n                        (current_run_cost - rec_run_cost) / current_run_cost * 100, 2\n                    ),\n                    \"amount\": round(current_run_cost - rec_run_cost, 6),\n                }\n\n        html_template_path = path.join(\n            path.dirname(__file__), \"report_template\", \"report.html\"\n        )\n        with open(html_template_path) as f:\n            html_template = f.read()\n        html = render_template(html_template, ctx)\n        return Report(html)\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Start the selected resource trackers in the background as subprocess(es).</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start(self):\n    \"\"\"Start the selected resource trackers in the background as subprocess(es).\"\"\"\n    if self.running:\n        raise RuntimeError(\"Resource tracker already running, cannot start again.\")\n    if hasattr(self, \"stop_time\"):\n        raise RuntimeError(\n            \"Resource tracker already stopped. Create a new instance instead of trying to restart it.\"\n        )\n\n    self.start_time = time()\n    self.stop_time = None\n    # round to the nearest interval in the future\n    self.start_time = ceil(self.start_time / self.interval) * self.interval\n    # leave at least 50 ms for trackers to start\n    if self.start_time - time() &lt; 0.05:\n        self.start_time += self.interval\n\n    if \"process_tracker\" in self.trackers:\n        self.process_tracker_process = self.mpc.Process(\n            target=_run_tracker,\n            args=(\"process\", self.error_queue),\n            kwargs={\n                \"pid\": self.pid,\n                \"start_time\": self.start_time,\n                \"interval\": self.interval,\n                \"children\": self.children,\n                \"output_file\": self.process_tracker_filepath,\n            },\n            daemon=True,\n        )\n        self.process_tracker_process.start()\n\n    if \"system_tracker\" in self.trackers:\n        self.system_tracker_process = self.mpc.Process(\n            target=_run_tracker,\n            args=(\"system\", self.error_queue),\n            kwargs={\n                \"start_time\": self.start_time,\n                \"interval\": self.interval,\n                \"output_file\": self.system_tracker_filepath,\n            },\n            daemon=True,\n        )\n        self.system_tracker_process.start()\n\n    def collect_server_info():\n        \"\"\"Collect server info to be run in a background thread.\"\"\"\n        try:\n            self._server_info = get_server_info()\n        except Exception as e:\n            logger.warning(f\"Error fetching server info: {e}\")\n\n    def collect_cloud_info():\n        \"\"\"Collect cloud info to be run in a background thread.\"\"\"\n        try:\n            self._cloud_info = get_cloud_info()\n        except Exception as e:\n            logger.warning(f\"Error fetching cloud info: {e}\")\n\n    if self.discover_server:\n        server_thread = Thread(target=collect_server_info, daemon=True)\n        server_thread.start()\n    if self.discover_cloud:\n        cloud_thread = Thread(target=collect_cloud_info, daemon=True)\n        cloud_thread.start()\n\n    # make sure to cleanup the started subprocess(es)\n    finalize(\n        self,\n        cleanup_processes,\n        [\n            getattr(self, f\"{tracker_name}_process\")\n            for tracker_name in self.trackers\n        ],\n    )\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup temp files and background processes.</p> <p>Note that there is no need to call this method manually, as it is automatically handled by the garbage collector, but in some cases it might be useful to call it manually to avoid waiting for the garbage collector to run.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def cleanup(self):\n    \"\"\"Cleanup temp files and background processes.\n\n    Note that there is no need to call this method manually, as it is\n    automatically handled by the garbage collector, but in some cases it\n    might be useful to call it manually to avoid waiting for the garbage\n    collector to run.\n    \"\"\"\n    with suppress(Exception):\n        self.stop()\n    with suppress(Exception):\n        cleanup_files(\n            [\n                getattr(self, f\"{tracker_name}_filepath\")\n                for tracker_name in self.trackers\n            ]\n        )\n    with suppress(Exception):\n        cleanup_processes(\n            [\n                getattr(self, f\"{tracker_name}_process\")\n                for tracker_name in self.trackers\n            ]\n        )\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the previously started resource trackers' background processes.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the previously started resource trackers' background processes.\"\"\"\n    self.stop_time = time()\n    # check for errors in the subprocesses\n    if not self.error_queue.empty():\n        error_data = self.error_queue.get()\n        logger.warning(\n            \"Resource tracker subprocess failed!\\n\"\n            f\"Error type: {error_data['name']} (from module {error_data['module']})\\n\"\n            f\"Error message: {error_data['message']}\\n\"\n            f\"Original traceback:\\n{error_data['traceback']}\"\n        )\n    # terminate tracker processes\n    for tracker_name in self.trackers:\n        process_attr = f\"{tracker_name}_process\"\n        if hasattr(self, process_attr):\n            cleanup_processes([getattr(self, process_attr)])\n    self.error_queue.close()\n    logger.debug(\n        \"Resource tracker stopped after %s seconds, logging %d process-level and %d system-wide records\",\n        self.stop_time - self.start_time,\n        len(self.process_metrics),\n        len(self.system_metrics),\n    )\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.n_samples","title":"n_samples  <code>property</code>","text":"<pre><code>n_samples\n</code></pre> <p>Number of samples collected by the resource tracker.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.server_info","title":"server_info  <code>property</code>","text":"<pre><code>server_info\n</code></pre> <p>High-level server info.</p> <p>Collected data from resource_tracker.get_server_info plus a guess for the allocation type of the server: if it's dedicated to the tracked process(es) or shared with other processes. The guess is based on the [resource_tracker.column_maps.SERVER_ALLOCATION_CHECKS][] checks.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.cloud_info","title":"cloud_info  <code>property</code>","text":"<pre><code>cloud_info\n</code></pre> <p>High-level cloud info.</p> <p>Collected data from resource_tracker.get_cloud_info.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.process_metrics","title":"process_metrics  <code>property</code>","text":"<pre><code>process_metrics\n</code></pre> <p>Collected data from resource_tracker.ProcessTracker.</p> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the collected data or an empty list if the resource_tracker.ProcessTracker is not running.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.system_metrics","title":"system_metrics  <code>property</code>","text":"<pre><code>system_metrics\n</code></pre> <p>Collected data from resource_tracker.SystemTracker.</p> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the collected data or an empty list if the resource_tracker.SystemTracker is not running.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.snapshot","title":"snapshot","text":"<pre><code>snapshot()\n</code></pre> <p>Collect the current state of the resource tracker.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the current state of the resource tracker.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def snapshot(self) -&gt; dict:\n    \"\"\"Collect the current state of the resource tracker.\n\n    Returns:\n        A dictionary containing the current state of the resource tracker.\n    \"\"\"\n    return {\n        \"metadata\": {\n            \"version\": 1,\n            \"resource_tracker\": {\n                \"version\": __version__,\n                \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n            },\n            \"pid\": self.pid,\n            \"children\": self.children,\n            \"interval\": self.interval,\n            \"method\": self.method,\n            \"autostart\": self.autostart,\n            \"track_processes\": \"process_tracker\" in self.trackers,\n            \"track_system\": \"system_tracker\" in self.trackers,\n            \"discover_server\": self.discover_server,\n            \"discover_cloud\": self.discover_cloud,\n            \"start_time\": self.start_time,\n            \"stop_time\": self.stop_time or time(),\n            \"duration\": round(\n                (self.stop_time or time()) - self.start_time + self.interval, 2\n            ),\n        },\n        \"server_info\": self.server_info,\n        \"cloud_info\": self.cloud_info,\n        \"process_metrics\": self.process_metrics.to_dict(),\n        \"system_metrics\": self.system_metrics.to_dict(),\n    }\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.from_snapshot","title":"from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(snapshot)\n</code></pre> <p>Create a ResourceTracker from a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot</code> <code>dict</code> <p>A dictionary containing the current state of the resource tracker, created by resource_tracker.ResourceTracker.snapshot.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef from_snapshot(cls, snapshot: dict):\n    \"\"\"Create a ResourceTracker from a snapshot.\n\n    Args:\n        snapshot: A dictionary containing the current state of the resource tracker, created by [resource_tracker.ResourceTracker.snapshot][].\n    \"\"\"\n    tracker = cls(\n        pid=snapshot[\"metadata\"][\"pid\"],\n        children=snapshot[\"metadata\"][\"children\"],\n        interval=snapshot[\"metadata\"][\"interval\"],\n        method=snapshot[\"metadata\"][\"method\"],\n        autostart=False,\n        track_processes=snapshot[\"metadata\"][\"track_processes\"],\n        track_system=snapshot[\"metadata\"][\"track_system\"],\n        discover_server=snapshot[\"metadata\"][\"discover_server\"],\n        discover_cloud=snapshot[\"metadata\"][\"discover_cloud\"],\n    )\n    tracker.start_time = snapshot[\"metadata\"][\"start_time\"]\n    tracker.stop_time = snapshot[\"metadata\"][\"stop_time\"]\n    tracker._server_info = snapshot[\"server_info\"]\n    tracker._cloud_info = snapshot[\"cloud_info\"]\n    TinyDataFrame(data=snapshot[\"process_metrics\"]).to_csv(\n        tracker.process_tracker_filepath\n    )\n    TinyDataFrame(data=snapshot[\"system_metrics\"]).to_csv(\n        tracker.system_tracker_filepath\n    )\n    return tracker\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.dumps","title":"dumps","text":"<pre><code>dumps()\n</code></pre> <p>Serialize the resource tracker to a JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing the current state of the resource tracker.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def dumps(self) -&gt; str:\n    \"\"\"Serialize the resource tracker to a JSON string.\n\n    Returns:\n        A JSON string containing the current state of the resource tracker.\n    \"\"\"\n    return json_dumps(self.snapshot())\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.loads","title":"loads  <code>classmethod</code>","text":"<pre><code>loads(s)\n</code></pre> <p>Deserialize the resource tracker from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The JSON string to deserialize the resource tracker from.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef loads(cls, s: str):\n    \"\"\"Deserialize the resource tracker from a JSON string.\n\n    Args:\n        s: The JSON string to deserialize the resource tracker from.\n    \"\"\"\n    return cls.from_snapshot(json_loads(s))\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.dump","title":"dump","text":"<pre><code>dump(file)\n</code></pre> <p>Serialize the resource tracker to a gzipped JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the file to write the serialized resource tracker to.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>def dump(self, file: str):\n    \"\"\"Serialize the resource tracker to a gzipped JSON file.\n\n    Args:\n        file: The path to the file to write the serialized resource tracker to.\n    \"\"\"\n    with gzip_open(file, \"wb\") as f:\n        f.write(self.dumps().encode())\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(file)\n</code></pre> <p>Deserialize the resource tracker from a gzipped JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the file to read the serialized resource tracker from.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef load(cls, file: str):\n    \"\"\"Deserialize the resource tracker from a gzipped JSON file.\n\n    Args:\n        file: The path to the file to read the serialized resource tracker from.\n    \"\"\"\n    with gzip_open(file, \"rb\") as f:\n        return cls.loads(f.read().decode())\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.get_combined_metrics","title":"get_combined_metrics","text":"<pre><code>get_combined_metrics(bytes=False, human_names=False, system_prefix=None, process_prefix=None)\n</code></pre> <p>Collected data both from the resource_tracker.ProcessTracker and resource_tracker.SystemTracker.</p> <p>This is effectively binding the two dataframes together by timestamp, and adding a prefix to the column names to distinguish between the system and process metrics.</p> <p>Parameters:</p> Name Type Description Default <code>bytes</code> <code>bool</code> <p>Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at resource_tracker.ProcessTracker and resource_tracker.SystemTracker (kB, MiB, or GiB).</p> <code>False</code> <code>human_names</code> <code>bool</code> <p>Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at resource_tracker.ProcessTracker and resource_tracker.SystemTracker with prefixes.</p> <code>False</code> <code>system_prefix</code> <code>Optional[str]</code> <p>Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of <code>human_names</code>.</p> <code>None</code> <code>process_prefix</code> <code>Optional[str]</code> <p>Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of <code>human_names</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the combined data or an empty list if tracker(s) not running.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def get_combined_metrics(\n    self,\n    bytes: bool = False,\n    human_names: bool = False,\n    system_prefix: Optional[str] = None,\n    process_prefix: Optional[str] = None,\n) -&gt; TinyDataFrame:\n    \"\"\"Collected data both from the [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][].\n\n    This is effectively binding the two dataframes together by timestamp,\n    and adding a prefix to the column names to distinguish between the system and process metrics.\n\n    Args:\n        bytes: Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] (kB, MiB, or GiB).\n        human_names: Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] with prefixes.\n        system_prefix: Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of `human_names`.\n        process_prefix: Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of `human_names`.\n\n    Returns:\n        A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the combined data or an empty list if tracker(s) not running.\n    \"\"\"\n    try:\n        process_metrics = self.process_metrics\n        system_metrics = self.system_metrics\n\n        # ensure both have the same length\n        if len(process_metrics) &gt; len(system_metrics):\n            process_metrics = process_metrics[: len(system_metrics)]\n        elif len(system_metrics) &gt; len(process_metrics):\n            system_metrics = system_metrics[: len(process_metrics)]\n\n        # nothing to report on\n        if len(process_metrics) == 0:\n            return TinyDataFrame(data=[])\n\n        if bytes:\n            for col, factor in BYTE_MAPPING.items():\n                for metrics in (system_metrics, process_metrics):\n                    if col in metrics.columns:\n                        metrics[col] = [v * factor for v in metrics[col]]\n\n        if system_prefix is None:\n            system_prefix = \"system_\" if not human_names else \"System \"\n        if process_prefix is None:\n            process_prefix = \"process_\" if not human_names else \"Process \"\n\n        # cbind the two dataframes with column name prefixes and optional human-friendly names\n        combined = system_metrics.rename(\n            columns={\n                n: (\n                    (system_prefix if n != \"timestamp\" else \"\")\n                    + (n if not human_names else HUMAN_NAMES_MAPPING.get(n, n))\n                )\n                for n in system_metrics.columns\n            }\n        )\n        for col in process_metrics.columns[1:]:\n            combined[\n                process_prefix\n                + (col if not human_names else HUMAN_NAMES_MAPPING.get(col, col))\n            ] = process_metrics[col]\n\n        return combined\n    except Exception as e:\n        with suppress(Exception):\n            logger.warning(\n                f\"Kept {len(process_metrics) if 'process_metrics' in locals() else 'unknown'} records of process metrics out of {len(self.process_metrics)} collected records ({self.process_metrics.columns}), \"\n                f\"and {len(system_metrics) if 'system_metrics' in locals() else 'unknown'} records of system metrics out of {len(self.system_metrics)} collected records ({self.system_metrics.columns}), \"\n                f\"but creating the combined metrics dataframe failed with error: {e}\"\n            )\n            logger.warning(f\"Process metrics: {self.process_metrics.to_dict()}\")\n            logger.warning(f\"System metrics: {self.system_metrics.to_dict()}\")\n        raise\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.stats","title":"stats","text":"<pre><code>stats(specs=[StatSpec(column='process_cpu_usage', agg=mean, round=2), StatSpec(column='process_cpu_usage', agg=max, round=2), StatSpec(column='process_memory', agg=mean, round=2), StatSpec(column='process_memory', agg=max, round=2), StatSpec(column='process_gpu_usage', agg=mean, round=2), StatSpec(column='process_gpu_usage', agg=max, round=2), StatSpec(column='process_gpu_vram', agg=mean, round=2), StatSpec(column='process_gpu_vram', agg=max, round=2), StatSpec(column='process_gpu_utilized', agg=mean, round=2), StatSpec(column='process_gpu_utilized', agg=max, round=2), StatSpec(column='system_disk_space_used_gb', agg=max, round=2), StatSpec(column='system_net_recv_bytes', agg=sum), StatSpec(column='system_net_sent_bytes', agg=sum), StatSpec(column='timestamp', agg=lambda x: max(x) - min(x), agg_name='duration')])\n</code></pre> <p>Collect statistics from the resource tracker.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>List[StatSpec]</code> <p>A list of resource_tracker.tiny_data_frame.StatSpec objects specifying the statistics to collect.</p> <code>[StatSpec(column='process_cpu_usage', agg=mean, round=2), StatSpec(column='process_cpu_usage', agg=max, round=2), StatSpec(column='process_memory', agg=mean, round=2), StatSpec(column='process_memory', agg=max, round=2), StatSpec(column='process_gpu_usage', agg=mean, round=2), StatSpec(column='process_gpu_usage', agg=max, round=2), StatSpec(column='process_gpu_vram', agg=mean, round=2), StatSpec(column='process_gpu_vram', agg=max, round=2), StatSpec(column='process_gpu_utilized', agg=mean, round=2), StatSpec(column='process_gpu_utilized', agg=max, round=2), StatSpec(column='system_disk_space_used_gb', agg=max, round=2), StatSpec(column='system_net_recv_bytes', agg=sum), StatSpec(column='system_net_sent_bytes', agg=sum), StatSpec(column='timestamp', agg=lambda x: max(x) - min(x), agg_name='duration')]</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the collected statistics.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def stats(\n    self,\n    specs: List[StatSpec] = [\n        StatSpec(column=\"process_cpu_usage\", agg=mean, round=2),\n        StatSpec(column=\"process_cpu_usage\", agg=max, round=2),\n        StatSpec(column=\"process_memory\", agg=mean, round=2),\n        StatSpec(column=\"process_memory\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_usage\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_usage\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_vram\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_vram\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_utilized\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_utilized\", agg=max, round=2),\n        StatSpec(column=\"system_disk_space_used_gb\", agg=max, round=2),\n        StatSpec(column=\"system_net_recv_bytes\", agg=sum),\n        StatSpec(column=\"system_net_sent_bytes\", agg=sum),\n        StatSpec(\n            column=\"timestamp\", agg=lambda x: max(x) - min(x), agg_name=\"duration\"\n        ),\n    ],\n) -&gt; dict:\n    \"\"\"Collect statistics from the resource tracker.\n\n    Args:\n        specs: A list of [resource_tracker.tiny_data_frame.StatSpec][] objects specifying the statistics to collect.\n\n    Returns:\n        A dictionary containing the collected statistics.\n    \"\"\"\n    if self.n_samples &gt; 0:\n        stats = self.get_combined_metrics().stats(specs)\n        stats[\"timestamp\"][\"duration\"] += self.interval\n        return stats\n    else:\n        raise RuntimeError(\"No metrics collected (yet)\")\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.running","title":"running  <code>property</code>","text":"<pre><code>running\n</code></pre> <p>Check if the resource tracker is running.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the resource tracker is running, False if already stopped.</p>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.wait_for_samples","title":"wait_for_samples","text":"<pre><code>wait_for_samples(n=1, timeout=5)\n</code></pre> <p>Wait for at least one sample to be collected.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The minimum number of samples to collect. Defaults to 1.</p> <code>1</code> <code>timeout</code> <code>float</code> <p>The maximum time to wait for a sample. Defaults to 5 seconds.</p> <code>5</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def wait_for_samples(self, n: int = 1, timeout: float = 5):\n    \"\"\"Wait for at least one sample to be collected.\n\n    Args:\n        n: The minimum number of samples to collect. Defaults to 1.\n        timeout: The maximum time to wait for a sample. Defaults to 5 seconds.\n    \"\"\"\n    if self.running:\n        while self.n_samples &lt; n:\n            sleep(self.interval / 10)\n            if time() - self.start_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out waiting for resource tracker to collect {n} samples\"\n                )\n    else:\n        if self.n_samples &lt; n:\n            raise RuntimeError(\n                f\"Resource tracker has been already stopped with {self.n_samples} sample(s), \"\n                f\"cannot wait to collect the requested {n} sample(s).\"\n            )\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.recommend_resources","title":"recommend_resources","text":"<pre><code>recommend_resources(historical_stats=[])\n</code></pre> <p>Recommend optimal resource allocation based on the measured resource tracker data.</p> <p>The recommended resources are based on the following rules:</p> <ul> <li>target average CPU usage of the process(es)</li> <li>target maximum memory usage of the process(es) with a 20% buffer</li> <li>target maximum number of GPUs used by the process(es)</li> <li>target maximum VRAM usage of the process(es) with a 20% buffer</li> </ul> <p>Parameters:</p> Name Type Description Default <code>historical_stats</code> <code>List[dict]</code> <p>Optional list of historical statistics (as returned by resource_tracker.ResourceTracker.stats)               to consider when making recommendations. These will be combined with the current stats.</p> <code>[]</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the recommended resources (cpu, memory, gpu, vram).</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def recommend_resources(self, historical_stats: List[dict] = []) -&gt; dict:\n    \"\"\"Recommend optimal resource allocation based on the measured resource tracker data.\n\n    The recommended resources are based on the following rules:\n\n    - target average CPU usage of the process(es)\n    - target maximum memory usage of the process(es) with a 20% buffer\n    - target maximum number of GPUs used by the process(es)\n    - target maximum VRAM usage of the process(es) with a 20% buffer\n\n    Args:\n        historical_stats: Optional list of historical statistics (as returned by [resource_tracker.ResourceTracker.stats][])\n                          to consider when making recommendations. These will be combined with the current stats.\n\n    Returns:\n        A dictionary containing the recommended resources (cpu, memory, gpu, vram).\n    \"\"\"\n    self.wait_for_samples(n=1, timeout=self.interval * 5)\n\n    current_stats = self.stats()\n    if historical_stats:\n        stats = aggregate_stats([current_stats] + historical_stats)\n    else:\n        stats = current_stats\n\n    rec = {}\n    # target average CPU usage\n    rec[\"cpu\"] = max(1, round(stats[\"process_cpu_usage\"][\"mean\"]))\n    # target maximum memory usage (kB-&gt;MB) with a 20% buffer\n    rec[\"memory\"] = round_memory(mb=stats[\"process_memory\"][\"max\"] * 1.2 / 1024)\n    # target maximum GPU number of GPUs used\n    rec[\"gpu\"] = (\n        max(1, round(stats[\"process_gpu_usage\"][\"max\"]))\n        if stats[\"process_gpu_usage\"][\"mean\"] &gt; 0\n        else 0\n    )\n    # target maximum VRAM usage (MiB) with a 20% buffer\n    rec[\"vram\"] = (\n        round_memory(mb=stats[\"process_gpu_vram\"][\"max\"] * 1.2)\n        if stats[\"process_gpu_vram\"][\"max\"] &gt; 0\n        else 0\n    )\n    return rec\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.ResourceTracker.recommend_server","title":"recommend_server","text":"<pre><code>recommend_server(**kwargs)\n</code></pre> <p>Recommend the cheapest cloud server matching the recommended resources.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the recommended cloud server. Response format is described at https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def recommend_server(self, **kwargs) -&gt; dict:\n    \"\"\"Recommend the cheapest cloud server matching the recommended resources.\n\n    Args:\n        **kwargs: Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.\n\n    Returns:\n        A dictionary containing the recommended cloud server. Response format is described at &lt;https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get&gt;.\n    \"\"\"\n    historical_stats = kwargs.pop(\"historical_stats\", [])\n    rec = self.recommend_resources(historical_stats=historical_stats)\n    return get_recommended_cloud_servers(**rec, **kwargs, n=1)[0]\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.SystemTracker","title":"SystemTracker","text":"<p>Track system-wide resource usage.</p> <p>This class monitors system resources like CPU times and usage, memory usage, GPU and VRAM utilization, disk I/O, and network traffic for the entire system.</p> <p>Data is collected every <code>interval</code> seconds and written to the stdout or <code>output_file</code> (if provided) as CSV. Currently, the following columns are tracked:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>processes (int): The number of running processes.</li> <li>utime (int): The total user+nice mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>cpu_usage (float): The current CPU usage between 0 and number of CPUs.</li> <li>memory_free (int): The amount of free memory in kB.</li> <li>memory_used (int): The amount of used memory in kB.</li> <li>memory_buffers (int): The amount of memory used for buffers in kB.</li> <li>memory_cached (int): The amount of memory used for caching in kB.</li> <li>memory_active (int): The amount of memory used for active pages in kB.</li> <li>memory_inactive (int): The amount of memory used for inactive pages in kB.</li> <li>disk_read_bytes (int): The total number of bytes read from disk.</li> <li>disk_write_bytes (int): The total number of bytes written to disk.</li> <li>disk_space_total_gb (float): The total disk space in GB.</li> <li>disk_space_used_gb (float): The used disk space in GB.</li> <li>disk_space_free_gb (float): The free disk space in GB.</li> <li>net_recv_bytes (int): The total number of bytes received over network.</li> <li>net_sent_bytes (int): The total number of bytes sent over network.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Time when to start tracking. Defaults to current time.</p> <code>time()</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>File to write the output to. Defaults to None, print to stdout.</p> <code>None</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Dummy method to make this class callable.</p> <code>diff_stats</code> <p>Calculate stats since last call.</p> <code>start_tracking</code> <p>Start an infinite loop tracking system resource usage.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class SystemTracker:\n    \"\"\"Track system-wide resource usage.\n\n    This class monitors system resources like CPU times and usage, memory usage,\n    GPU and VRAM utilization, disk I/O, and network traffic for the entire system.\n\n    Data is collected every `interval` seconds and written to the stdout or\n    `output_file` (if provided) as CSV. Currently, the following columns are\n    tracked:\n\n    - timestamp (float): The current timestamp.\n    - processes (int): The number of running processes.\n    - utime (int): The total user+nice mode CPU time in seconds.\n    - stime (int): The total system mode CPU time in seconds.\n    - cpu_usage (float): The current CPU usage between 0 and number of CPUs.\n    - memory_free (int): The amount of free memory in kB.\n    - memory_used (int): The amount of used memory in kB.\n    - memory_buffers (int): The amount of memory used for buffers in kB.\n    - memory_cached (int): The amount of memory used for caching in kB.\n    - memory_active (int): The amount of memory used for active pages in kB.\n    - memory_inactive (int): The amount of memory used for inactive pages in kB.\n    - disk_read_bytes (int): The total number of bytes read from disk.\n    - disk_write_bytes (int): The total number of bytes written to disk.\n    - disk_space_total_gb (float): The total disk space in GB.\n    - disk_space_used_gb (float): The used disk space in GB.\n    - disk_space_free_gb (float): The free disk space in GB.\n    - net_recv_bytes (int): The total number of bytes received over network.\n    - net_sent_bytes (int): The total number of bytes sent over network.\n    - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n    - gpu_vram (float): The current GPU memory used in MiB.\n    - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n\n    Args:\n        start_time: Time when to start tracking. Defaults to current time.\n        interval: Sampling interval in seconds. Defaults to 1.\n        autostart: Whether to start tracking immediately. Defaults to True.\n        output_file: File to write the output to. Defaults to None, print to stdout.\n    \"\"\"\n\n    def __init__(\n        self,\n        start_time: float = time(),\n        interval: float = 1,\n        autostart: bool = True,\n        output_file: str = None,\n    ):\n        _, self.get_system_stats = get_tracker_implementation()\n\n        self.status = \"running\"\n        self.interval = interval\n        self.cycle = 0\n        self.start_time = start_time\n\n        # dummy data collection so that diffing on the first time does not fail\n        self.stats = self.get_system_stats()\n\n        if autostart:\n            # wait for the start time to be reached\n            if start_time &gt; time():\n                sleep(start_time - time())\n            # we can now start. 1st interval used to collect baseline\n            self.start_tracking(output_file)\n\n    def __call__(self):\n        \"\"\"Dummy method to make this class callable.\"\"\"\n        pass\n\n    def diff_stats(self):\n        \"\"\"Calculate stats since last call.\"\"\"\n        last_stats = self.stats\n        self.stats = self.get_system_stats()\n        self.cycle += 1\n\n        time_diff = self.stats[\"timestamp\"] - last_stats[\"timestamp\"]\n\n        total_read_bytes = 0\n        total_write_bytes = 0\n        for disk_name in set(self.stats[\"disk_stats\"]) &amp; set(last_stats[\"disk_stats\"]):\n            read_bytes = max(\n                0,\n                self.stats[\"disk_stats\"][disk_name][\"read_bytes\"]\n                - last_stats[\"disk_stats\"][disk_name][\"read_bytes\"],\n            )\n            write_bytes = max(\n                0,\n                self.stats[\"disk_stats\"][disk_name][\"write_bytes\"]\n                - last_stats[\"disk_stats\"][disk_name][\"write_bytes\"],\n            )\n            total_read_bytes += read_bytes\n            total_write_bytes += write_bytes\n\n        disk_space_total = 0\n        disk_space_used = 0\n        disk_space_free = 0\n        for disk_space in self.stats[\"disk_spaces\"].values():\n            disk_space_total += disk_space[\"total\"]\n            disk_space_used += disk_space[\"used\"]\n            disk_space_free += disk_space[\"free\"]\n\n        return {\n            \"timestamp\": round(self.stats[\"timestamp\"], 3),\n            \"processes\": self.stats[\"processes\"],\n            \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n            \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n            \"cpu_usage\": round(\n                max(\n                    0,\n                    (\n                        (self.stats[\"utime\"] + self.stats[\"stime\"])\n                        - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                    )\n                    / time_diff,\n                ),\n                4,\n            ),\n            \"memory_free\": self.stats[\"memory_free\"],\n            \"memory_used\": self.stats[\"memory_used\"],\n            \"memory_buffers\": self.stats[\"memory_buffers\"],\n            \"memory_cached\": self.stats[\"memory_cached\"],\n            \"memory_active\": self.stats[\"memory_active\"],\n            \"memory_inactive\": self.stats[\"memory_inactive\"],\n            \"disk_read_bytes\": total_read_bytes,\n            \"disk_write_bytes\": total_write_bytes,\n            \"disk_space_total_gb\": round(disk_space_total / (1024**3), 2),\n            \"disk_space_used_gb\": round(disk_space_used / (1024**3), 2),\n            \"disk_space_free_gb\": round(disk_space_free / (1024**3), 2),\n            \"net_recv_bytes\": max(\n                0, self.stats[\"net_recv_bytes\"] - last_stats[\"net_recv_bytes\"]\n            ),\n            \"net_sent_bytes\": max(\n                0, self.stats[\"net_sent_bytes\"] - last_stats[\"net_sent_bytes\"]\n            ),\n            \"gpu_usage\": self.stats[\"gpu_usage\"],\n            \"gpu_vram\": self.stats[\"gpu_vram\"],\n            \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n        }\n\n    def start_tracking(\n        self, output_file: Optional[str] = None, print_header: bool = True\n    ):\n        \"\"\"Start an infinite loop tracking system resource usage.\n\n        A CSV line is written every `interval` seconds.\n\n        Args:\n            output_file: File to write the output to. Defaults to None, printing to stdout.\n            print_header: Whether to print the header of the CSV. Defaults to True.\n        \"\"\"\n        file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n        try:\n            while True:\n                current_stats = self.diff_stats()\n                # don't print values yet, we collect data for the 1st baseline\n                if self.cycle == 1:\n                    if print_header:\n                        file_handle.write(\n                            render_csv_row(\n                                current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                            )\n                        )\n                else:\n                    file_handle.write(\n                        render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                    )\n                if output_file:\n                    file_handle.flush()\n                # sleep until the next interval\n                sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n        finally:\n            if output_file and not file_handle.closed:\n                file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.SystemTracker.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Dummy method to make this class callable.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def __call__(self):\n    \"\"\"Dummy method to make this class callable.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.SystemTracker.diff_stats","title":"diff_stats","text":"<pre><code>diff_stats()\n</code></pre> <p>Calculate stats since last call.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def diff_stats(self):\n    \"\"\"Calculate stats since last call.\"\"\"\n    last_stats = self.stats\n    self.stats = self.get_system_stats()\n    self.cycle += 1\n\n    time_diff = self.stats[\"timestamp\"] - last_stats[\"timestamp\"]\n\n    total_read_bytes = 0\n    total_write_bytes = 0\n    for disk_name in set(self.stats[\"disk_stats\"]) &amp; set(last_stats[\"disk_stats\"]):\n        read_bytes = max(\n            0,\n            self.stats[\"disk_stats\"][disk_name][\"read_bytes\"]\n            - last_stats[\"disk_stats\"][disk_name][\"read_bytes\"],\n        )\n        write_bytes = max(\n            0,\n            self.stats[\"disk_stats\"][disk_name][\"write_bytes\"]\n            - last_stats[\"disk_stats\"][disk_name][\"write_bytes\"],\n        )\n        total_read_bytes += read_bytes\n        total_write_bytes += write_bytes\n\n    disk_space_total = 0\n    disk_space_used = 0\n    disk_space_free = 0\n    for disk_space in self.stats[\"disk_spaces\"].values():\n        disk_space_total += disk_space[\"total\"]\n        disk_space_used += disk_space[\"used\"]\n        disk_space_free += disk_space[\"free\"]\n\n    return {\n        \"timestamp\": round(self.stats[\"timestamp\"], 3),\n        \"processes\": self.stats[\"processes\"],\n        \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n        \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n        \"cpu_usage\": round(\n            max(\n                0,\n                (\n                    (self.stats[\"utime\"] + self.stats[\"stime\"])\n                    - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                )\n                / time_diff,\n            ),\n            4,\n        ),\n        \"memory_free\": self.stats[\"memory_free\"],\n        \"memory_used\": self.stats[\"memory_used\"],\n        \"memory_buffers\": self.stats[\"memory_buffers\"],\n        \"memory_cached\": self.stats[\"memory_cached\"],\n        \"memory_active\": self.stats[\"memory_active\"],\n        \"memory_inactive\": self.stats[\"memory_inactive\"],\n        \"disk_read_bytes\": total_read_bytes,\n        \"disk_write_bytes\": total_write_bytes,\n        \"disk_space_total_gb\": round(disk_space_total / (1024**3), 2),\n        \"disk_space_used_gb\": round(disk_space_used / (1024**3), 2),\n        \"disk_space_free_gb\": round(disk_space_free / (1024**3), 2),\n        \"net_recv_bytes\": max(\n            0, self.stats[\"net_recv_bytes\"] - last_stats[\"net_recv_bytes\"]\n        ),\n        \"net_sent_bytes\": max(\n            0, self.stats[\"net_sent_bytes\"] - last_stats[\"net_sent_bytes\"]\n        ),\n        \"gpu_usage\": self.stats[\"gpu_usage\"],\n        \"gpu_vram\": self.stats[\"gpu_vram\"],\n        \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n    }\n</code></pre>"},{"location":"reference/resource_tracker/#resource_tracker.SystemTracker.start_tracking","title":"start_tracking","text":"<pre><code>start_tracking(output_file=None, print_header=True)\n</code></pre> <p>Start an infinite loop tracking system resource usage.</p> <p>A CSV line is written every <code>interval</code> seconds.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>Optional[str]</code> <p>File to write the output to. Defaults to None, printing to stdout.</p> <code>None</code> <code>print_header</code> <code>bool</code> <p>Whether to print the header of the CSV. Defaults to True.</p> <code>True</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start_tracking(\n    self, output_file: Optional[str] = None, print_header: bool = True\n):\n    \"\"\"Start an infinite loop tracking system resource usage.\n\n    A CSV line is written every `interval` seconds.\n\n    Args:\n        output_file: File to write the output to. Defaults to None, printing to stdout.\n        print_header: Whether to print the header of the CSV. Defaults to True.\n    \"\"\"\n    file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n    try:\n        while True:\n            current_stats = self.diff_stats()\n            # don't print values yet, we collect data for the 1st baseline\n            if self.cycle == 1:\n                if print_header:\n                    file_handle.write(\n                        render_csv_row(\n                            current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                        )\n                    )\n            else:\n                file_handle.write(\n                    render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                )\n            if output_file:\n                file_handle.flush()\n            # sleep until the next interval\n            sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n    finally:\n        if output_file and not file_handle.closed:\n            file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/cloud_info/","title":"resource_tracker.cloud_info","text":""},{"location":"reference/resource_tracker/cloud_info/#resource_tracker.cloud_info","title":"resource_tracker.cloud_info","text":"<p>Detect cloud environment (provider, region, instance type) via VM metadata services.</p> <p>Functions:</p> Name Description <code>get_cloud_info</code> <p>Detect cloud environment and return standardized information on provider, region, and instance type.</p>"},{"location":"reference/resource_tracker/cloud_info/#resource_tracker.cloud_info.get_cloud_info","title":"get_cloud_info  <code>cached</code>","text":"<pre><code>get_cloud_info()\n</code></pre> <p>Detect cloud environment and return standardized information on provider, region, and instance type.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing standardized cloud information:</p> <ul> <li><code>vendor</code>: The cloud provider (aws, gcp, azure, hcloud, upcloud), or \"unknown\"</li> <li><code>instance_type</code>: The instance type/size/flavor, or \"unknown\"</li> <li><code>region</code>: The region/zone where the instance is running, or \"unknown\"</li> <li><code>discovery_time</code>: The time taken to discover the cloud environment, in seconds</li> </ul> Source code in <code>resource_tracker/cloud_info.py</code> <pre><code>@cache\ndef get_cloud_info() -&gt; dict:\n    \"\"\"\n    Detect cloud environment and return standardized information on provider, region, and instance type.\n\n    Returns:\n        A dictionary containing standardized cloud information:\n\n            - `vendor`: The cloud provider (aws, gcp, azure, hcloud, upcloud), or \"unknown\"\n            - `instance_type`: The instance type/size/flavor, or \"unknown\"\n            - `region`: The region/zone where the instance is running, or \"unknown\"\n            - `discovery_time`: The time taken to discover the cloud environment, in seconds\n    \"\"\"\n    start_time = time()\n    check_functions = [\n        _check_aws,\n        _check_gcp,\n        _check_azure,\n        _check_hetzner,\n        _check_upcloud,\n    ]\n\n    # run checks in parallel, return early if any check succeeds\n    with ThreadPoolExecutor(max_workers=len(check_functions)) as executor:\n        futures = {executor.submit(check_fn): check_fn for check_fn in check_functions}\n        pending = set(futures.keys())\n        while pending:\n            done, pending = wait(pending, return_when=FIRST_COMPLETED)\n            for future in done:\n                with suppress(Exception):\n                    info = future.result()\n                    if info:\n                        # stop all remaining checks early\n                        for f in pending:\n                            f.cancel()\n                        return info | {\"discovery_time\": time() - start_time}\n\n    return {\n        \"vendor\": \"unknown\",\n        \"instance_type\": \"unknown\",\n        \"region\": \"unknown\",\n        \"discovery_time\": time() - start_time,\n    }\n</code></pre>"},{"location":"reference/resource_tracker/column_maps/","title":"resource_tracker.column_maps","text":""},{"location":"reference/resource_tracker/column_maps/#resource_tracker.column_maps","title":"resource_tracker.column_maps","text":""},{"location":"reference/resource_tracker/dummy_workloads/","title":"resource_tracker.dummy_workloads","text":""},{"location":"reference/resource_tracker/dummy_workloads/#resource_tracker.dummy_workloads","title":"resource_tracker.dummy_workloads","text":"<p>Functions:</p> Name Description <code>cpu_single</code> <p>Multiplies random numbers for a given duration.</p> <code>cpu_multi</code> <p>Multiplies random numbers for a given duration using ncores cores in parallel.</p>"},{"location":"reference/resource_tracker/dummy_workloads/#resource_tracker.dummy_workloads.cpu_single","title":"cpu_single","text":"<pre><code>cpu_single(duration=1)\n</code></pre> <p>Multiplies random numbers for a given duration.</p> <p>Parameters:</p> Name Type Description Default <code>duration</code> <code>float</code> <p>The duration in seconds to run the workload.</p> <code>1</code> Source code in <code>resource_tracker/dummy_workloads.py</code> <pre><code>def cpu_single(duration: float = 1) -&gt; None:\n    \"\"\"Multiplies random numbers for a given duration.\n\n    Args:\n        duration: The duration in seconds to run the workload.\n    \"\"\"\n    start = time()\n    while time() &lt; start + duration:\n        random() * random()\n</code></pre>"},{"location":"reference/resource_tracker/dummy_workloads/#resource_tracker.dummy_workloads.cpu_multi","title":"cpu_multi","text":"<pre><code>cpu_multi(duration=1, ncores=cpu_count())\n</code></pre> <p>Multiplies random numbers for a given duration using ncores cores in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>duration</code> <code>float</code> <p>The duration in seconds to run the workload.</p> <code>1</code> <code>ncores</code> <code>int</code> <p>The number of cores to use. Defaults to all available logical cores.</p> <code>cpu_count()</code> Source code in <code>resource_tracker/dummy_workloads.py</code> <pre><code>def cpu_multi(duration: float = 1, ncores: int = cpu_count()) -&gt; float:\n    \"\"\"Multiplies random numbers for a given duration using ncores cores in parallel.\n\n    Args:\n        duration: The duration in seconds to run the workload.\n        ncores: The number of cores to use. Defaults to all available logical cores.\n    \"\"\"\n    with Pool(ncores) as p:\n        p.map(cpu_single, [duration] * ncores)\n</code></pre>"},{"location":"reference/resource_tracker/helpers/","title":"resource_tracker.helpers","text":""},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers","title":"resource_tracker.helpers","text":"<p>General helpers.</p> <p>Functions:</p> Name Description <code>is_partition</code> <p>Determine if a disk name represents a partition rather than a whole disk.</p> <code>is_psutil_available</code> <p>Check if psutil is installed and available for import.</p> <code>is_procfs_available</code> <p>Check if procfs is available on the system.</p> <code>get_tracker_implementation</code> <p>Determine which tracker implementation to use based on available system resources.</p> <code>get_zfs_pools_space</code> <p>Get the space of ZFS pools.</p> <code>cleanup_files</code> <p>Cleanup files.</p> <code>cleanup_processes</code> <p>Gracefully, then if needed forcefully terminate and close processes.</p> <code>aggregate_stats</code> <p>Aggregate statistics from multiple sources.</p> <code>render_csv_row</code> <p>Format a single CSV row as a string in memory.</p>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.is_partition","title":"is_partition  <code>cached</code>","text":"<pre><code>is_partition(disk_name)\n</code></pre> <p>Determine if a disk name represents a partition rather than a whole disk.</p> <p>Parameters:</p> Name Type Description Default <code>disk_name</code> <code>str</code> <p>Name of the disk device (e.g., 'sda1', 'nvme0n1p1')</p> required <p>Returns:s     True if the device is likely a partition, False otherwise</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>@cache\ndef is_partition(disk_name: str) -&gt; bool:\n    \"\"\"\n    Determine if a disk name represents a partition rather than a whole disk.\n\n    Args:\n        disk_name: Name of the disk device (e.g., 'sda1', 'nvme0n1p1')\n\n    Returns:s\n        True if the device is likely a partition, False otherwise\n    \"\"\"\n    # common partition name patterns: sdXN, nvmeXnYpZ, mmcblkXpY\n    if search(r\"(sd[a-z]+|nvme\\d+n\\d+|mmcblk\\d+)p?\\d+$\", disk_name):\n        # check if there's a parent device in /sys/block/\n        parent_devices = [d.split(\"/\")[-2] for d in glob(\"/sys/block/*/\")]\n        if any(\n            disk_name.startswith(parent) and disk_name != parent\n            for parent in parent_devices\n        ):\n            return True\n    return False\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.is_psutil_available","title":"is_psutil_available  <code>cached</code>","text":"<pre><code>is_psutil_available()\n</code></pre> <p>Check if psutil is installed and available for import.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if psutil is available, False otherwise</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>@cache\ndef is_psutil_available() -&gt; bool:\n    \"\"\"\n    Check if psutil is installed and available for import.\n\n    Returns:\n        bool: True if psutil is available, False otherwise\n    \"\"\"\n    try:\n        return find_spec(\"psutil\") is not None\n    except ImportError:\n        return False\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.is_procfs_available","title":"is_procfs_available  <code>cached</code>","text":"<pre><code>is_procfs_available()\n</code></pre> <p>Check if procfs is available on the system.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if procfs is available, False otherwise</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>@cache\ndef is_procfs_available() -&gt; bool:\n    \"\"\"\n    Check if procfs is available on the system.\n\n    Returns:\n        bool: True if procfs is available, False otherwise\n    \"\"\"\n    return os.path.isdir(\"/proc\") and os.access(\"/proc\", os.R_OK)\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.get_tracker_implementation","title":"get_tracker_implementation  <code>cached</code>","text":"<pre><code>get_tracker_implementation()\n</code></pre> <p>Determine which tracker implementation to use based on available system resources.</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[Callable, Callable]</code> <p>A tuple containing (get_process_stats, get_system_stats) functions from the appropriate implementation module.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>If no suitable implementation is available.</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>@cache\ndef get_tracker_implementation() -&gt; tuple[Callable, Callable]:\n    \"\"\"\n    Determine which tracker implementation to use based on available system resources.\n\n    Returns:\n        tuple: A tuple containing (get_process_stats, get_system_stats) functions from the appropriate implementation module.\n\n    Raises:\n        ImportError: If no suitable implementation is available.\n    \"\"\"\n    if is_psutil_available():\n        from .tracker_psutil import get_process_stats, get_system_stats\n    elif is_procfs_available():\n        from .tracker_procfs import get_process_stats, get_system_stats\n    else:\n        raise ImportError(\n            \"No tracker implementation available - install psutil or use a Linux system with procfs.\"\n        )\n    return get_process_stats, get_system_stats\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.get_zfs_pools_space","title":"get_zfs_pools_space","text":"<pre><code>get_zfs_pools_space()\n</code></pre> <p>Get the space of ZFS pools.</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>def get_zfs_pools_space() -&gt; Dict[str, Dict[str, int]]:\n    \"\"\"\n    Get the space of ZFS pools.\n    \"\"\"\n    disks = {}\n    with suppress(FileNotFoundError, OSError):\n        zpool_process = Popen(\n            [\"zpool\", \"list\", \"-Hp\", \"-o\", \"name,size,allocated,free\"],\n            stdout=PIPE,\n        )\n        try:\n            stdout, _ = zpool_process.communicate(timeout=0.25)\n            if zpool_process.returncode == 0:\n                for line in stdout.splitlines():\n                    parts = line.decode().split(\"\\t\")\n                    if len(parts) &gt;= 4:\n                        disks[f\"zfs:{parts[0]}\"] = {\n                            \"total\": int(parts[1]),\n                            \"used\": int(parts[2]),\n                            \"free\": int(parts[3]),\n                        }\n        except TimeoutExpired:\n            zpool_process.kill()\n        except Exception:\n            pass\n    return disks\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.cleanup_files","title":"cleanup_files","text":"<pre><code>cleanup_files(files)\n</code></pre> <p>Cleanup files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[str]</code> <p>List of file paths to cleanup.</p> required Source code in <code>resource_tracker/helpers.py</code> <pre><code>def cleanup_files(files: List[str]):\n    \"\"\"Cleanup files.\n\n    Args:\n        files: List of file paths to cleanup.\n    \"\"\"\n    for f in files:\n        with suppress(Exception):\n            unlink(f)\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.cleanup_processes","title":"cleanup_processes","text":"<pre><code>cleanup_processes(processes)\n</code></pre> <p>Gracefully, then if needed forcefully terminate and close processes.</p> <p>Parameters:</p> Name Type Description Default <code>processes</code> <code>List[Process]</code> <p>List of <code>multiprocessing.Process</code> objects to cleanup.</p> required Source code in <code>resource_tracker/helpers.py</code> <pre><code>def cleanup_processes(processes: List[Process]):\n    \"\"\"Gracefully, then if needed forcefully terminate and close processes.\n\n    Args:\n        processes: List of `multiprocessing.Process` objects to cleanup.\n    \"\"\"\n    for process in processes:\n        with suppress(Exception):\n            if process.is_alive():\n                process.terminate()\n                process.join(timeout=1.0)\n        with suppress(Exception):\n            if process.is_alive():\n                process.kill()\n                process.join(timeout=1.0)\n        with suppress(Exception):\n            process.close()\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.aggregate_stats","title":"aggregate_stats","text":"<pre><code>aggregate_stats(stats)\n</code></pre> <p>Aggregate statistics from multiple sources.</p> <p>This function combines statistics from multiple runs or sources generated by resource_tracker.ResourceTracker.stats, handling different aggregation types appropriately:</p> <ul> <li>For <code>mean</code> and <code>duration</code> values: computes the average value across all sources.</li> <li>For <code>max</code> and <code>sum</code> values: takes the maximum value across all sources.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>stats</code> <code>List[Dict[str, Dict[str, Any]]]</code> <p>A list of dictionaries containing statistics. Each dictionary should    have column names as keys and dictionaries of aggregation types as values.</p> required <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>A dictionary with the same structure as the input dictionaries, but with</p> <code>Dict[str, Dict[str, Any]]</code> <p>aggregated values.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; stats1 = {'cpu_usage': {'mean': 1.5, 'max': 2.0}, 'memory': {'mean': 100, 'max': 150}}\n&gt;&gt;&gt; stats2 = {'cpu_usage': {'mean': 2.5, 'max': 3.0}, 'memory': {'mean': 200, 'max': 250}}\n&gt;&gt;&gt; aggregate_stats([stats1, stats2])\n{'cpu_usage': {'max': 3.0, 'mean': 2.0}, 'memory': {'max': 250, 'mean': 150.0}}\n</code></pre> Source code in <code>resource_tracker/helpers.py</code> <pre><code>def aggregate_stats(\n    stats: List[Dict[str, Dict[str, Any]]],\n) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"Aggregate statistics from multiple sources.\n\n    This function combines statistics from multiple runs or sources generated by\n    [resource_tracker.ResourceTracker.stats][], handling different aggregation\n    types appropriately:\n\n    - For `mean` and `duration` values: computes the average value across all sources.\n    - For `max` and `sum` values: takes the maximum value across all sources.\n\n    Args:\n        stats: A list of dictionaries containing statistics. Each dictionary should\n               have column names as keys and dictionaries of aggregation types as values.\n\n    Returns:\n        A dictionary with the same structure as the input dictionaries, but with\n        aggregated values.\n\n    Example:\n\n        &gt;&gt;&gt; stats1 = {'cpu_usage': {'mean': 1.5, 'max': 2.0}, 'memory': {'mean': 100, 'max': 150}}\n        &gt;&gt;&gt; stats2 = {'cpu_usage': {'mean': 2.5, 'max': 3.0}, 'memory': {'mean': 200, 'max': 250}}\n        &gt;&gt;&gt; aggregate_stats([stats1, stats2])\n        {'cpu_usage': {'max': 3.0, 'mean': 2.0}, 'memory': {'max': 250, 'mean': 150.0}}\n    \"\"\"\n    if not stats:\n        return {}\n\n    result = {}\n    for stats_dict in stats:\n        for col, values in stats_dict.items():\n            if col not in result:\n                result[col] = {}\n            for agg_type, value in values.items():\n                # collect values in a temp list to aggregate later\n                if agg_type in [\"mean\", \"duration\"]:\n                    if f\"{agg_type}_values\" not in result[col]:\n                        result[col][f\"{agg_type}_values\"] = []\n                    result[col][f\"{agg_type}_values\"].append(value)\n                # keep the largest value\n                elif agg_type in [\"max\", \"sum\"]:\n                    if agg_type not in result[col] or value &gt; result[col][agg_type]:\n                        result[col][agg_type] = value\n                else:\n                    raise ValueError(f\"Unsupported aggregation type: {agg_type}\")\n\n    # compute averages for mean and duration values\n    for col, values in result.items():\n        for agg_type in [\"mean\", \"duration\"]:\n            temp_key = f\"{agg_type}_values\"\n            if temp_key in values:\n                values[agg_type] = sum(values[temp_key]) / len(values[temp_key])\n                del values[temp_key]\n\n    return result\n</code></pre>"},{"location":"reference/resource_tracker/helpers/#resource_tracker.helpers.render_csv_row","title":"render_csv_row","text":"<pre><code>render_csv_row(row, quoting=QUOTE_NONNUMERIC)\n</code></pre> <p>Format a single CSV row as a string in memory.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Iterable[Union[str, float, int, None]]</code> <p>A list or iterable of values to write (strings, numbers, or None) with csv.writer.</p> required <code>quoting</code> <code>int</code> <p>Quoting strategy for the CSV writer. Defaults to QUOTE_NONNUMERIC.</p> <code>QUOTE_NONNUMERIC</code> <p>Returns:</p> Type Description <code>bytes</code> <p>A bytes object representing the full CSV-formatted row (including newline).</p> Source code in <code>resource_tracker/helpers.py</code> <pre><code>def render_csv_row(\n    row: Iterable[Union[str, float, int, None]], quoting: int = QUOTE_NONNUMERIC\n) -&gt; bytes:\n    \"\"\"\n    Format a single CSV row as a string in memory.\n\n    Args:\n        row: A list or iterable of values to write (strings, numbers, or None) with [csv.writer][].\n        quoting: Quoting strategy for the CSV writer. Defaults to QUOTE_NONNUMERIC.\n\n    Returns:\n        A bytes object representing the full CSV-formatted row (including newline).\n    \"\"\"\n    buffer = StringIO(newline=\"\")\n    writer = csv_writer(buffer, quoting=quoting, lineterminator=\"\\n\")\n    writer.writerow(row)\n    return buffer.getvalue().encode(\"utf-8\")\n</code></pre>"},{"location":"reference/resource_tracker/keeper/","title":"resource_tracker.keeper","text":""},{"location":"reference/resource_tracker/keeper/#resource_tracker.keeper","title":"resource_tracker.keeper","text":"<p>Functions:</p> Name Description <code>keeper_request</code> <p>Fetch data from a SC Keeper URL with a custom header.</p> <code>get_instance_price</code> <p>Get the on-demand price for a specific instance type in a region.</p> <code>get_recommended_cloud_servers</code> <p>Get the cheapest cloud servers for the given resources from Spare Cores.</p>"},{"location":"reference/resource_tracker/keeper/#resource_tracker.keeper.keeper_request","title":"keeper_request","text":"<pre><code>keeper_request(path, timeout=2, endpoint='https://keeper.sparecores.net')\n</code></pre> <p>Fetch data from a SC Keeper URL with a custom header.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to fetch data from.</p> required <code>timeout</code> <code>int</code> <p>The timeout for the request.</p> <code>2</code> <code>endpoint</code> <code>str</code> <p>The endpoint to fetch data from.</p> <code>'https://keeper.sparecores.net'</code> <p>Returns:</p> Type Description <code>Optional[dict]</code> <p>The JSON-decoded response data, or None if an error occurs.</p> Source code in <code>resource_tracker/keeper.py</code> <pre><code>def keeper_request(\n    path: str, timeout: int = 2, endpoint: str = \"https://keeper.sparecores.net\"\n) -&gt; Optional[dict]:\n    \"\"\"Fetch data from a SC Keeper URL with a custom header.\n\n    Args:\n        path: The path to fetch data from.\n        timeout: The timeout for the request.\n        endpoint: The endpoint to fetch data from.\n\n    Returns:\n        The JSON-decoded response data, or None if an error occurs.\n    \"\"\"\n    try:\n        request = Request(urljoin(endpoint, path))\n        request.add_header(\"X-Application-ID\", \"resource-tracker\")\n        with urlopen(request, timeout=timeout) as response:\n            return loads(response.read().decode(\"utf-8\"))\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/resource_tracker/keeper/#resource_tracker.keeper.get_instance_price","title":"get_instance_price","text":"<pre><code>get_instance_price(vendor_id, region_id, instance_type)\n</code></pre> <p>Get the on-demand price for a specific instance type in a region.</p> <p>Parameters:</p> Name Type Description Default <code>vendor_id</code> <p>The ID of the vendor (e.g. \"aws\", \"azure\", \"gcp\")</p> required <code>region_id</code> <p>The ID of the region (e.g. \"us-east-1\", \"us-west-2\")</p> required <code>instance_type</code> <p>The type of instance (e.g. \"t3.micro\", \"m5.large\")</p> required <p>Returns:</p> Type Description <code>Optional[float]</code> <p>The on-demand price for the instance type in the region, or None if no price is found.</p> Source code in <code>resource_tracker/keeper.py</code> <pre><code>def get_instance_price(vendor_id, region_id, instance_type) -&gt; Optional[float]:\n    \"\"\"Get the on-demand price for a specific instance type in a region.\n\n    Args:\n        vendor_id: The ID of the vendor (e.g. \"aws\", \"azure\", \"gcp\")\n        region_id: The ID of the region (e.g. \"us-east-1\", \"us-west-2\")\n        instance_type: The type of instance (e.g. \"t3.micro\", \"m5.large\")\n\n    Returns:\n        The on-demand price for the instance type in the region, or None if no price is found.\n    \"\"\"\n    try:\n        pricing_data = keeper_request(f\"/server/{vendor_id}/{instance_type}/prices\")\n\n        for item in pricing_data:\n            if (\n                item.get(\"region_id\") == region_id\n                and item.get(\"allocation\") == \"ondemand\"\n                and item.get(\"operating_system\") == \"Linux\"\n            ):\n                return item.get(\"price\")\n\n        # fallback to the first on-demand price in other regions\n        for item in pricing_data:\n            if (\n                item.get(\"allocation\") == \"ondemand\"\n                and item.get(\"operating_system\") == \"Linux\"\n            ):\n                return item.get(\"price\")\n\n        return None\n    except Exception:\n        return None\n</code></pre>"},{"location":"reference/resource_tracker/keeper/#resource_tracker.keeper.get_recommended_cloud_servers","title":"get_recommended_cloud_servers","text":"<pre><code>get_recommended_cloud_servers(cpu, memory, gpu=None, vram=None, n=10)\n</code></pre> <p>Get the cheapest cloud servers for the given resources from Spare Cores.</p> <p>Parameters:</p> Name Type Description Default <code>cpu</code> <code>int</code> <p>The minimum number of vCPUs.</p> required <code>memory</code> <code>int</code> <p>The minimum amount of memory in MB.</p> required <code>gpu</code> <code>Optional[int]</code> <p>The minimum number of GPUs.</p> <code>None</code> <code>vram</code> <code>Optional[int]</code> <p>The minimum amount of VRAM in GB.</p> <code>None</code> <code>n</code> <code>int</code> <p>The number of recommended servers to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>A list of recommended server configurations ordered by price.</p> References <ul> <li>https://sparecores.com/servers</li> </ul> Source code in <code>resource_tracker/keeper.py</code> <pre><code>def get_recommended_cloud_servers(\n    cpu: int,\n    memory: int,\n    gpu: Optional[int] = None,\n    vram: Optional[int] = None,\n    n: int = 10,\n) -&gt; List[Dict]:\n    \"\"\"Get the cheapest cloud servers for the given resources from Spare Cores.\n\n    Args:\n        cpu: The minimum number of vCPUs.\n        memory: The minimum amount of memory in MB.\n        gpu: The minimum number of GPUs.\n        vram: The minimum amount of VRAM in GB.\n        n: The number of recommended servers to return.\n\n    Returns:\n        A list of recommended server configurations ordered by price.\n\n    References:\n        - https://sparecores.com/servers\n    \"\"\"\n    try:\n        params = {\n            \"vcpus_min\": cpu,\n            \"memory_min\": round(memory / 1024),  # convert MiB to GiB\n            \"order_by\": \"min_price_ondemand\",\n            \"order_dir\": \"asc\",\n            \"limit\": n,\n        }\n        if gpu and gpu &gt; 0:\n            params[\"gpu_min\"] = gpu\n        if vram and vram &gt; 0:\n            params[\"gpu_memory_total\"] = vram\n        return keeper_request(f\"/servers?{urlencode(params)}\")\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/resource_tracker/nvidia/","title":"resource_tracker.nvidia","text":""},{"location":"reference/resource_tracker/nvidia/#resource_tracker.nvidia","title":"resource_tracker.nvidia","text":"<p>Helpers to monitor NVIDIA GPUs.</p> <p>Functions:</p> Name Description <code>start_nvidia_smi_pmon</code> <p>Start a subprocess to monitor NVIDIA GPUs at the process level using <code>nvidia-smi pmon</code>.</p> <code>process_nvidia_smi_pmon</code> <p>Wait for the <code>nvidia-smi pmon</code> subprocess to finish and process the output.</p> <code>start_nvidia_smi</code> <p>Start a subprocess to monitor NVIDIA GPUs' utilization and memory usage using <code>nvidia-smi</code>.</p> <code>process_nvidia_smi</code> <p>Wait for the <code>nvidia-smi</code> subprocess to finish and process the output.</p>"},{"location":"reference/resource_tracker/nvidia/#resource_tracker.nvidia.start_nvidia_smi_pmon","title":"start_nvidia_smi_pmon","text":"<pre><code>start_nvidia_smi_pmon()\n</code></pre> <p>Start a subprocess to monitor NVIDIA GPUs at the process level using <code>nvidia-smi pmon</code>.</p> <p>Note that <code>nvidia-smi pmon</code> is limited to monitoring max. 4 GPUs.</p> <p>Returns:</p> Type Description <code>Optional[Popen]</code> <p>The subprocess object or None if nvidia-smi is not installed.</p> Source code in <code>resource_tracker/nvidia.py</code> <pre><code>def start_nvidia_smi_pmon() -&gt; Optional[Popen]:\n    \"\"\"Start a subprocess to monitor NVIDIA GPUs at the process level using `nvidia-smi pmon`.\n\n    Note that `nvidia-smi pmon` is limited to monitoring max. 4 GPUs.\n\n    Returns:\n        The subprocess object or None if nvidia-smi is not installed.\n    \"\"\"\n    with suppress(FileNotFoundError):\n        return Popen(\n            [\"nvidia-smi\", \"pmon\", \"-c\", \"1\", \"-s\", \"um\", \"-d\", \"1\"],\n            stdout=PIPE,\n        )\n</code></pre>"},{"location":"reference/resource_tracker/nvidia/#resource_tracker.nvidia.process_nvidia_smi_pmon","title":"process_nvidia_smi_pmon","text":"<pre><code>process_nvidia_smi_pmon(nvidia_process, pids=None)\n</code></pre> <p>Wait for the <code>nvidia-smi pmon</code> subprocess to finish and process the output.</p> <p>Parameters:</p> Name Type Description Default <code>nvidia_process</code> <code>Optional[Popen]</code> <p>The subprocess object to monitor or None if not started. Returned by <code>start_nvidia_smi_pmon</code>.</p> required <code>pids</code> <code>Optional[Set[int]]</code> <p>A set of process IDs to monitor. If None, all processes are monitored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float, Set[int]]]</code> <p>A dictionary of GPU stats:</p> <ul> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory/VRAM used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> <li>gpu_utilized_indexes (set[int]): The set of GPU indexes with utilization &gt; 0.</li> </ul> Source code in <code>resource_tracker/nvidia.py</code> <pre><code>def process_nvidia_smi_pmon(\n    nvidia_process: Optional[Popen], pids: Optional[Set[int]] = None\n) -&gt; Dict[str, Union[int, float, Set[int]]]:\n    \"\"\"Wait for the `nvidia-smi pmon` subprocess to finish and process the output.\n\n    Args:\n        nvidia_process: The subprocess object to monitor or None if not started.\n          Returned by `start_nvidia_smi_pmon`.\n        pids: A set of process IDs to monitor. If None, all processes are monitored.\n\n    Returns:\n        A dictionary of GPU stats:\n\n            - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n            - gpu_vram (float): The current GPU memory/VRAM used in MiB.\n            - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n            - gpu_utilized_indexes (set[int]): The set of GPU indexes with utilization &gt; 0.\n    \"\"\"\n    gpu_stats = {\n        \"gpu_usage\": 0,  # between 0 and GPU count\n        \"gpu_vram\": 0,  # MiB\n        \"gpu_utilized\": 0,  # number of GPUs with utilization &gt; 0\n        \"gpu_utilized_indexes\": set(),  # set of GPU indexes\n    }\n    try:\n        stdout, _ = nvidia_process.communicate(timeout=0.5)\n        if nvidia_process.returncode == 0:\n            for index, line in enumerate(stdout.splitlines()):\n                if index &lt; 2:\n                    continue  # skip the header lines\n                parts = line.decode().split()\n                # skip unmonitored processes\n                if pids is None or int(parts[1]) in pids:\n                    usage = 0\n                    if parts[3] != \"-\":  # sm%\n                        usage = float(parts[3])\n                        gpu_stats[\"gpu_utilized_indexes\"].add(int(parts[0]))\n                    gpu_stats[\"gpu_usage\"] += usage / 100\n                    gpu_stats[\"gpu_vram\"] += float(parts[9])\n            gpu_stats[\"gpu_utilized\"] = len(gpu_stats[\"gpu_utilized_indexes\"])\n    except TimeoutExpired:\n        nvidia_process.kill()\n    except Exception:\n        pass\n    return gpu_stats\n</code></pre>"},{"location":"reference/resource_tracker/nvidia/#resource_tracker.nvidia.start_nvidia_smi","title":"start_nvidia_smi","text":"<pre><code>start_nvidia_smi()\n</code></pre> <p>Start a subprocess to monitor NVIDIA GPUs' utilization and memory usage using <code>nvidia-smi</code>.</p> <p>Returns:</p> Type Description <code>Optional[Popen]</code> <p>The subprocess object or None if nvidia-smi is not installed.</p> Source code in <code>resource_tracker/nvidia.py</code> <pre><code>def start_nvidia_smi() -&gt; Optional[Popen]:\n    \"\"\"Start a subprocess to monitor NVIDIA GPUs' utilization and memory usage using `nvidia-smi`.\n\n    Returns:\n        The subprocess object or None if nvidia-smi is not installed.\n    \"\"\"\n    with suppress(FileNotFoundError):\n        return Popen(\n            [\"nvidia-smi\", \"--query-gpu=utilization.gpu,memory.used\", \"--format=csv\"],\n            stdout=PIPE,\n        )\n</code></pre>"},{"location":"reference/resource_tracker/nvidia/#resource_tracker.nvidia.process_nvidia_smi","title":"process_nvidia_smi","text":"<pre><code>process_nvidia_smi(nvidia_process)\n</code></pre> <p>Wait for the <code>nvidia-smi</code> subprocess to finish and process the output.</p> <p>Parameters:</p> Name Type Description Default <code>nvidia_process</code> <code>Optional[Popen]</code> <p>The subprocess object to monitor or None if not started. Returned by <code>start_nvidia_smi</code>.</p> required <p>Returns:</p> Type Description <code>Dict[str, Union[int, float]]</code> <p>A dictionary of GPU stats:</p> <ul> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory/VRAM used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> Source code in <code>resource_tracker/nvidia.py</code> <pre><code>def process_nvidia_smi(nvidia_process: Optional[Popen]) -&gt; Dict[str, Union[int, float]]:\n    \"\"\"Wait for the `nvidia-smi` subprocess to finish and process the output.\n\n    Args:\n        nvidia_process: The subprocess object to monitor or None if not started.\n          Returned by `start_nvidia_smi`.\n\n    Returns:\n        A dictionary of GPU stats:\n\n            - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n            - gpu_vram (float): The current GPU memory/VRAM used in MiB.\n            - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n    \"\"\"\n    gpu_stats = {\n        \"gpu_usage\": 0,  # between 0 and GPU count\n        \"gpu_vram\": 0,  # MiB\n        \"gpu_utilized\": 0,  # number of GPUs with utilization &gt; 0\n    }\n    try:\n        stdout, _ = nvidia_process.communicate(timeout=0.5)\n        if nvidia_process.returncode == 0:\n            for index, line in enumerate(stdout.splitlines()):\n                if index == 0:\n                    continue  # skip the header\n                parts = line.decode().split(\", \")\n                if len(parts) == 2:\n                    usage = float(parts[0].rstrip(\" %\"))\n                    gpu_stats[\"gpu_usage\"] += usage / 100\n                    gpu_stats[\"gpu_vram\"] += float(parts[1].rstrip(\" MiB\"))\n                    gpu_stats[\"gpu_utilized\"] += usage &gt; 0\n    except TimeoutExpired:\n        nvidia_process.kill()\n    except Exception:\n        pass\n    return gpu_stats\n</code></pre>"},{"location":"reference/resource_tracker/report/","title":"resource_tracker.report","text":""},{"location":"reference/resource_tracker/report/#resource_tracker.report","title":"resource_tracker.report","text":"<p>Classes:</p> Name Description <code>Report</code> <p>A string subclass representing an HTML report with methods to view and save it.</p> <p>Functions:</p> Name Description <code>round_memory</code> <p>Round a number to the nearest meaningful memory amount.</p>"},{"location":"reference/resource_tracker/report/#resource_tracker.report.Report","title":"Report","text":"<p>               Bases: <code>str</code></p> <p>A string subclass representing an HTML report with methods to view and save it.</p> <p>Methods:</p> Name Description <code>browse</code> <p>Open the report in the default web browser.</p> <code>save</code> <p>Save the report to a file.</p> Source code in <code>resource_tracker/report.py</code> <pre><code>class Report(str):\n    \"\"\"A string subclass representing an HTML report with methods to view and save it.\"\"\"\n\n    def browse(self):\n        \"\"\"Open the report in the default web browser.\n\n        Creates a temporary HTML file and opens it in the default web browser.\n\n        Returns:\n            self: Returns the Report object for method chaining\n        \"\"\"\n        with tempfile.NamedTemporaryFile(suffix=\".html\", delete=False) as f:\n            f.write(self.encode(\"utf-8\"))\n            temp_path = f.name\n\n        webbrowser.open(\"file://\" + temp_path)\n        return self\n\n    def save(self, filepath):\n        \"\"\"Save the report to a file.\n\n        Args:\n            filepath: The path where to save the HTML report\n\n        Returns:\n            self: Returns the Report object for method chaining\n        \"\"\"\n        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n            f.write(self)\n        return self\n</code></pre>"},{"location":"reference/resource_tracker/report/#resource_tracker.report.Report.browse","title":"browse","text":"<pre><code>browse()\n</code></pre> <p>Open the report in the default web browser.</p> <p>Creates a temporary HTML file and opens it in the default web browser.</p> <p>Returns:</p> Name Type Description <code>self</code> <p>Returns the Report object for method chaining</p> Source code in <code>resource_tracker/report.py</code> <pre><code>def browse(self):\n    \"\"\"Open the report in the default web browser.\n\n    Creates a temporary HTML file and opens it in the default web browser.\n\n    Returns:\n        self: Returns the Report object for method chaining\n    \"\"\"\n    with tempfile.NamedTemporaryFile(suffix=\".html\", delete=False) as f:\n        f.write(self.encode(\"utf-8\"))\n        temp_path = f.name\n\n    webbrowser.open(\"file://\" + temp_path)\n    return self\n</code></pre>"},{"location":"reference/resource_tracker/report/#resource_tracker.report.Report.save","title":"save","text":"<pre><code>save(filepath)\n</code></pre> <p>Save the report to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <p>The path where to save the HTML report</p> required <p>Returns:</p> Name Type Description <code>self</code> <p>Returns the Report object for method chaining</p> Source code in <code>resource_tracker/report.py</code> <pre><code>def save(self, filepath):\n    \"\"\"Save the report to a file.\n\n    Args:\n        filepath: The path where to save the HTML report\n\n    Returns:\n        self: Returns the Report object for method chaining\n    \"\"\"\n    with open(filepath, \"w\", encoding=\"utf-8\") as f:\n        f.write(self)\n    return self\n</code></pre>"},{"location":"reference/resource_tracker/report/#resource_tracker.report.round_memory","title":"round_memory","text":"<pre><code>round_memory(mb)\n</code></pre> <p>Round a number to the nearest meaningful memory amount.</p> <p>Parameters:</p> Name Type Description Default <code>mb</code> <code>Union[int, float]</code> <p>The value in MB to round.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The rounded value in MB as an integer.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; round_memory(68)\n128\n&gt;&gt;&gt; round_memory(896)\n1024\n&gt;&gt;&gt; round_memory(3863)\n4096\n</code></pre> Source code in <code>resource_tracker/report.py</code> <pre><code>def round_memory(mb: Union[int, float]) -&gt; int:\n    \"\"\"Round a number to the nearest meaningful memory amount.\n\n    Args:\n        mb: The value in MB to round.\n\n    Returns:\n        The rounded value in MB as an integer.\n\n    Example:\n\n        &gt;&gt;&gt; round_memory(68)\n        128\n        &gt;&gt;&gt; round_memory(896)\n        1024\n        &gt;&gt;&gt; round_memory(3863)\n        4096\n    \"\"\"\n    if mb &lt;= 128:\n        rounded = 128\n    elif mb &lt;= 256:\n        rounded = 256\n    elif mb &lt;= 512:\n        rounded = 512\n    elif mb &lt;= 1024:\n        rounded = 1024\n    elif mb &lt;= 2048:\n        rounded = 2048\n    else:\n        # round up to the next GB\n        rounded_gb = mb / 1024\n        rounded = int(1024 * (rounded_gb // 1 + (1 if rounded_gb % 1 &gt; 0 else 0)))\n    return rounded\n</code></pre>"},{"location":"reference/resource_tracker/server_info/","title":"resource_tracker.server_info","text":""},{"location":"reference/resource_tracker/server_info/#resource_tracker.server_info","title":"resource_tracker.server_info","text":"<p>Detect server hardware (CPU count, memory amount, disk space, GPU count and VRAM amount) via <code>procfs</code> or <code>psutil</code>, and <code>nvidia-smi</code>.</p> <p>Functions:</p> Name Description <code>get_total_memory_mb</code> <p>Get total system memory in MB from <code>/proc/meminfo</code> or using <code>psutil</code>.</p> <code>get_gpu_info</code> <p>Get GPU information using <code>nvidia-smi</code> command.</p> <code>get_server_info</code> <p>Collects important information about the Linux server.</p>"},{"location":"reference/resource_tracker/server_info/#resource_tracker.server_info.get_total_memory_mb","title":"get_total_memory_mb","text":"<pre><code>get_total_memory_mb()\n</code></pre> <p>Get total system memory in MB from <code>/proc/meminfo</code> or using <code>psutil</code>.</p> Source code in <code>resource_tracker/server_info.py</code> <pre><code>def get_total_memory_mb() -&gt; float:\n    \"\"\"Get total system memory in MB from `/proc/meminfo` or using `psutil`.\"\"\"\n    with suppress(Exception):\n        with open(\"/proc/meminfo\", \"r\") as f:\n            for line in f:\n                if \"MemTotal\" in line:\n                    parts = line.split(\":\")\n                    kb = int(parts[1].strip().split()[0])\n                    return round(kb / (1024), 2)\n    with suppress(Exception):\n        from psutil import virtual_memory\n\n        return round(virtual_memory().total / (1024**2), 2)\n    return 0\n</code></pre>"},{"location":"reference/resource_tracker/server_info/#resource_tracker.server_info.get_gpu_info","title":"get_gpu_info","text":"<pre><code>get_gpu_info()\n</code></pre> <p>Get GPU information using <code>nvidia-smi</code> command.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing GPU information:</p> <ul> <li><code>count</code>: Number of GPUs</li> <li><code>memory_mb</code>: Total VRAM in MB</li> <li><code>gpu_names</code>: List of GPU names</li> </ul> Source code in <code>resource_tracker/server_info.py</code> <pre><code>def get_gpu_info() -&gt; dict:\n    \"\"\"Get GPU information using `nvidia-smi` command.\n\n    Returns:\n        A dictionary containing GPU information:\n\n            - `count`: Number of GPUs\n            - `memory_mb`: Total VRAM in MB\n            - `gpu_names`: List of GPU names\n    \"\"\"\n    result = {\"count\": 0, \"memory_mb\": 0, \"gpu_names\": []}\n\n    with suppress(Exception):\n        nvidia_smi_output = check_output(\n            [\n                \"nvidia-smi\",\n                \"--query-gpu=gpu_name,memory.total\",\n                \"--format=csv,noheader,nounits\",\n            ],\n            universal_newlines=True,\n        )\n\n        lines = nvidia_smi_output.strip().split(\"\\n\")\n        result[\"count\"] = len(lines)\n\n        total_memory_mb = 0\n        for line in lines:\n            if line.strip():\n                parts = line.split(\",\")\n                memory_mb = float(parts[1].strip())\n                total_memory_mb += memory_mb\n                result[\"gpu_names\"].append(parts[0].strip())\n\n        result[\"memory_mb\"] = total_memory_mb\n\n    return result\n</code></pre>"},{"location":"reference/resource_tracker/server_info/#resource_tracker.server_info.get_server_info","title":"get_server_info","text":"<pre><code>get_server_info()\n</code></pre> <p>Collects important information about the Linux server.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing server information:</p> <ul> <li><code>os</code>: Operating system</li> <li><code>vcpus</code>: Number of virtual CPUs</li> <li><code>memory_mb</code>: Total memory in MB</li> <li><code>gpu_count</code>: Number of GPUs (<code>0</code> if not available)</li> <li><code>gpu_names</code>: List of GPU names (<code>[]</code> if not available)</li> <li><code>gpu_name</code>: Most common GPU name (<code>\"\"</code> if not available)</li> <li><code>gpu_memory_mb</code>: Total VRAM in MB (<code>0</code> if not available)</li> </ul> Source code in <code>resource_tracker/server_info.py</code> <pre><code>def get_server_info() -&gt; dict:\n    \"\"\"\n    Collects important information about the Linux server.\n\n    Returns:\n        A dictionary containing server information:\n\n            - `os`: Operating system\n            - `vcpus`: Number of virtual CPUs\n            - `memory_mb`: Total memory in MB\n            - `gpu_count`: Number of GPUs (`0` if not available)\n            - `gpu_names`: List of GPU names (`[]` if not available)\n            - `gpu_name`: Most common GPU name (`\"\"` if not available)\n            - `gpu_memory_mb`: Total VRAM in MB (`0` if not available)\n    \"\"\"\n    gpu_info = get_gpu_info()\n    info = {\n        \"os\": system(),\n        \"vcpus\": cpu_count(),\n        \"memory_mb\": get_total_memory_mb(),\n        \"gpu_count\": gpu_info[\"count\"],\n        \"gpu_names\": gpu_info[\"gpu_names\"],\n        \"gpu_name\": (\n            Counter(gpu_info[\"gpu_names\"]).most_common(1)[0][0]\n            if gpu_info[\"gpu_names\"]\n            else \"\"\n        ),\n        \"gpu_memory_mb\": gpu_info[\"memory_mb\"],\n    }\n    return info\n</code></pre>"},{"location":"reference/resource_tracker/tiny_bars/","title":"resource_tracker.tiny_bars","text":""},{"location":"reference/resource_tracker/tiny_bars/#resource_tracker.tiny_bars","title":"resource_tracker.tiny_bars","text":"<p>A tiny, partial, and opinionated implementation of the Handlebars template engine.</p> <p>Functions:</p> Name Description <code>render_template</code> <p>Render a Handlebars-like template using a dictionary context.</p>"},{"location":"reference/resource_tracker/tiny_bars/#resource_tracker.tiny_bars.render_template","title":"render_template","text":"<pre><code>render_template(template, context)\n</code></pre> <p>Render a Handlebars-like template using a dictionary context.</p> <p>Supported features:</p> <ul> <li>Conditional flow using <code>{{#if expr}} ... {{#else}} ... {{/if}}</code></li> <li>Iteration using <code>{{#each expr as item}} ... {{/each}}</code></li> <li>Variable interpolation using <code>{{expr}}</code> (HTML-escaped) and <code>{{{expr}}}</code> (raw)</li> <li>Nested property access using dot notation (e.g. <code>user.name</code>) for dictionary keys and object attributes.</li> <li>Filters using pipe syntax: <code>{{expr | filter}}</code> or <code>{{expr | filter:param}}</code></li> </ul> <p>Parameters:</p> Name Type Description Default <code>template</code> <code>str</code> <p>The template to render.</p> required <code>context</code> <code>Dict[str, Any]</code> <p>The context to render the template with.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The rendered text.</p> <p>Example:</p> <pre><code>&gt;&gt;&gt; from resource_tracker.tiny_bars import render_template\n&gt;&gt;&gt; render_template(\"Hello, {{name}}!\", {\"name\": \"World\"})\n'Hello, World!'\n&gt;&gt;&gt; render_template(\"{{#each names as name}}Hello, {{name}}! {{/each}}\", {\"names\": [\"Foo\", \"Bar\"]})\n'Hello, Foo! Hello, Bar! '\n&gt;&gt;&gt; render_template(\"Odd numbers: {{#each numbers as number}}{{ #if number.odd}}{{number.value}} {{/if}}{{/each}}\", {\"numbers\": [{\"value\": i, \"odd\": i % 2 == 1} for i in range(10)]})\n'Odd numbers: 1 3 5 7 9 '\n&gt;&gt;&gt; render_template(\"{{#if present}}Yes{{/if}}\", {\"present\": True})\n'Yes'\n&gt;&gt;&gt; render_template(\"{{#if present}}Yes{{/if}}\", {\"present\": False})\n''\n&gt;&gt;&gt; render_template(\"{{#if present}}Yes{{#else}}No{{/if}}\", {\"present\": False})\n'No'\n&gt;&gt;&gt; render_template(\"{{value | pretty_number}}\", {\"value\": 1234.5678})\n'1,235'\n&gt;&gt;&gt; render_template(\"{{value | pretty_number:2}}\", {\"value\": 1234.5678})\n'1,234.57'\n&gt;&gt;&gt; render_template(\"{{value | divide:1000}}\", {\"value\": 1234567})\n'1234.567'\n</code></pre> Source code in <code>resource_tracker/tiny_bars.py</code> <pre><code>def render_template(template: str, context: Dict[str, Any]) -&gt; str:\n    \"\"\"Render a Handlebars-like template using a dictionary context.\n\n    Supported features:\n\n    - Conditional flow using `{{#if expr}} ... {{#else}} ... {{/if}}`\n    - Iteration using `{{#each expr as item}} ... {{/each}}`\n    - Variable interpolation using `{{expr}}` (HTML-escaped) and `{{{expr}}}` (raw)\n    - Nested property access using dot notation (e.g. `user.name`) for dictionary keys and object attributes.\n    - Filters using pipe syntax: `{{expr | filter}}` or `{{expr | filter:param}}`\n\n    Args:\n        template: The template to render.\n        context: The context to render the template with.\n\n    Returns:\n        The rendered text.\n\n    Example:\n\n        &gt;&gt;&gt; from resource_tracker.tiny_bars import render_template\n        &gt;&gt;&gt; render_template(\"Hello, {{name}}!\", {\"name\": \"World\"})\n        'Hello, World!'\n        &gt;&gt;&gt; render_template(\"{{#each names as name}}Hello, {{name}}! {{/each}}\", {\"names\": [\"Foo\", \"Bar\"]})\n        'Hello, Foo! Hello, Bar! '\n        &gt;&gt;&gt; render_template(\"Odd numbers: {{#each numbers as number}}{{ #if number.odd}}{{number.value}} {{/if}}{{/each}}\", {\"numbers\": [{\"value\": i, \"odd\": i % 2 == 1} for i in range(10)]})\n        'Odd numbers: 1 3 5 7 9 '\n        &gt;&gt;&gt; render_template(\"{{#if present}}Yes{{/if}}\", {\"present\": True})\n        'Yes'\n        &gt;&gt;&gt; render_template(\"{{#if present}}Yes{{/if}}\", {\"present\": False})\n        ''\n        &gt;&gt;&gt; render_template(\"{{#if present}}Yes{{#else}}No{{/if}}\", {\"present\": False})\n        'No'\n        &gt;&gt;&gt; render_template(\"{{value | pretty_number}}\", {\"value\": 1234.5678})\n        '1,235'\n        &gt;&gt;&gt; render_template(\"{{value | pretty_number:2}}\", {\"value\": 1234.5678})\n        '1,234.57'\n        &gt;&gt;&gt; render_template(\"{{value | divide:1000}}\", {\"value\": 1234567})\n        '1234.567'\n    \"\"\"\n\n    def _render_block(tmpl: str, ctx: Dict[str, Any]) -&gt; str:\n        pos = 0\n        output = []\n\n        while pos &lt; len(tmpl):\n            m_triple = TRIPLE_RE.search(tmpl, pos)\n            m_double = DOUBLE_RE.search(tmpl, pos)\n\n            if not m_triple and not m_double:\n                output.append(tmpl[pos:])\n                break\n\n            # triple braces are processed first: outputs raw value\n            if m_triple and (not m_double or m_triple.start() &lt; m_double.start()):\n                output.append(tmpl[pos : m_triple.start()])\n                expr = m_triple.group(1)\n                try:\n                    val = _resolve_var(expr, ctx)\n                    if val is not None:\n                        output.append(str(val))\n                except Exception as e:\n                    output.append(f\"[Error: {expr} - {str(e)}]\")\n                pos = m_triple.end()\n\n            # double braces: control flow, iteration, HTML-escaped output\n            else:\n                output.append(tmpl[pos : m_double.start()])\n                tag, expr = m_double.groups()\n                pos = m_double.end()\n\n                if tag == \"#if\":\n                    if_block, else_block, new_pos = _find_if_else_blocks(tmpl, pos)\n                    try:\n                        try:\n                            condition_met = bool(_resolve_var(expr, ctx))\n                        except Exception:\n                            condition_met = False\n                        if condition_met:\n                            output.append(_render_block(if_block, ctx))\n                        elif else_block is not None:\n                            output.append(_render_block(else_block, ctx))\n                    except Exception as e:\n                        output.append(f\"[Error evaluating if: {expr} - {str(e)}]\")\n                    pos = new_pos\n\n                elif tag == \"#each\":\n                    inner, new_pos = _find_matching_block(tmpl, pos, \"#each\", \"/each\")\n\n                    each_match = EACH_RE.match(expr)\n                    if each_match:\n                        collection_expr, item_var = each_match.groups()\n                        collection_expr = collection_expr.strip()\n                        item_var = item_var.strip()\n                    else:\n                        output.append(\n                            \"[Error: Invalid #each syntax. Use '{#each expr as item}']\"\n                        )\n                        pos = new_pos\n                        continue\n\n                    items = _resolve_var(collection_expr, ctx)\n                    if isinstance(items, list):\n                        for item in items:\n                            item_ctx = ctx.copy()\n                            item_ctx[item_var] = item\n                            output.append(_render_block(inner, item_ctx))\n                    else:\n                        output.append(f\"[Error: {collection_expr} is not a list]\")\n                    pos = new_pos\n\n                # double braces outputs value after HTML-escaping\n                elif tag is None:\n                    try:\n                        val = _resolve_var(expr, ctx)\n                        if val is not None:\n                            output.append(escape(str(val), quote=True))\n                    except Exception as e:\n                        output.append(f\"[Error: {expr} - {str(e)}]\")\n\n        return \"\".join(output)\n\n    def _find_matching_block(\n        tmpl: str, start_pos: int, open_tag: str, close_tag: str\n    ) -&gt; Tuple[str, int]:\n        depth = 1\n        search_pos = start_pos\n        while depth &gt; 0:\n            m = DOUBLE_RE.search(tmpl, search_pos)\n            if not m:\n                raise ValueError(f\"Unclosed tag: {open_tag}\")\n            tag_type, _ = m.groups()\n            if tag_type == open_tag:\n                depth += 1\n            elif tag_type == close_tag:\n                depth -= 1\n            search_pos = m.end()\n        return tmpl[start_pos : m.start()], m.end()\n\n    def _find_if_else_blocks(tmpl: str, start_pos: int) -&gt; Tuple[str, str, int]:\n        \"\"\"Find the if and else blocks in an if statement.\n\n        Returns:\n            A tuple of (if_block, else_block, end_position) where else_block may\n            be None if there is no else block.\n        \"\"\"\n        depth = 1\n        search_pos = start_pos\n        else_pos = None\n\n        while depth &gt; 0:\n            m = DOUBLE_RE.search(tmpl, search_pos)\n            if not m:\n                raise ValueError(\"Unclosed #if tag\")\n\n            tag_type, _ = m.groups()\n\n            if tag_type == \"#if\":\n                depth += 1\n            elif tag_type == \"/if\":\n                depth -= 1\n            elif tag_type == \"#else\" and depth == 1:\n                # only consider #else at the same depth as our starting #if\n                else_pos = m.start()\n\n            search_pos = m.end()\n\n        end_pos = m.end()\n\n        if else_pos is not None:\n            if_block = tmpl[start_pos:else_pos]\n            else_start = DOUBLE_RE.search(tmpl, else_pos).end()\n            else_block = tmpl[else_start : m.start()]\n            return if_block, else_block, end_pos\n        else:\n            return tmpl[start_pos : m.start()], None, end_pos\n\n    return _render_block(template, context.copy())\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/","title":"resource_tracker.tiny_data_frame","text":""},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame","title":"resource_tracker.tiny_data_frame","text":"<p>A very inefficient data-frame implementation for manipulating resource usage data.</p> <p>If you don't like this helper class, grab the <code>_data</code> instance attribute that is a list of lists (column vectors) and do whatever you want with it.</p> <p>Classes:</p> Name Description <code>StatSpec</code> <p>Specification of a statistic to collect on a column.</p> <code>TinyDataFrame</code> <p>A very inefficient data-frame implementation with a few features.</p>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.StatSpec","title":"StatSpec","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Specification of a statistic to collect on a column.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <p>The name of the column to apply the agg function to.</p> required <code>agg</code> <p>The aggregation function to apply to the column.</p> required <code>agg_name</code> <p>The name of the field to store the statistic in. Defaults to None, which means using the name of the agg function.</p> required <code>round</code> <p>The number of decimal places to round the statistic to. Defaults to None, which means no rounding.</p> required Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>class StatSpec(NamedTuple):\n    \"\"\"Specification of a statistic to collect on a column.\n\n    Args:\n        column: The name of the column to apply the agg function to.\n        agg: The aggregation function to apply to the column.\n        agg_name: The name of the field to store the statistic in. Defaults to None, which means using the name of the agg function.\n        round: The number of decimal places to round the statistic to. Defaults to None, which means no rounding.\n    \"\"\"\n\n    column: str\n    agg: Callable\n    agg_name: Optional[str] = None\n    round: Optional[int] = None\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame","title":"TinyDataFrame","text":"<p>A very inefficient data-frame implementation with a few features.</p> <p>Supported features:</p> <ul> <li>reading CSV files from a remote URL</li> <li>reading CSV files from a local file</li> <li>converting a dictionary of lists/arrays to a data-frame</li> <li>converting a list of dictionaries to a data-frame</li> <li>slicing rows</li> <li>slicing columns</li> <li>slicing rows and columns</li> <li>printing a summary of the data-frame</li> <li>printing the data-frame as a human-readable (grid) table</li> <li>renaming columns</li> <li>writing to a CSV file</li> <li>computing statistics on columns</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[Union[Dict[str, List[float]], List[Dict[str, float]]]]</code> <p>Dictionary of lists/arrays or list of dictionaries.</p> <code>None</code> <code>csv_file_path</code> <code>Optional[str]</code> <p>Path to a properly quoted CSV file.</p> <code>None</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; df = TinyDataFrame(csv_file_path=\"https://raw.githubusercontent.com/plotly/datasets/refs/heads/master/mtcars.csv\")\n&gt;&gt;&gt; df\nTinyDataFrame with 32 rows and 12 columns. First row as a dict: {'manufacturer': 'Mazda RX4', 'mpg': 21.0, 'cyl': 6.0, 'disp': 160.0, 'hp': 110.0, 'drat': 3.9, 'wt': 2.62, 'qsec': 16.46, 'vs': 0.0, 'am': 1.0, 'gear': 4.0, 'carb': 4.0}\n&gt;&gt;&gt; df[2:5][['manufacturer', 'hp']]\nTinyDataFrame with 3 rows and 2 columns. First row as a dict: {'manufacturer': 'Datsun 710', 'hp': 93.0}\n&gt;&gt;&gt; print(df[2:5][['manufacturer', 'hp']])  # doctest: +NORMALIZE_WHITESPACE\nTinyDataFrame with 3 rows and 2 columns:\nmanufacturer      | hp\n------------------+------\nDatsun 710        |  93.0\nHornet 4 Drive    | 110.0\nHornet Sportabout | 175.0\n&gt;&gt;&gt; print(df[2:5][['manufacturer', 'hp']].to_csv())  # doctest: +NORMALIZE_WHITESPACE\n\"manufacturer\",\"hp\"\n\"Datsun 710\",93.0\n\"Hornet 4 Drive\",110.0\n\"Hornet Sportabout\",175.0\n</code></pre> <p>Methods:</p> Name Description <code>__init__</code> <p>Initialize with either:</p> <code>__len__</code> <p>Return the number of rows in the data-frame</p> <code>__getitem__</code> <p>Get a single column or multiple columns or a row or a slice of rows. Can be chained.</p> <code>__iter__</code> <p>Iterate through rows of the dataframe as dictionaries.</p> <code>__setitem__</code> <p>Set a column with the given key to the provided values.</p> <code>head</code> <p>Return first n rows as a new TinyDataFrame.</p> <code>tail</code> <p>Return last n rows as a new TinyDataFrame.</p> <code>__repr__</code> <p>Return a string representation of the data-frame.</p> <code>__str__</code> <p>Print the first 10 rows of the data-frame in a human-readable table.</p> <code>rename</code> <p>Rename one or multiple columns.</p> <code>to_csv</code> <p>Write the data-frame to a CSV file or return as string if no path is provided.</p> <code>to_dict</code> <p>Convert the data-frame to a list of row dictionaries.</p> <code>stats</code> <p>Collect statistics from the resource tracker.</p> <p>Attributes:</p> Name Type Description <code>columns</code> <code>List[str]</code> <p>Column names</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>class TinyDataFrame:\n    \"\"\"A very inefficient data-frame implementation with a few features.\n\n    Supported features:\n\n    - reading CSV files from a remote URL\n    - reading CSV files from a local file\n    - converting a dictionary of lists/arrays to a data-frame\n    - converting a list of dictionaries to a data-frame\n    - slicing rows\n    - slicing columns\n    - slicing rows and columns\n    - printing a summary of the data-frame\n    - printing the data-frame as a human-readable (grid) table\n    - renaming columns\n    - writing to a CSV file\n    - computing statistics on columns\n\n    Args:\n        data: Dictionary of lists/arrays or list of dictionaries.\n        csv_file_path: Path to a properly quoted CSV file.\n\n    Example:\n\n        &gt;&gt;&gt; df = TinyDataFrame(csv_file_path=\"https://raw.githubusercontent.com/plotly/datasets/refs/heads/master/mtcars.csv\")\n        &gt;&gt;&gt; df\n        TinyDataFrame with 32 rows and 12 columns. First row as a dict: {'manufacturer': 'Mazda RX4', 'mpg': 21.0, 'cyl': 6.0, 'disp': 160.0, 'hp': 110.0, 'drat': 3.9, 'wt': 2.62, 'qsec': 16.46, 'vs': 0.0, 'am': 1.0, 'gear': 4.0, 'carb': 4.0}\n        &gt;&gt;&gt; df[2:5][['manufacturer', 'hp']]\n        TinyDataFrame with 3 rows and 2 columns. First row as a dict: {'manufacturer': 'Datsun 710', 'hp': 93.0}\n        &gt;&gt;&gt; print(df[2:5][['manufacturer', 'hp']])  # doctest: +NORMALIZE_WHITESPACE\n        TinyDataFrame with 3 rows and 2 columns:\n        manufacturer      | hp\n        ------------------+------\n        Datsun 710        |  93.0\n        Hornet 4 Drive    | 110.0\n        Hornet Sportabout | 175.0\n        &gt;&gt;&gt; print(df[2:5][['manufacturer', 'hp']].to_csv())  # doctest: +NORMALIZE_WHITESPACE\n        \"manufacturer\",\"hp\"\n        \"Datsun 710\",93.0\n        \"Hornet 4 Drive\",110.0\n        \"Hornet Sportabout\",175.0\n    \"\"\"\n\n    _data: List[List[float]] = []\n    \"\"\"Column vectors\"\"\"\n    columns: List[str] = []\n    \"\"\"Column names\"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[Dict[str, List[float]], List[Dict[str, float]]]] = None,\n        csv_file_path: Optional[str] = None,\n        retries: int = 0,\n        retry_delay: float = 0.1,\n    ):\n        \"\"\"\n        Initialize with either:\n\n        - Dictionary of lists/arrays\n        - List of dictionaries\n        - CSV file path\n\n        Args:\n            data: Dictionary of lists/arrays or list of dictionaries.\n            csv_file_path: Path to a properly quoted CSV file.\n            retries: Number of retry attempts if reading CSV fails. Defaults to 0.\n            retry_delay: Initial delay between retries in seconds. Doubles after each retry. Defaults to 0.1.\n        \"\"\"\n        assert data is not None or csv_file_path is not None, (\n            \"either data or csv_file_path must be provided\"\n        )\n        assert data is None or csv_file_path is None, (\n            \"only one of data or csv_file_path must be provided\"\n        )\n        assert data is None or isinstance(data, dict) or isinstance(data, list), (\n            \"data must be a dictionary or a list\"\n        )\n        assert csv_file_path is None or isinstance(csv_file_path, str), (\n            \"csv_file_path must be a string\"\n        )\n\n        if csv_file_path:\n            data = self._read_csv(\n                csv_file_path, retries=retries, retry_delay=retry_delay\n            )\n\n        if isinstance(data, dict):\n            self._data = list(data.values())\n            self.columns = list(data.keys())\n        elif isinstance(data, list) and data and isinstance(data[0], dict):\n            # let's preserve column order as seen in the first row(s)\n            self.columns = []\n            seen_columns = set()\n            for row in data:\n                for col in row.keys():\n                    if col not in seen_columns:\n                        self.columns.append(col)\n                        seen_columns.add(col)\n            self._data = [[row.get(col) for row in data] for col in self.columns]\n\n    def _read_csv(\n        self, csv_file_path: str, retries: int = 0, retry_delay: float = 0.1\n    ) -&gt; list[dict]:\n        \"\"\"Read a CSV file and return a list of dictionaries.\n\n        This function is used to read a properly formatted CSV file (quoted\n        strings, header, same columns used on all lines) and return a list of\n        dictionaries. It will retry up to `retries` times if the file is not\n        found or cannot be read, e.g. because the file is being locked for\n        writing by another process. The retry delay will be doubled after each\n        attempt.\n\n        Args:\n            csv_file_path: CSV file path or URL.\n            retries: Number of retry attempts if reading fails. Defaults to 0.\n            retry_delay: Initial delay between retries in seconds. Doubles after each retry. Defaults to 0.1.\n\n        Raises:\n            Exception: If reading the CSV file fails after all retry attempts.\n        \"\"\"\n        last_error = None\n        for attempt in range(retries + 1):\n            csv_source = None\n            try:\n                parsed = urlparse(csv_file_path)\n                if parsed.scheme in (\"http\", \"https\"):\n                    with urlopen(csv_file_path) as response:\n                        content = response.read().decode(\"utf-8\").splitlines()\n                        csv_source = content\n                else:\n                    csv_source = open(csv_file_path, \"r\", newline=\"\")\n                records = list(DictReader(csv_source, quoting=QUOTE_NONNUMERIC))\n                for record in records:\n                    if None in record.keys():\n                        raise ValueError(\"Corrupt CSV file with unknown column names.\")\n                return records\n            except Exception as e:\n                last_error = e\n                if attempt &lt; retries:\n                    logger.debug(\n                        f\"Failed to read CSV file: {csv_file_path}. \"\n                        f\"Retry attempt {attempt + 1}/{retries}...\"\n                    )\n                    sleep(retry_delay)\n                    retry_delay *= 2\n                    continue\n            finally:\n                if hasattr(csv_source, \"close\"):\n                    csv_source.close()\n\n        # all retries failed\n        raise RuntimeError(\n            f\"Failed to read CSV file after {retries + 1} attempts: {csv_file_path}\"\n        ) from last_error\n\n    def __len__(self):\n        \"\"\"Return the number of rows in the data-frame\"\"\"\n        return len(self._data[0]) if self.columns else 0\n\n    def __getitem__(\n        self, key: Union[str, List[str], int, slice]\n    ) -&gt; Union[List[float], Dict[str, float], \"TinyDataFrame\"]:\n        \"\"\"Get a single column or multiple columns or a row or a slice of rows. Can be chained.\n\n        Args:\n            key: A single column name, a list of column names, a row index, or a slice of row indexes.\n\n        Returns:\n            A single column as a list, a list of columns as a new TinyDataFrame, a row as a dictionary, or a slice of rows as a new TinyDataFrame.\n        \"\"\"\n        # a single column\n        if isinstance(key, str):\n            if key in self.columns:\n                return self._data[self.columns.index(key)]\n            else:\n                raise KeyError(f\"Column '{key}' not found\")\n        # multiple columns\n        elif isinstance(key, List) and all(isinstance(k, str) for k in key):\n            for k in key:\n                if k not in self.columns:\n                    raise KeyError(f\"Column '{k}' not found\")\n            return TinyDataFrame(\n                {col: self._data[self.columns.index(col)] for col in key}\n            )\n        # row index\n        elif isinstance(key, int):\n            return {col: self._data[i][key] for i, col in enumerate(self.columns)}\n        # row indexes\n        elif isinstance(key, slice):\n            return TinyDataFrame(\n                {col: self._data[i][key] for i, col in enumerate(self.columns)}\n            )\n        else:\n            raise TypeError(f\"Invalid key type: {type(key)}\")\n\n    def __iter__(self) -&gt; Iterator[dict]:\n        \"\"\"Iterate through rows of the dataframe as dictionaries.\n\n        Returns:\n            Iterator of row dictionaries\n        \"\"\"\n        for i in range(len(self)):\n            yield self[i]\n\n    def __setitem__(self, key: str, value: List[float]) -&gt; None:\n        \"\"\"Set a column with the given key to the provided values.\n\n        Args:\n            key: Column name (string)\n            value: List of values for the column\n\n        Raises:\n            TypeError: If key is not a string\n            ValueError: If the length of values doesn't match the dataframe length\n        \"\"\"\n        if not isinstance(key, str):\n            raise TypeError(f\"Column name must be a string, got {type(key)}\")\n\n        if len(self) &gt; 0 and len(value) != len(self):\n            raise ValueError(\n                f\"Length of values ({len(value)}) must match dataframe length ({len(self)})\"\n            )\n\n        if key in self.columns:\n            self._data[self.columns.index(key)] = value\n        else:\n            self.columns.append(key)\n            self._data.append(value)\n\n    def head(self, n: int = 5) -&gt; \"TinyDataFrame\":\n        \"\"\"Return first n rows as a new TinyDataFrame.\"\"\"\n        return self[slice(0, n)]\n\n    def tail(self, n: int = 5) -&gt; \"TinyDataFrame\":\n        \"\"\"Return last n rows as a new TinyDataFrame.\"\"\"\n        return self[slice(-n, None)]\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return a string representation of the data-frame.\"\"\"\n        return f\"TinyDataFrame with {len(self)} rows and {len(self.columns)} columns. First row as a dict: {self[0]}\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Print the first 10 rows of the data-frame in a human-readable table.\"\"\"\n        header = (\n            f\"TinyDataFrame with {len(self)} rows and {len(self.columns)} columns:\\n\"\n        )\n        if len(self) == 0:\n            return header + \"Empty dataframe\"\n\n        max_rows = min(10, len(self))\n\n        col_widths = {}\n        for col in self.columns:\n            col_widths[col] = len(str(col))\n            for i in range(max_rows):\n                col_widths[col] = max(col_widths[col], len(str(self[col][i])))\n\n        rows = []\n        header_row = \" | \".join(str(col).ljust(col_widths[col]) for col in self.columns)\n        rows.append(header_row)\n        separator = \"-+-\".join(\"-\" * col_widths[col] for col in self.columns)\n        rows.append(separator)\n\n        for i in range(max_rows):\n            row_values = []\n            for col in self.columns:\n                value = str(self[col][i])\n                # right-align numbers, left-align strings\n                try:\n                    float(value)  # check if it's a number\n                    row_values.append(value.rjust(col_widths[col]))\n                except ValueError:\n                    row_values.append(value.ljust(col_widths[col]))\n            rows.append(\" | \".join(row_values))\n\n        # add ellipsis if there are more rows\n        if len(self) &gt; max_rows:\n            rows.append(\"...\" + \" \" * (len(rows[0]) - 3))\n        return header + \"\\n\".join(rows)\n\n    def rename(self, columns: dict) -&gt; \"TinyDataFrame\":\n        \"\"\"Rename one or multiple columns.\n\n        Args:\n            columns: Dictionary mapping old column names to new column names.\n\n        Returns:\n            Self for method chaining.\n\n        Raises:\n            KeyError: If any old column name doesn't exist in the dataframe.\n        \"\"\"\n        for old_name in columns.keys():\n            if old_name not in self.columns:\n                raise KeyError(f\"Column '{old_name}' not found in dataframe\")\n\n        self.columns = [columns.get(col, col) for col in self.columns]\n        return self\n\n    def to_csv(\n        self, csv_file_path: Optional[str] = None, quote_strings: bool = True\n    ) -&gt; str:\n        \"\"\"Write the data-frame to a CSV file or return as string if no path is provided.\n\n        Args:\n            csv_file_path: Path to write CSV file. If None, returns CSV as string.\n            quote_strings: Whether to quote strings.\n        \"\"\"\n        if csv_file_path:\n            f = open(csv_file_path, \"w\", newline=\"\")\n        else:\n            f = StringIO(newline=\"\")\n\n        try:\n            writer = csv_writer(\n                f, quoting=QUOTE_NONNUMERIC if quote_strings else QUOTE_MINIMAL\n            )\n            writer.writerow(self.columns)\n            for i in range(len(self)):\n                writer.writerow([self[col][i] for col in self.columns])\n\n            if not csv_file_path:\n                return f.getvalue()\n        finally:\n            f.close()\n\n    def to_dict(self) -&gt; List[dict]:\n        \"\"\"Convert the data-frame to a list of row dictionaries.\n\n        Returns:\n            List of dictionaries where each dictionary represents a row\n        \"\"\"\n        return list(self)\n\n    def stats(self, specs: List[StatSpec]) -&gt; dict:\n        \"\"\"Collect statistics from the resource tracker.\n\n        Args:\n            specs: A list of [resource_tracker.StatSpec][] objects specifying the statistics to collect.\n\n        Returns:\n            A dictionary containing the collected statistics.\n        \"\"\"\n        stats = defaultdict(dict)\n        for spec in specs:\n            try:\n                agg_name = spec.agg_name or spec.agg.__name__\n                stats[spec.column][agg_name] = spec.agg(self[spec.column])\n                if spec.round is not None:\n                    stats[spec.column][agg_name] = round(\n                        stats[spec.column][agg_name], spec.round\n                    )\n            except Exception as e:\n                logger.error(\n                    f\"Error collecting statistic on {spec.column} with {spec.agg_name}: {e}\"\n                )\n        return stats\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.columns","title":"columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>columns = []\n</code></pre> <p>Column names</p>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__init__","title":"__init__","text":"<pre><code>__init__(data=None, csv_file_path=None, retries=0, retry_delay=0.1)\n</code></pre> <p>Initialize with either:</p> <ul> <li>Dictionary of lists/arrays</li> <li>List of dictionaries</li> <li>CSV file path</li> </ul> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[Union[Dict[str, List[float]], List[Dict[str, float]]]]</code> <p>Dictionary of lists/arrays or list of dictionaries.</p> <code>None</code> <code>csv_file_path</code> <code>Optional[str]</code> <p>Path to a properly quoted CSV file.</p> <code>None</code> <code>retries</code> <code>int</code> <p>Number of retry attempts if reading CSV fails. Defaults to 0.</p> <code>0</code> <code>retry_delay</code> <code>float</code> <p>Initial delay between retries in seconds. Doubles after each retry. Defaults to 0.1.</p> <code>0.1</code> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[Dict[str, List[float]], List[Dict[str, float]]]] = None,\n    csv_file_path: Optional[str] = None,\n    retries: int = 0,\n    retry_delay: float = 0.1,\n):\n    \"\"\"\n    Initialize with either:\n\n    - Dictionary of lists/arrays\n    - List of dictionaries\n    - CSV file path\n\n    Args:\n        data: Dictionary of lists/arrays or list of dictionaries.\n        csv_file_path: Path to a properly quoted CSV file.\n        retries: Number of retry attempts if reading CSV fails. Defaults to 0.\n        retry_delay: Initial delay between retries in seconds. Doubles after each retry. Defaults to 0.1.\n    \"\"\"\n    assert data is not None or csv_file_path is not None, (\n        \"either data or csv_file_path must be provided\"\n    )\n    assert data is None or csv_file_path is None, (\n        \"only one of data or csv_file_path must be provided\"\n    )\n    assert data is None or isinstance(data, dict) or isinstance(data, list), (\n        \"data must be a dictionary or a list\"\n    )\n    assert csv_file_path is None or isinstance(csv_file_path, str), (\n        \"csv_file_path must be a string\"\n    )\n\n    if csv_file_path:\n        data = self._read_csv(\n            csv_file_path, retries=retries, retry_delay=retry_delay\n        )\n\n    if isinstance(data, dict):\n        self._data = list(data.values())\n        self.columns = list(data.keys())\n    elif isinstance(data, list) and data and isinstance(data[0], dict):\n        # let's preserve column order as seen in the first row(s)\n        self.columns = []\n        seen_columns = set()\n        for row in data:\n            for col in row.keys():\n                if col not in seen_columns:\n                    self.columns.append(col)\n                    seen_columns.add(col)\n        self._data = [[row.get(col) for row in data] for col in self.columns]\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__len__","title":"__len__","text":"<pre><code>__len__()\n</code></pre> <p>Return the number of rows in the data-frame</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __len__(self):\n    \"\"\"Return the number of rows in the data-frame\"\"\"\n    return len(self._data[0]) if self.columns else 0\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key)\n</code></pre> <p>Get a single column or multiple columns or a row or a slice of rows. Can be chained.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[str, List[str], int, slice]</code> <p>A single column name, a list of column names, a row index, or a slice of row indexes.</p> required <p>Returns:</p> Type Description <code>Union[List[float], Dict[str, float], TinyDataFrame]</code> <p>A single column as a list, a list of columns as a new TinyDataFrame, a row as a dictionary, or a slice of rows as a new TinyDataFrame.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __getitem__(\n    self, key: Union[str, List[str], int, slice]\n) -&gt; Union[List[float], Dict[str, float], \"TinyDataFrame\"]:\n    \"\"\"Get a single column or multiple columns or a row or a slice of rows. Can be chained.\n\n    Args:\n        key: A single column name, a list of column names, a row index, or a slice of row indexes.\n\n    Returns:\n        A single column as a list, a list of columns as a new TinyDataFrame, a row as a dictionary, or a slice of rows as a new TinyDataFrame.\n    \"\"\"\n    # a single column\n    if isinstance(key, str):\n        if key in self.columns:\n            return self._data[self.columns.index(key)]\n        else:\n            raise KeyError(f\"Column '{key}' not found\")\n    # multiple columns\n    elif isinstance(key, List) and all(isinstance(k, str) for k in key):\n        for k in key:\n            if k not in self.columns:\n                raise KeyError(f\"Column '{k}' not found\")\n        return TinyDataFrame(\n            {col: self._data[self.columns.index(col)] for col in key}\n        )\n    # row index\n    elif isinstance(key, int):\n        return {col: self._data[i][key] for i, col in enumerate(self.columns)}\n    # row indexes\n    elif isinstance(key, slice):\n        return TinyDataFrame(\n            {col: self._data[i][key] for i, col in enumerate(self.columns)}\n        )\n    else:\n        raise TypeError(f\"Invalid key type: {type(key)}\")\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Iterate through rows of the dataframe as dictionaries.</p> <p>Returns:</p> Type Description <code>Iterator[dict]</code> <p>Iterator of row dictionaries</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __iter__(self) -&gt; Iterator[dict]:\n    \"\"\"Iterate through rows of the dataframe as dictionaries.\n\n    Returns:\n        Iterator of row dictionaries\n    \"\"\"\n    for i in range(len(self)):\n        yield self[i]\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__setitem__","title":"__setitem__","text":"<pre><code>__setitem__(key, value)\n</code></pre> <p>Set a column with the given key to the provided values.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Column name (string)</p> required <code>value</code> <code>List[float]</code> <p>List of values for the column</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If key is not a string</p> <code>ValueError</code> <p>If the length of values doesn't match the dataframe length</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __setitem__(self, key: str, value: List[float]) -&gt; None:\n    \"\"\"Set a column with the given key to the provided values.\n\n    Args:\n        key: Column name (string)\n        value: List of values for the column\n\n    Raises:\n        TypeError: If key is not a string\n        ValueError: If the length of values doesn't match the dataframe length\n    \"\"\"\n    if not isinstance(key, str):\n        raise TypeError(f\"Column name must be a string, got {type(key)}\")\n\n    if len(self) &gt; 0 and len(value) != len(self):\n        raise ValueError(\n            f\"Length of values ({len(value)}) must match dataframe length ({len(self)})\"\n        )\n\n    if key in self.columns:\n        self._data[self.columns.index(key)] = value\n    else:\n        self.columns.append(key)\n        self._data.append(value)\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.head","title":"head","text":"<pre><code>head(n=5)\n</code></pre> <p>Return first n rows as a new TinyDataFrame.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def head(self, n: int = 5) -&gt; \"TinyDataFrame\":\n    \"\"\"Return first n rows as a new TinyDataFrame.\"\"\"\n    return self[slice(0, n)]\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.tail","title":"tail","text":"<pre><code>tail(n=5)\n</code></pre> <p>Return last n rows as a new TinyDataFrame.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def tail(self, n: int = 5) -&gt; \"TinyDataFrame\":\n    \"\"\"Return last n rows as a new TinyDataFrame.\"\"\"\n    return self[slice(-n, None)]\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__repr__","title":"__repr__","text":"<pre><code>__repr__()\n</code></pre> <p>Return a string representation of the data-frame.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return a string representation of the data-frame.\"\"\"\n    return f\"TinyDataFrame with {len(self)} rows and {len(self.columns)} columns. First row as a dict: {self[0]}\"\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.__str__","title":"__str__","text":"<pre><code>__str__()\n</code></pre> <p>Print the first 10 rows of the data-frame in a human-readable table.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Print the first 10 rows of the data-frame in a human-readable table.\"\"\"\n    header = (\n        f\"TinyDataFrame with {len(self)} rows and {len(self.columns)} columns:\\n\"\n    )\n    if len(self) == 0:\n        return header + \"Empty dataframe\"\n\n    max_rows = min(10, len(self))\n\n    col_widths = {}\n    for col in self.columns:\n        col_widths[col] = len(str(col))\n        for i in range(max_rows):\n            col_widths[col] = max(col_widths[col], len(str(self[col][i])))\n\n    rows = []\n    header_row = \" | \".join(str(col).ljust(col_widths[col]) for col in self.columns)\n    rows.append(header_row)\n    separator = \"-+-\".join(\"-\" * col_widths[col] for col in self.columns)\n    rows.append(separator)\n\n    for i in range(max_rows):\n        row_values = []\n        for col in self.columns:\n            value = str(self[col][i])\n            # right-align numbers, left-align strings\n            try:\n                float(value)  # check if it's a number\n                row_values.append(value.rjust(col_widths[col]))\n            except ValueError:\n                row_values.append(value.ljust(col_widths[col]))\n        rows.append(\" | \".join(row_values))\n\n    # add ellipsis if there are more rows\n    if len(self) &gt; max_rows:\n        rows.append(\"...\" + \" \" * (len(rows[0]) - 3))\n    return header + \"\\n\".join(rows)\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.rename","title":"rename","text":"<pre><code>rename(columns)\n</code></pre> <p>Rename one or multiple columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>dict</code> <p>Dictionary mapping old column names to new column names.</p> required <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>Self for method chaining.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If any old column name doesn't exist in the dataframe.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def rename(self, columns: dict) -&gt; \"TinyDataFrame\":\n    \"\"\"Rename one or multiple columns.\n\n    Args:\n        columns: Dictionary mapping old column names to new column names.\n\n    Returns:\n        Self for method chaining.\n\n    Raises:\n        KeyError: If any old column name doesn't exist in the dataframe.\n    \"\"\"\n    for old_name in columns.keys():\n        if old_name not in self.columns:\n            raise KeyError(f\"Column '{old_name}' not found in dataframe\")\n\n    self.columns = [columns.get(col, col) for col in self.columns]\n    return self\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.to_csv","title":"to_csv","text":"<pre><code>to_csv(csv_file_path=None, quote_strings=True)\n</code></pre> <p>Write the data-frame to a CSV file or return as string if no path is provided.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file_path</code> <code>Optional[str]</code> <p>Path to write CSV file. If None, returns CSV as string.</p> <code>None</code> <code>quote_strings</code> <code>bool</code> <p>Whether to quote strings.</p> <code>True</code> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def to_csv(\n    self, csv_file_path: Optional[str] = None, quote_strings: bool = True\n) -&gt; str:\n    \"\"\"Write the data-frame to a CSV file or return as string if no path is provided.\n\n    Args:\n        csv_file_path: Path to write CSV file. If None, returns CSV as string.\n        quote_strings: Whether to quote strings.\n    \"\"\"\n    if csv_file_path:\n        f = open(csv_file_path, \"w\", newline=\"\")\n    else:\n        f = StringIO(newline=\"\")\n\n    try:\n        writer = csv_writer(\n            f, quoting=QUOTE_NONNUMERIC if quote_strings else QUOTE_MINIMAL\n        )\n        writer.writerow(self.columns)\n        for i in range(len(self)):\n            writer.writerow([self[col][i] for col in self.columns])\n\n        if not csv_file_path:\n            return f.getvalue()\n    finally:\n        f.close()\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.to_dict","title":"to_dict","text":"<pre><code>to_dict()\n</code></pre> <p>Convert the data-frame to a list of row dictionaries.</p> <p>Returns:</p> Type Description <code>List[dict]</code> <p>List of dictionaries where each dictionary represents a row</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def to_dict(self) -&gt; List[dict]:\n    \"\"\"Convert the data-frame to a list of row dictionaries.\n\n    Returns:\n        List of dictionaries where each dictionary represents a row\n    \"\"\"\n    return list(self)\n</code></pre>"},{"location":"reference/resource_tracker/tiny_data_frame/#resource_tracker.tiny_data_frame.TinyDataFrame.stats","title":"stats","text":"<pre><code>stats(specs)\n</code></pre> <p>Collect statistics from the resource tracker.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>List[StatSpec]</code> <p>A list of [resource_tracker.StatSpec][] objects specifying the statistics to collect.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the collected statistics.</p> Source code in <code>resource_tracker/tiny_data_frame.py</code> <pre><code>def stats(self, specs: List[StatSpec]) -&gt; dict:\n    \"\"\"Collect statistics from the resource tracker.\n\n    Args:\n        specs: A list of [resource_tracker.StatSpec][] objects specifying the statistics to collect.\n\n    Returns:\n        A dictionary containing the collected statistics.\n    \"\"\"\n    stats = defaultdict(dict)\n    for spec in specs:\n        try:\n            agg_name = spec.agg_name or spec.agg.__name__\n            stats[spec.column][agg_name] = spec.agg(self[spec.column])\n            if spec.round is not None:\n                stats[spec.column][agg_name] = round(\n                    stats[spec.column][agg_name], spec.round\n                )\n        except Exception as e:\n            logger.error(\n                f\"Error collecting statistic on {spec.column} with {spec.agg_name}: {e}\"\n            )\n    return stats\n</code></pre>"},{"location":"reference/resource_tracker/tracker/","title":"resource_tracker.tracker","text":""},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker","title":"resource_tracker.tracker","text":"<p>Track resource usage of a process and/or the system.</p> <p>To start tracker(s) in the background as spawned or forked process(es), use the resource_tracker.ResourceTracker class. Starting this will not block the main process and will allow you to access the collected data via the <code>process_metrics</code> and <code>system_metrics</code> properties of the instance in real-time, or after stopping the resource tracker(s).</p> <p>For more custom use cases, you can also use the resource_tracker.ProcessTracker and resource_tracker.SystemTracker classes directly logging either to the standard output or a file, and handle putting those into a background thread/process yourself.</p> <p>Classes:</p> Name Description <code>ProcessTracker</code> <p>Track resource usage of a process and optionally its children.</p> <code>PidTracker</code> <p>Old name for resource_tracker.ProcessTracker.</p> <code>SystemTracker</code> <p>Track system-wide resource usage.</p> <code>ResourceTracker</code> <p>Track resource usage of processes and the system in a non-blocking way.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ProcessTracker","title":"ProcessTracker","text":"<p>Track resource usage of a process and optionally its children.</p> <p>This class monitors system resources like CPU times and usage, memory usage, GPU and VRAM utilization, I/O operations for a given process ID and optionally its child processes.</p> <p>Data is collected every <code>interval</code> seconds and written to the stdout or <code>output_file</code> (if provided) as CSV. Currently, the following columns are tracked:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>pid (int): The monitored process ID.</li> <li>children (int | None): The current number of child processes.</li> <li>utime (int): The total user+nice mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>cpu_usage (float): The current CPU usage between 0 and number of CPUs.</li> <li>memory (int): The current memory usage in kB. Implementation depends on the   operating system, and it is preferably PSS (Proportional Set Size) on Linux,   USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on   Windows.</li> <li>read_bytes (int): The total number of bytes read from disk.</li> <li>write_bytes (int): The total number of bytes written to disk.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>Process ID to track. Defaults to current process ID.</p> <code>getpid()</code> <code>start_time</code> <code>float</code> <p>Time when to start tracking. Defaults to current time.</p> <code>time()</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>children</code> <code>bool</code> <p>Whether to track child processes. Defaults to True.</p> <code>True</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>File to write the output to. Defaults to None, print to stdout.</p> <code>None</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Dummy method to make this class callable.</p> <code>diff_stats</code> <p>Calculate stats since last call.</p> <code>start_tracking</code> <p>Start an infinite loop tracking resource usage of the process until it exits.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class ProcessTracker:\n    \"\"\"Track resource usage of a process and optionally its children.\n\n    This class monitors system resources like CPU times and usage, memory usage,\n    GPU and VRAM utilization, I/O operations for a given process ID and\n    optionally its child processes.\n\n    Data is collected every `interval` seconds and written to the stdout or\n    `output_file` (if provided) as CSV. Currently, the following columns are\n    tracked:\n\n    - timestamp (float): The current timestamp.\n    - pid (int): The monitored process ID.\n    - children (int | None): The current number of child processes.\n    - utime (int): The total user+nice mode CPU time in seconds.\n    - stime (int): The total system mode CPU time in seconds.\n    - cpu_usage (float): The current CPU usage between 0 and number of CPUs.\n    - memory (int): The current memory usage in kB. Implementation depends on the\n      operating system, and it is preferably PSS (Proportional Set Size) on Linux,\n      USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on\n      Windows.\n    - read_bytes (int): The total number of bytes read from disk.\n    - write_bytes (int): The total number of bytes written to disk.\n    - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n    - gpu_vram (float): The current GPU memory used in MiB.\n    - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n\n    Args:\n        pid (int, optional): Process ID to track. Defaults to current process ID.\n        start_time: Time when to start tracking. Defaults to current time.\n        interval (float, optional): Sampling interval in seconds. Defaults to 1.\n        children (bool, optional): Whether to track child processes. Defaults to True.\n        autostart (bool, optional): Whether to start tracking immediately. Defaults to True.\n        output_file (str, optional): File to write the output to. Defaults to None, print to stdout.\n    \"\"\"\n\n    def __init__(\n        self,\n        pid: int = getpid(),\n        start_time: float = time(),\n        interval: float = 1,\n        children: bool = True,\n        autostart: bool = True,\n        output_file: str = None,\n    ):\n        self.get_process_stats, _ = get_tracker_implementation()\n\n        self.pid = pid\n        self.status = \"running\"\n        self.interval = interval\n        self.cycle = 0\n        self.children = children\n        self.start_time = start_time\n\n        # initial data collection so that we can use that as a baseline when diffing after the first interval\n        self.stats = self.get_process_stats(pid, children)\n\n        if autostart:\n            # wait for the start time to be reached\n            if start_time &gt; time():\n                sleep(start_time - time())\n            # we can now start. 1st interval used to collect baseline\n            self.start_tracking(output_file)\n\n    def __call__(self):\n        \"\"\"Dummy method to make this class callable.\"\"\"\n        pass\n\n    def diff_stats(self):\n        \"\"\"Calculate stats since last call.\"\"\"\n        last_stats = self.stats\n        self.stats = self.get_process_stats(self.pid, self.children)\n        self.cycle += 1\n\n        return {\n            \"timestamp\": round(self.stats[\"timestamp\"], 3),\n            \"pid\": self.pid,\n            \"children\": self.stats[\"children\"],\n            \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n            \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n            \"cpu_usage\": round(\n                max(\n                    0,\n                    (\n                        (self.stats[\"utime\"] + self.stats[\"stime\"])\n                        - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                    )\n                    / (self.stats[\"timestamp\"] - last_stats[\"timestamp\"]),\n                ),\n                4,\n            ),\n            \"memory\": self.stats[\"memory\"],\n            \"read_bytes\": max(0, self.stats[\"read_bytes\"] - last_stats[\"read_bytes\"]),\n            \"write_bytes\": max(\n                0, self.stats[\"write_bytes\"] - last_stats[\"write_bytes\"]\n            ),\n            \"gpu_usage\": self.stats[\"gpu_usage\"],\n            \"gpu_vram\": self.stats[\"gpu_vram\"],\n            \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n        }\n\n    def start_tracking(\n        self, output_file: Optional[str] = None, print_header: bool = True\n    ):\n        \"\"\"Start an infinite loop tracking resource usage of the process until it exits.\n\n        A CSV line is written every `interval` seconds.\n\n        Args:\n            output_file: File to write the output to. Defaults to None, printing to stdout.\n            print_header: Whether to print the header of the CSV. Defaults to True.\n        \"\"\"\n        file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n        try:\n            while True:\n                current_stats = self.diff_stats()\n                if current_stats[\"memory\"] == 0:\n                    # the process has exited\n                    self.status = \"exited\"\n                    break\n                # don't print values yet, we collect data for the 1st baseline\n                if self.cycle == 1:\n                    if print_header:\n                        file_handle.write(\n                            render_csv_row(\n                                current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                            )\n                        )\n                else:\n                    file_handle.write(\n                        render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                    )\n                if output_file:\n                    file_handle.flush()\n                # sleep until the next interval\n                sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n        finally:\n            if output_file and not file_handle.closed:\n                file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ProcessTracker.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Dummy method to make this class callable.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def __call__(self):\n    \"\"\"Dummy method to make this class callable.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ProcessTracker.diff_stats","title":"diff_stats","text":"<pre><code>diff_stats()\n</code></pre> <p>Calculate stats since last call.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def diff_stats(self):\n    \"\"\"Calculate stats since last call.\"\"\"\n    last_stats = self.stats\n    self.stats = self.get_process_stats(self.pid, self.children)\n    self.cycle += 1\n\n    return {\n        \"timestamp\": round(self.stats[\"timestamp\"], 3),\n        \"pid\": self.pid,\n        \"children\": self.stats[\"children\"],\n        \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n        \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n        \"cpu_usage\": round(\n            max(\n                0,\n                (\n                    (self.stats[\"utime\"] + self.stats[\"stime\"])\n                    - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                )\n                / (self.stats[\"timestamp\"] - last_stats[\"timestamp\"]),\n            ),\n            4,\n        ),\n        \"memory\": self.stats[\"memory\"],\n        \"read_bytes\": max(0, self.stats[\"read_bytes\"] - last_stats[\"read_bytes\"]),\n        \"write_bytes\": max(\n            0, self.stats[\"write_bytes\"] - last_stats[\"write_bytes\"]\n        ),\n        \"gpu_usage\": self.stats[\"gpu_usage\"],\n        \"gpu_vram\": self.stats[\"gpu_vram\"],\n        \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n    }\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ProcessTracker.start_tracking","title":"start_tracking","text":"<pre><code>start_tracking(output_file=None, print_header=True)\n</code></pre> <p>Start an infinite loop tracking resource usage of the process until it exits.</p> <p>A CSV line is written every <code>interval</code> seconds.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>Optional[str]</code> <p>File to write the output to. Defaults to None, printing to stdout.</p> <code>None</code> <code>print_header</code> <code>bool</code> <p>Whether to print the header of the CSV. Defaults to True.</p> <code>True</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start_tracking(\n    self, output_file: Optional[str] = None, print_header: bool = True\n):\n    \"\"\"Start an infinite loop tracking resource usage of the process until it exits.\n\n    A CSV line is written every `interval` seconds.\n\n    Args:\n        output_file: File to write the output to. Defaults to None, printing to stdout.\n        print_header: Whether to print the header of the CSV. Defaults to True.\n    \"\"\"\n    file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n    try:\n        while True:\n            current_stats = self.diff_stats()\n            if current_stats[\"memory\"] == 0:\n                # the process has exited\n                self.status = \"exited\"\n                break\n            # don't print values yet, we collect data for the 1st baseline\n            if self.cycle == 1:\n                if print_header:\n                    file_handle.write(\n                        render_csv_row(\n                            current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                        )\n                    )\n            else:\n                file_handle.write(\n                    render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                )\n            if output_file:\n                file_handle.flush()\n            # sleep until the next interval\n            sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n    finally:\n        if output_file and not file_handle.closed:\n            file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.PidTracker","title":"PidTracker","text":"<p>               Bases: <code>ProcessTracker</code></p> <p>Old name for resource_tracker.ProcessTracker.</p> <p>This class is deprecated and will be removed in the future.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class PidTracker(ProcessTracker):\n    \"\"\"Old name for [resource_tracker.ProcessTracker][].\n\n    This class is deprecated and will be removed in the future.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        warn(\n            \"PidTracker is deprecated and will be removed in a future release. \"\n            \"Please use ProcessTracker instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.SystemTracker","title":"SystemTracker","text":"<p>Track system-wide resource usage.</p> <p>This class monitors system resources like CPU times and usage, memory usage, GPU and VRAM utilization, disk I/O, and network traffic for the entire system.</p> <p>Data is collected every <code>interval</code> seconds and written to the stdout or <code>output_file</code> (if provided) as CSV. Currently, the following columns are tracked:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>processes (int): The number of running processes.</li> <li>utime (int): The total user+nice mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>cpu_usage (float): The current CPU usage between 0 and number of CPUs.</li> <li>memory_free (int): The amount of free memory in kB.</li> <li>memory_used (int): The amount of used memory in kB.</li> <li>memory_buffers (int): The amount of memory used for buffers in kB.</li> <li>memory_cached (int): The amount of memory used for caching in kB.</li> <li>memory_active (int): The amount of memory used for active pages in kB.</li> <li>memory_inactive (int): The amount of memory used for inactive pages in kB.</li> <li>disk_read_bytes (int): The total number of bytes read from disk.</li> <li>disk_write_bytes (int): The total number of bytes written to disk.</li> <li>disk_space_total_gb (float): The total disk space in GB.</li> <li>disk_space_used_gb (float): The used disk space in GB.</li> <li>disk_space_free_gb (float): The free disk space in GB.</li> <li>net_recv_bytes (int): The total number of bytes received over network.</li> <li>net_sent_bytes (int): The total number of bytes sent over network.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Time when to start tracking. Defaults to current time.</p> <code>time()</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>output_file</code> <code>str</code> <p>File to write the output to. Defaults to None, print to stdout.</p> <code>None</code> <p>Methods:</p> Name Description <code>__call__</code> <p>Dummy method to make this class callable.</p> <code>diff_stats</code> <p>Calculate stats since last call.</p> <code>start_tracking</code> <p>Start an infinite loop tracking system resource usage.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class SystemTracker:\n    \"\"\"Track system-wide resource usage.\n\n    This class monitors system resources like CPU times and usage, memory usage,\n    GPU and VRAM utilization, disk I/O, and network traffic for the entire system.\n\n    Data is collected every `interval` seconds and written to the stdout or\n    `output_file` (if provided) as CSV. Currently, the following columns are\n    tracked:\n\n    - timestamp (float): The current timestamp.\n    - processes (int): The number of running processes.\n    - utime (int): The total user+nice mode CPU time in seconds.\n    - stime (int): The total system mode CPU time in seconds.\n    - cpu_usage (float): The current CPU usage between 0 and number of CPUs.\n    - memory_free (int): The amount of free memory in kB.\n    - memory_used (int): The amount of used memory in kB.\n    - memory_buffers (int): The amount of memory used for buffers in kB.\n    - memory_cached (int): The amount of memory used for caching in kB.\n    - memory_active (int): The amount of memory used for active pages in kB.\n    - memory_inactive (int): The amount of memory used for inactive pages in kB.\n    - disk_read_bytes (int): The total number of bytes read from disk.\n    - disk_write_bytes (int): The total number of bytes written to disk.\n    - disk_space_total_gb (float): The total disk space in GB.\n    - disk_space_used_gb (float): The used disk space in GB.\n    - disk_space_free_gb (float): The free disk space in GB.\n    - net_recv_bytes (int): The total number of bytes received over network.\n    - net_sent_bytes (int): The total number of bytes sent over network.\n    - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n    - gpu_vram (float): The current GPU memory used in MiB.\n    - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n\n    Args:\n        start_time: Time when to start tracking. Defaults to current time.\n        interval: Sampling interval in seconds. Defaults to 1.\n        autostart: Whether to start tracking immediately. Defaults to True.\n        output_file: File to write the output to. Defaults to None, print to stdout.\n    \"\"\"\n\n    def __init__(\n        self,\n        start_time: float = time(),\n        interval: float = 1,\n        autostart: bool = True,\n        output_file: str = None,\n    ):\n        _, self.get_system_stats = get_tracker_implementation()\n\n        self.status = \"running\"\n        self.interval = interval\n        self.cycle = 0\n        self.start_time = start_time\n\n        # dummy data collection so that diffing on the first time does not fail\n        self.stats = self.get_system_stats()\n\n        if autostart:\n            # wait for the start time to be reached\n            if start_time &gt; time():\n                sleep(start_time - time())\n            # we can now start. 1st interval used to collect baseline\n            self.start_tracking(output_file)\n\n    def __call__(self):\n        \"\"\"Dummy method to make this class callable.\"\"\"\n        pass\n\n    def diff_stats(self):\n        \"\"\"Calculate stats since last call.\"\"\"\n        last_stats = self.stats\n        self.stats = self.get_system_stats()\n        self.cycle += 1\n\n        time_diff = self.stats[\"timestamp\"] - last_stats[\"timestamp\"]\n\n        total_read_bytes = 0\n        total_write_bytes = 0\n        for disk_name in set(self.stats[\"disk_stats\"]) &amp; set(last_stats[\"disk_stats\"]):\n            read_bytes = max(\n                0,\n                self.stats[\"disk_stats\"][disk_name][\"read_bytes\"]\n                - last_stats[\"disk_stats\"][disk_name][\"read_bytes\"],\n            )\n            write_bytes = max(\n                0,\n                self.stats[\"disk_stats\"][disk_name][\"write_bytes\"]\n                - last_stats[\"disk_stats\"][disk_name][\"write_bytes\"],\n            )\n            total_read_bytes += read_bytes\n            total_write_bytes += write_bytes\n\n        disk_space_total = 0\n        disk_space_used = 0\n        disk_space_free = 0\n        for disk_space in self.stats[\"disk_spaces\"].values():\n            disk_space_total += disk_space[\"total\"]\n            disk_space_used += disk_space[\"used\"]\n            disk_space_free += disk_space[\"free\"]\n\n        return {\n            \"timestamp\": round(self.stats[\"timestamp\"], 3),\n            \"processes\": self.stats[\"processes\"],\n            \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n            \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n            \"cpu_usage\": round(\n                max(\n                    0,\n                    (\n                        (self.stats[\"utime\"] + self.stats[\"stime\"])\n                        - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                    )\n                    / time_diff,\n                ),\n                4,\n            ),\n            \"memory_free\": self.stats[\"memory_free\"],\n            \"memory_used\": self.stats[\"memory_used\"],\n            \"memory_buffers\": self.stats[\"memory_buffers\"],\n            \"memory_cached\": self.stats[\"memory_cached\"],\n            \"memory_active\": self.stats[\"memory_active\"],\n            \"memory_inactive\": self.stats[\"memory_inactive\"],\n            \"disk_read_bytes\": total_read_bytes,\n            \"disk_write_bytes\": total_write_bytes,\n            \"disk_space_total_gb\": round(disk_space_total / (1024**3), 2),\n            \"disk_space_used_gb\": round(disk_space_used / (1024**3), 2),\n            \"disk_space_free_gb\": round(disk_space_free / (1024**3), 2),\n            \"net_recv_bytes\": max(\n                0, self.stats[\"net_recv_bytes\"] - last_stats[\"net_recv_bytes\"]\n            ),\n            \"net_sent_bytes\": max(\n                0, self.stats[\"net_sent_bytes\"] - last_stats[\"net_sent_bytes\"]\n            ),\n            \"gpu_usage\": self.stats[\"gpu_usage\"],\n            \"gpu_vram\": self.stats[\"gpu_vram\"],\n            \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n        }\n\n    def start_tracking(\n        self, output_file: Optional[str] = None, print_header: bool = True\n    ):\n        \"\"\"Start an infinite loop tracking system resource usage.\n\n        A CSV line is written every `interval` seconds.\n\n        Args:\n            output_file: File to write the output to. Defaults to None, printing to stdout.\n            print_header: Whether to print the header of the CSV. Defaults to True.\n        \"\"\"\n        file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n        try:\n            while True:\n                current_stats = self.diff_stats()\n                # don't print values yet, we collect data for the 1st baseline\n                if self.cycle == 1:\n                    if print_header:\n                        file_handle.write(\n                            render_csv_row(\n                                current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                            )\n                        )\n                else:\n                    file_handle.write(\n                        render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                    )\n                if output_file:\n                    file_handle.flush()\n                # sleep until the next interval\n                sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n        finally:\n            if output_file and not file_handle.closed:\n                file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.SystemTracker.__call__","title":"__call__","text":"<pre><code>__call__()\n</code></pre> <p>Dummy method to make this class callable.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def __call__(self):\n    \"\"\"Dummy method to make this class callable.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.SystemTracker.diff_stats","title":"diff_stats","text":"<pre><code>diff_stats()\n</code></pre> <p>Calculate stats since last call.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def diff_stats(self):\n    \"\"\"Calculate stats since last call.\"\"\"\n    last_stats = self.stats\n    self.stats = self.get_system_stats()\n    self.cycle += 1\n\n    time_diff = self.stats[\"timestamp\"] - last_stats[\"timestamp\"]\n\n    total_read_bytes = 0\n    total_write_bytes = 0\n    for disk_name in set(self.stats[\"disk_stats\"]) &amp; set(last_stats[\"disk_stats\"]):\n        read_bytes = max(\n            0,\n            self.stats[\"disk_stats\"][disk_name][\"read_bytes\"]\n            - last_stats[\"disk_stats\"][disk_name][\"read_bytes\"],\n        )\n        write_bytes = max(\n            0,\n            self.stats[\"disk_stats\"][disk_name][\"write_bytes\"]\n            - last_stats[\"disk_stats\"][disk_name][\"write_bytes\"],\n        )\n        total_read_bytes += read_bytes\n        total_write_bytes += write_bytes\n\n    disk_space_total = 0\n    disk_space_used = 0\n    disk_space_free = 0\n    for disk_space in self.stats[\"disk_spaces\"].values():\n        disk_space_total += disk_space[\"total\"]\n        disk_space_used += disk_space[\"used\"]\n        disk_space_free += disk_space[\"free\"]\n\n    return {\n        \"timestamp\": round(self.stats[\"timestamp\"], 3),\n        \"processes\": self.stats[\"processes\"],\n        \"utime\": max(0, round(self.stats[\"utime\"] - last_stats[\"utime\"], 6)),\n        \"stime\": max(0, round(self.stats[\"stime\"] - last_stats[\"stime\"], 6)),\n        \"cpu_usage\": round(\n            max(\n                0,\n                (\n                    (self.stats[\"utime\"] + self.stats[\"stime\"])\n                    - (last_stats[\"utime\"] + last_stats[\"stime\"])\n                )\n                / time_diff,\n            ),\n            4,\n        ),\n        \"memory_free\": self.stats[\"memory_free\"],\n        \"memory_used\": self.stats[\"memory_used\"],\n        \"memory_buffers\": self.stats[\"memory_buffers\"],\n        \"memory_cached\": self.stats[\"memory_cached\"],\n        \"memory_active\": self.stats[\"memory_active\"],\n        \"memory_inactive\": self.stats[\"memory_inactive\"],\n        \"disk_read_bytes\": total_read_bytes,\n        \"disk_write_bytes\": total_write_bytes,\n        \"disk_space_total_gb\": round(disk_space_total / (1024**3), 2),\n        \"disk_space_used_gb\": round(disk_space_used / (1024**3), 2),\n        \"disk_space_free_gb\": round(disk_space_free / (1024**3), 2),\n        \"net_recv_bytes\": max(\n            0, self.stats[\"net_recv_bytes\"] - last_stats[\"net_recv_bytes\"]\n        ),\n        \"net_sent_bytes\": max(\n            0, self.stats[\"net_sent_bytes\"] - last_stats[\"net_sent_bytes\"]\n        ),\n        \"gpu_usage\": self.stats[\"gpu_usage\"],\n        \"gpu_vram\": self.stats[\"gpu_vram\"],\n        \"gpu_utilized\": self.stats[\"gpu_utilized\"],\n    }\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.SystemTracker.start_tracking","title":"start_tracking","text":"<pre><code>start_tracking(output_file=None, print_header=True)\n</code></pre> <p>Start an infinite loop tracking system resource usage.</p> <p>A CSV line is written every <code>interval</code> seconds.</p> <p>Parameters:</p> Name Type Description Default <code>output_file</code> <code>Optional[str]</code> <p>File to write the output to. Defaults to None, printing to stdout.</p> <code>None</code> <code>print_header</code> <code>bool</code> <p>Whether to print the header of the CSV. Defaults to True.</p> <code>True</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start_tracking(\n    self, output_file: Optional[str] = None, print_header: bool = True\n):\n    \"\"\"Start an infinite loop tracking system resource usage.\n\n    A CSV line is written every `interval` seconds.\n\n    Args:\n        output_file: File to write the output to. Defaults to None, printing to stdout.\n        print_header: Whether to print the header of the CSV. Defaults to True.\n    \"\"\"\n    file_handle = open(output_file, \"wb\") if output_file else stdout.buffer\n    try:\n        while True:\n            current_stats = self.diff_stats()\n            # don't print values yet, we collect data for the 1st baseline\n            if self.cycle == 1:\n                if print_header:\n                    file_handle.write(\n                        render_csv_row(\n                            current_stats.keys(), quoting=QUOTE_NONNUMERIC\n                        )\n                    )\n            else:\n                file_handle.write(\n                    render_csv_row(current_stats.values(), quoting=QUOTE_NONNUMERIC)\n                )\n            if output_file:\n                file_handle.flush()\n            # sleep until the next interval\n            sleep(max(0, self.start_time + self.interval * self.cycle - time()))\n    finally:\n        if output_file and not file_handle.closed:\n            file_handle.close()\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker","title":"ResourceTracker","text":"<p>Track resource usage of processes and the system in a non-blocking way.</p> <p>Start a resource_tracker.ProcessTracker and/or a resource_tracker.SystemTracker in the background as spawned or forked process(es), and make the collected data available easily in the main process via the <code>process_metrics</code> and <code>system_metrics</code> properties.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>Process ID to track. Defaults to current process ID.</p> <code>getpid()</code> <code>children</code> <code>bool</code> <p>Whether to track child processes. Defaults to True.</p> <code>True</code> <code>interval</code> <code>float</code> <p>Sampling interval in seconds. Defaults to 1.</p> <code>1</code> <code>method</code> <code>Optional[str]</code> <p>Multiprocessing method. Defaults to None, which tries to fork on Linux and macOS, and spawn on Windows.</p> <code>None</code> <code>autostart</code> <code>bool</code> <p>Whether to start tracking immediately. Defaults to True.</p> <code>True</code> <code>track_processes</code> <code>bool</code> <p>Whether to track resource usage at the process level. Defaults to True.</p> <code>True</code> <code>track_system</code> <code>bool</code> <p>Whether to track system-wide resource usage. Defaults to True.</p> <code>True</code> <code>discover_server</code> <code>bool</code> <p>Whether to discover the server specs in the background at startup. Defaults to True.</p> <code>True</code> <code>discover_cloud</code> <code>bool</code> <p>Whether to discover the cloud environment in the background at startup. Defaults to True.</p> <code>True</code> <p>Example:</p> <pre><code>&gt;&gt;&gt; from resource_tracker.dummy_workloads import cpu_single, cpu_multi\n&gt;&gt;&gt; tracker = ResourceTracker()\n&gt;&gt;&gt; cpu_single()\n&gt;&gt;&gt; tracker.recommend_resources()  # doctest: +SKIP\n{'cpu': 1, 'memory': 128, 'gpu': 0, 'vram': 0}\n&gt;&gt;&gt; tracker = ResourceTracker()\n&gt;&gt;&gt; while tracker.n_samples == 0:\n...     cpu_multi(duration=0.25, ncores=2)\n&gt;&gt;&gt; tracker.recommend_resources()  # multiprocessing is not enough efficient on Windows/macOS  # doctest: +SKIP\n{'cpu': 2, 'memory': 128, 'gpu': 0, 'vram': 0}\n</code></pre> <p>Methods:</p> Name Description <code>start</code> <p>Start the selected resource trackers in the background as subprocess(es).</p> <code>cleanup</code> <p>Cleanup temp files and background processes.</p> <code>stop</code> <p>Stop the previously started resource trackers' background processes.</p> <code>snapshot</code> <p>Collect the current state of the resource tracker.</p> <code>from_snapshot</code> <p>Create a ResourceTracker from a snapshot.</p> <code>dumps</code> <p>Serialize the resource tracker to a JSON string.</p> <code>loads</code> <p>Deserialize the resource tracker from a JSON string.</p> <code>dump</code> <p>Serialize the resource tracker to a gzipped JSON file.</p> <code>load</code> <p>Deserialize the resource tracker from a gzipped JSON file.</p> <code>get_combined_metrics</code> <p>Collected data both from the resource_tracker.ProcessTracker and resource_tracker.SystemTracker.</p> <code>stats</code> <p>Collect statistics from the resource tracker.</p> <code>wait_for_samples</code> <p>Wait for at least one sample to be collected.</p> <code>recommend_resources</code> <p>Recommend optimal resource allocation based on the measured resource tracker data.</p> <code>recommend_server</code> <p>Recommend the cheapest cloud server matching the recommended resources.</p> <p>Attributes:</p> Name Type Description <code>n_samples</code> <code>int</code> <p>Number of samples collected by the resource tracker.</p> <code>server_info</code> <code>dict</code> <p>High-level server info.</p> <code>cloud_info</code> <code>dict</code> <p>High-level cloud info.</p> <code>process_metrics</code> <code>TinyDataFrame</code> <p>Collected data from resource_tracker.ProcessTracker.</p> <code>system_metrics</code> <code>TinyDataFrame</code> <p>Collected data from resource_tracker.SystemTracker.</p> <code>running</code> <code>bool</code> <p>Check if the resource tracker is running.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>class ResourceTracker:\n    \"\"\"Track resource usage of processes and the system in a non-blocking way.\n\n    Start a [resource_tracker.ProcessTracker][] and/or a [resource_tracker.SystemTracker][] in the background as spawned\n    or forked process(es), and make the collected data available easily in the\n    main process via the `process_metrics` and `system_metrics` properties.\n\n    Args:\n        pid: Process ID to track. Defaults to current process ID.\n        children: Whether to track child processes. Defaults to True.\n        interval: Sampling interval in seconds. Defaults to 1.\n        method: Multiprocessing method. Defaults to None, which tries to fork on\n            Linux and macOS, and spawn on Windows.\n        autostart: Whether to start tracking immediately. Defaults to True.\n        track_processes: Whether to track resource usage at the process level.\n            Defaults to True.\n        track_system: Whether to track system-wide resource usage. Defaults to True.\n        discover_server: Whether to discover the server specs in the background at\n            startup. Defaults to True.\n        discover_cloud: Whether to discover the cloud environment in the background\n            at startup. Defaults to True.\n\n    Example:\n\n        &gt;&gt;&gt; from resource_tracker.dummy_workloads import cpu_single, cpu_multi\n        &gt;&gt;&gt; tracker = ResourceTracker()\n        &gt;&gt;&gt; cpu_single()\n        &gt;&gt;&gt; tracker.recommend_resources()  # doctest: +SKIP\n        {'cpu': 1, 'memory': 128, 'gpu': 0, 'vram': 0}\n        &gt;&gt;&gt; tracker = ResourceTracker()\n        &gt;&gt;&gt; while tracker.n_samples == 0:\n        ...     cpu_multi(duration=0.25, ncores=2)\n        &gt;&gt;&gt; tracker.recommend_resources()  # multiprocessing is not enough efficient on Windows/macOS  # doctest: +SKIP\n        {'cpu': 2, 'memory': 128, 'gpu': 0, 'vram': 0}\n    \"\"\"\n\n    _server_info: Optional[dict] = None\n    _cloud_info: Optional[dict] = None\n\n    def __init__(\n        self,\n        pid: int = getpid(),\n        children: bool = True,\n        interval: float = 1,\n        method: Optional[str] = None,\n        autostart: bool = True,\n        track_processes: bool = True,\n        track_system: bool = True,\n        discover_server: bool = True,\n        discover_cloud: bool = True,\n    ):\n        self.pid = pid\n        self.children = children\n        self.interval = interval\n        self.method = method\n        self.autostart = autostart\n        self.trackers = []\n        if track_processes:\n            self.trackers.append(\"process_tracker\")\n        if track_system:\n            self.trackers.append(\"system_tracker\")\n        self.discover_server = discover_server\n        self.discover_cloud = discover_cloud\n\n        if platform != \"linux\" and not is_psutil_available():\n            raise ImportError(\n                \"psutil is required for resource tracking on non-Linux platforms\"\n            )\n\n        if method is None:\n            # try to fork when possible due to leaked semaphores on older Python versions\n            # see e.g. https://github.com/python/cpython/issues/90549\n            if platform in [\"linux\", \"darwin\"]:\n                self.mpc = get_context(\"fork\")\n            else:\n                self.mpc = get_context(\"spawn\")\n        else:\n            self.mpc = get_context(method)\n\n        # error details from subprocesses\n        self.error_queue = self.mpc.SimpleQueue()\n\n        # create temporary CSV file(s) for the tracker(s), and record only the file path(s)\n        # to be passed later to subprocess(es) avoiding pickling the file object(s)\n        for tracker_name in self.trackers:\n            temp_file = NamedTemporaryFile(delete=False)\n            setattr(self, f\"{tracker_name}_filepath\", temp_file.name)\n            temp_file.close()\n        # make sure to cleanup the temp file(s)\n        finalize(\n            self,\n            cleanup_files,\n            [\n                getattr(self, f\"{tracker_name}_filepath\")\n                for tracker_name in self.trackers\n            ],\n        )\n\n        if autostart:\n            self.start()\n\n    def start(self):\n        \"\"\"Start the selected resource trackers in the background as subprocess(es).\"\"\"\n        if self.running:\n            raise RuntimeError(\"Resource tracker already running, cannot start again.\")\n        if hasattr(self, \"stop_time\"):\n            raise RuntimeError(\n                \"Resource tracker already stopped. Create a new instance instead of trying to restart it.\"\n            )\n\n        self.start_time = time()\n        self.stop_time = None\n        # round to the nearest interval in the future\n        self.start_time = ceil(self.start_time / self.interval) * self.interval\n        # leave at least 50 ms for trackers to start\n        if self.start_time - time() &lt; 0.05:\n            self.start_time += self.interval\n\n        if \"process_tracker\" in self.trackers:\n            self.process_tracker_process = self.mpc.Process(\n                target=_run_tracker,\n                args=(\"process\", self.error_queue),\n                kwargs={\n                    \"pid\": self.pid,\n                    \"start_time\": self.start_time,\n                    \"interval\": self.interval,\n                    \"children\": self.children,\n                    \"output_file\": self.process_tracker_filepath,\n                },\n                daemon=True,\n            )\n            self.process_tracker_process.start()\n\n        if \"system_tracker\" in self.trackers:\n            self.system_tracker_process = self.mpc.Process(\n                target=_run_tracker,\n                args=(\"system\", self.error_queue),\n                kwargs={\n                    \"start_time\": self.start_time,\n                    \"interval\": self.interval,\n                    \"output_file\": self.system_tracker_filepath,\n                },\n                daemon=True,\n            )\n            self.system_tracker_process.start()\n\n        def collect_server_info():\n            \"\"\"Collect server info to be run in a background thread.\"\"\"\n            try:\n                self._server_info = get_server_info()\n            except Exception as e:\n                logger.warning(f\"Error fetching server info: {e}\")\n\n        def collect_cloud_info():\n            \"\"\"Collect cloud info to be run in a background thread.\"\"\"\n            try:\n                self._cloud_info = get_cloud_info()\n            except Exception as e:\n                logger.warning(f\"Error fetching cloud info: {e}\")\n\n        if self.discover_server:\n            server_thread = Thread(target=collect_server_info, daemon=True)\n            server_thread.start()\n        if self.discover_cloud:\n            cloud_thread = Thread(target=collect_cloud_info, daemon=True)\n            cloud_thread.start()\n\n        # make sure to cleanup the started subprocess(es)\n        finalize(\n            self,\n            cleanup_processes,\n            [\n                getattr(self, f\"{tracker_name}_process\")\n                for tracker_name in self.trackers\n            ],\n        )\n\n    def cleanup(self):\n        \"\"\"Cleanup temp files and background processes.\n\n        Note that there is no need to call this method manually, as it is\n        automatically handled by the garbage collector, but in some cases it\n        might be useful to call it manually to avoid waiting for the garbage\n        collector to run.\n        \"\"\"\n        with suppress(Exception):\n            self.stop()\n        with suppress(Exception):\n            cleanup_files(\n                [\n                    getattr(self, f\"{tracker_name}_filepath\")\n                    for tracker_name in self.trackers\n                ]\n            )\n        with suppress(Exception):\n            cleanup_processes(\n                [\n                    getattr(self, f\"{tracker_name}_process\")\n                    for tracker_name in self.trackers\n                ]\n            )\n\n    def stop(self):\n        \"\"\"Stop the previously started resource trackers' background processes.\"\"\"\n        self.stop_time = time()\n        # check for errors in the subprocesses\n        if not self.error_queue.empty():\n            error_data = self.error_queue.get()\n            logger.warning(\n                \"Resource tracker subprocess failed!\\n\"\n                f\"Error type: {error_data['name']} (from module {error_data['module']})\\n\"\n                f\"Error message: {error_data['message']}\\n\"\n                f\"Original traceback:\\n{error_data['traceback']}\"\n            )\n        # terminate tracker processes\n        for tracker_name in self.trackers:\n            process_attr = f\"{tracker_name}_process\"\n            if hasattr(self, process_attr):\n                cleanup_processes([getattr(self, process_attr)])\n        self.error_queue.close()\n        logger.debug(\n            \"Resource tracker stopped after %s seconds, logging %d process-level and %d system-wide records\",\n            self.stop_time - self.start_time,\n            len(self.process_metrics),\n            len(self.system_metrics),\n        )\n\n    @property\n    def n_samples(self) -&gt; int:\n        \"\"\"Number of samples collected by the resource tracker.\"\"\"\n        return min(len(self.process_metrics), len(self.system_metrics))\n\n    @property\n    def server_info(self) -&gt; dict:\n        \"\"\"High-level server info.\n\n        Collected data from [resource_tracker.get_server_info][] plus a guess\n        for the allocation type of the server: if it's dedicated to the tracked\n        process(es) or shared with other processes. The guess is based on the\n        [resource_tracker.column_maps.SERVER_ALLOCATION_CHECKS][] checks.\n        \"\"\"\n        server_info = self._server_info\n        if server_info:\n            server_info[\"allocation\"] = None\n        if self.n_samples &gt; 0:\n            for check in SERVER_ALLOCATION_CHECKS:\n                try:\n                    system_val = mean(self.system_metrics[check[\"system_column\"]])\n                    task_val = mean(self.process_metrics[check[\"process_column\"]])\n                    if (system_val &gt; task_val * check[\"percent\"]) or (\n                        system_val &gt; task_val + check[\"absolute\"]\n                    ):\n                        server_info[\"allocation\"] = \"shared\"\n                        break\n                except Exception as e:\n                    logger.warning(\n                        f\"Error calculating server allocation based on {check['system_column']} and {check['process_column']}: {e}\"\n                    )\n                    server_info[\"allocation\"] = \"unknown\"\n                    break\n            server_info[\"allocation\"] = server_info.get(\"allocation\", \"dedicated\")\n        return server_info\n\n    @property\n    def cloud_info(self) -&gt; dict:\n        \"\"\"High-level cloud info.\n\n        Collected data from [resource_tracker.get_cloud_info][].\n        \"\"\"\n        return self._cloud_info\n\n    @property\n    def process_metrics(self) -&gt; TinyDataFrame:\n        \"\"\"Collected data from [resource_tracker.ProcessTracker][].\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the collected data or an empty list if the [resource_tracker.ProcessTracker][] is not running.\n        \"\"\"\n        try:\n            return TinyDataFrame(\n                csv_file_path=self.process_tracker_filepath,\n                retries=2,\n                retry_delay=min(0.05, self.interval / 10),\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to read process metrics: {e}\")\n            return TinyDataFrame(data=[])\n\n    @property\n    def system_metrics(self) -&gt; TinyDataFrame:\n        \"\"\"Collected data from [resource_tracker.SystemTracker][].\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the collected data or an empty list if the [resource_tracker.SystemTracker][] is not running.\n        \"\"\"\n        try:\n            return TinyDataFrame(\n                csv_file_path=self.system_tracker_filepath,\n                retries=2,\n                retry_delay=min(0.05, self.interval / 10),\n            )\n        except Exception as e:\n            logger.warning(f\"Failed to read system metrics: {e}\")\n            return TinyDataFrame(data=[])\n\n    def snapshot(self) -&gt; dict:\n        \"\"\"Collect the current state of the resource tracker.\n\n        Returns:\n            A dictionary containing the current state of the resource tracker.\n        \"\"\"\n        return {\n            \"metadata\": {\n                \"version\": 1,\n                \"resource_tracker\": {\n                    \"version\": __version__,\n                    \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n                },\n                \"pid\": self.pid,\n                \"children\": self.children,\n                \"interval\": self.interval,\n                \"method\": self.method,\n                \"autostart\": self.autostart,\n                \"track_processes\": \"process_tracker\" in self.trackers,\n                \"track_system\": \"system_tracker\" in self.trackers,\n                \"discover_server\": self.discover_server,\n                \"discover_cloud\": self.discover_cloud,\n                \"start_time\": self.start_time,\n                \"stop_time\": self.stop_time or time(),\n                \"duration\": round(\n                    (self.stop_time or time()) - self.start_time + self.interval, 2\n                ),\n            },\n            \"server_info\": self.server_info,\n            \"cloud_info\": self.cloud_info,\n            \"process_metrics\": self.process_metrics.to_dict(),\n            \"system_metrics\": self.system_metrics.to_dict(),\n        }\n\n    @classmethod\n    def from_snapshot(cls, snapshot: dict):\n        \"\"\"Create a ResourceTracker from a snapshot.\n\n        Args:\n            snapshot: A dictionary containing the current state of the resource tracker, created by [resource_tracker.ResourceTracker.snapshot][].\n        \"\"\"\n        tracker = cls(\n            pid=snapshot[\"metadata\"][\"pid\"],\n            children=snapshot[\"metadata\"][\"children\"],\n            interval=snapshot[\"metadata\"][\"interval\"],\n            method=snapshot[\"metadata\"][\"method\"],\n            autostart=False,\n            track_processes=snapshot[\"metadata\"][\"track_processes\"],\n            track_system=snapshot[\"metadata\"][\"track_system\"],\n            discover_server=snapshot[\"metadata\"][\"discover_server\"],\n            discover_cloud=snapshot[\"metadata\"][\"discover_cloud\"],\n        )\n        tracker.start_time = snapshot[\"metadata\"][\"start_time\"]\n        tracker.stop_time = snapshot[\"metadata\"][\"stop_time\"]\n        tracker._server_info = snapshot[\"server_info\"]\n        tracker._cloud_info = snapshot[\"cloud_info\"]\n        TinyDataFrame(data=snapshot[\"process_metrics\"]).to_csv(\n            tracker.process_tracker_filepath\n        )\n        TinyDataFrame(data=snapshot[\"system_metrics\"]).to_csv(\n            tracker.system_tracker_filepath\n        )\n        return tracker\n\n    def dumps(self) -&gt; str:\n        \"\"\"Serialize the resource tracker to a JSON string.\n\n        Returns:\n            A JSON string containing the current state of the resource tracker.\n        \"\"\"\n        return json_dumps(self.snapshot())\n\n    @classmethod\n    def loads(cls, s: str):\n        \"\"\"Deserialize the resource tracker from a JSON string.\n\n        Args:\n            s: The JSON string to deserialize the resource tracker from.\n        \"\"\"\n        return cls.from_snapshot(json_loads(s))\n\n    def dump(self, file: str):\n        \"\"\"Serialize the resource tracker to a gzipped JSON file.\n\n        Args:\n            file: The path to the file to write the serialized resource tracker to.\n        \"\"\"\n        with gzip_open(file, \"wb\") as f:\n            f.write(self.dumps().encode())\n\n    @classmethod\n    def load(cls, file: str):\n        \"\"\"Deserialize the resource tracker from a gzipped JSON file.\n\n        Args:\n            file: The path to the file to read the serialized resource tracker from.\n        \"\"\"\n        with gzip_open(file, \"rb\") as f:\n            return cls.loads(f.read().decode())\n\n    def get_combined_metrics(\n        self,\n        bytes: bool = False,\n        human_names: bool = False,\n        system_prefix: Optional[str] = None,\n        process_prefix: Optional[str] = None,\n    ) -&gt; TinyDataFrame:\n        \"\"\"Collected data both from the [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][].\n\n        This is effectively binding the two dataframes together by timestamp,\n        and adding a prefix to the column names to distinguish between the system and process metrics.\n\n        Args:\n            bytes: Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] (kB, MiB, or GiB).\n            human_names: Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] with prefixes.\n            system_prefix: Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of `human_names`.\n            process_prefix: Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of `human_names`.\n\n        Returns:\n            A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the combined data or an empty list if tracker(s) not running.\n        \"\"\"\n        try:\n            process_metrics = self.process_metrics\n            system_metrics = self.system_metrics\n\n            # ensure both have the same length\n            if len(process_metrics) &gt; len(system_metrics):\n                process_metrics = process_metrics[: len(system_metrics)]\n            elif len(system_metrics) &gt; len(process_metrics):\n                system_metrics = system_metrics[: len(process_metrics)]\n\n            # nothing to report on\n            if len(process_metrics) == 0:\n                return TinyDataFrame(data=[])\n\n            if bytes:\n                for col, factor in BYTE_MAPPING.items():\n                    for metrics in (system_metrics, process_metrics):\n                        if col in metrics.columns:\n                            metrics[col] = [v * factor for v in metrics[col]]\n\n            if system_prefix is None:\n                system_prefix = \"system_\" if not human_names else \"System \"\n            if process_prefix is None:\n                process_prefix = \"process_\" if not human_names else \"Process \"\n\n            # cbind the two dataframes with column name prefixes and optional human-friendly names\n            combined = system_metrics.rename(\n                columns={\n                    n: (\n                        (system_prefix if n != \"timestamp\" else \"\")\n                        + (n if not human_names else HUMAN_NAMES_MAPPING.get(n, n))\n                    )\n                    for n in system_metrics.columns\n                }\n            )\n            for col in process_metrics.columns[1:]:\n                combined[\n                    process_prefix\n                    + (col if not human_names else HUMAN_NAMES_MAPPING.get(col, col))\n                ] = process_metrics[col]\n\n            return combined\n        except Exception as e:\n            with suppress(Exception):\n                logger.warning(\n                    f\"Kept {len(process_metrics) if 'process_metrics' in locals() else 'unknown'} records of process metrics out of {len(self.process_metrics)} collected records ({self.process_metrics.columns}), \"\n                    f\"and {len(system_metrics) if 'system_metrics' in locals() else 'unknown'} records of system metrics out of {len(self.system_metrics)} collected records ({self.system_metrics.columns}), \"\n                    f\"but creating the combined metrics dataframe failed with error: {e}\"\n                )\n                logger.warning(f\"Process metrics: {self.process_metrics.to_dict()}\")\n                logger.warning(f\"System metrics: {self.system_metrics.to_dict()}\")\n            raise\n\n    def stats(\n        self,\n        specs: List[StatSpec] = [\n            StatSpec(column=\"process_cpu_usage\", agg=mean, round=2),\n            StatSpec(column=\"process_cpu_usage\", agg=max, round=2),\n            StatSpec(column=\"process_memory\", agg=mean, round=2),\n            StatSpec(column=\"process_memory\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_usage\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_usage\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_vram\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_vram\", agg=max, round=2),\n            StatSpec(column=\"process_gpu_utilized\", agg=mean, round=2),\n            StatSpec(column=\"process_gpu_utilized\", agg=max, round=2),\n            StatSpec(column=\"system_disk_space_used_gb\", agg=max, round=2),\n            StatSpec(column=\"system_net_recv_bytes\", agg=sum),\n            StatSpec(column=\"system_net_sent_bytes\", agg=sum),\n            StatSpec(\n                column=\"timestamp\", agg=lambda x: max(x) - min(x), agg_name=\"duration\"\n            ),\n        ],\n    ) -&gt; dict:\n        \"\"\"Collect statistics from the resource tracker.\n\n        Args:\n            specs: A list of [resource_tracker.tiny_data_frame.StatSpec][] objects specifying the statistics to collect.\n\n        Returns:\n            A dictionary containing the collected statistics.\n        \"\"\"\n        if self.n_samples &gt; 0:\n            stats = self.get_combined_metrics().stats(specs)\n            stats[\"timestamp\"][\"duration\"] += self.interval\n            return stats\n        else:\n            raise RuntimeError(\"No metrics collected (yet)\")\n\n    @property\n    def running(self) -&gt; bool:\n        \"\"\"Check if the resource tracker is running.\n\n        Returns:\n            True if the resource tracker is running, False if already stopped.\n        \"\"\"\n        return hasattr(self, \"stop_time\") and self.stop_time is None\n\n    def wait_for_samples(self, n: int = 1, timeout: float = 5):\n        \"\"\"Wait for at least one sample to be collected.\n\n        Args:\n            n: The minimum number of samples to collect. Defaults to 1.\n            timeout: The maximum time to wait for a sample. Defaults to 5 seconds.\n        \"\"\"\n        if self.running:\n            while self.n_samples &lt; n:\n                sleep(self.interval / 10)\n                if time() - self.start_time &gt; timeout:\n                    raise RuntimeError(\n                        f\"Timed out waiting for resource tracker to collect {n} samples\"\n                    )\n        else:\n            if self.n_samples &lt; n:\n                raise RuntimeError(\n                    f\"Resource tracker has been already stopped with {self.n_samples} sample(s), \"\n                    f\"cannot wait to collect the requested {n} sample(s).\"\n                )\n\n    def recommend_resources(self, historical_stats: List[dict] = []) -&gt; dict:\n        \"\"\"Recommend optimal resource allocation based on the measured resource tracker data.\n\n        The recommended resources are based on the following rules:\n\n        - target average CPU usage of the process(es)\n        - target maximum memory usage of the process(es) with a 20% buffer\n        - target maximum number of GPUs used by the process(es)\n        - target maximum VRAM usage of the process(es) with a 20% buffer\n\n        Args:\n            historical_stats: Optional list of historical statistics (as returned by [resource_tracker.ResourceTracker.stats][])\n                              to consider when making recommendations. These will be combined with the current stats.\n\n        Returns:\n            A dictionary containing the recommended resources (cpu, memory, gpu, vram).\n        \"\"\"\n        self.wait_for_samples(n=1, timeout=self.interval * 5)\n\n        current_stats = self.stats()\n        if historical_stats:\n            stats = aggregate_stats([current_stats] + historical_stats)\n        else:\n            stats = current_stats\n\n        rec = {}\n        # target average CPU usage\n        rec[\"cpu\"] = max(1, round(stats[\"process_cpu_usage\"][\"mean\"]))\n        # target maximum memory usage (kB-&gt;MB) with a 20% buffer\n        rec[\"memory\"] = round_memory(mb=stats[\"process_memory\"][\"max\"] * 1.2 / 1024)\n        # target maximum GPU number of GPUs used\n        rec[\"gpu\"] = (\n            max(1, round(stats[\"process_gpu_usage\"][\"max\"]))\n            if stats[\"process_gpu_usage\"][\"mean\"] &gt; 0\n            else 0\n        )\n        # target maximum VRAM usage (MiB) with a 20% buffer\n        rec[\"vram\"] = (\n            round_memory(mb=stats[\"process_gpu_vram\"][\"max\"] * 1.2)\n            if stats[\"process_gpu_vram\"][\"max\"] &gt; 0\n            else 0\n        )\n        return rec\n\n    def recommend_server(self, **kwargs) -&gt; dict:\n        \"\"\"Recommend the cheapest cloud server matching the recommended resources.\n\n        Args:\n            **kwargs: Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.\n\n        Returns:\n            A dictionary containing the recommended cloud server. Response format is described at &lt;https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get&gt;.\n        \"\"\"\n        historical_stats = kwargs.pop(\"historical_stats\", [])\n        rec = self.recommend_resources(historical_stats=historical_stats)\n        return get_recommended_cloud_servers(**rec, **kwargs, n=1)[0]\n\n    def report(\n        self,\n        integration: Literal[\"standalone\", \"Metaflow\", \"R\"] = \"standalone\",\n        historical_stats: List[dict] = [],\n        status_failed: bool = False,\n        integration_version: Optional[str] = None,\n    ) -&gt; Report:\n        self.wait_for_samples(n=1, timeout=self.interval * 5)\n        duration = (self.stop_time or time()) - self.start_time + self.interval\n\n        current_stats = self.stats()\n        if historical_stats:\n            combined_stats = aggregate_stats([current_stats] + historical_stats)\n        else:\n            combined_stats = current_stats\n\n        ctx = {\n            \"files\": _read_report_template_files(),\n            \"server_info\": self.server_info,\n            \"cloud_info\": self.cloud_info,\n            \"process_metrics\": self.process_metrics,\n            \"system_metrics\": self.system_metrics,\n            \"stats\": current_stats,\n            \"historical_stats\": historical_stats,\n            \"combined_stats\": combined_stats,\n            \"recommended_resources\": self.recommend_resources(\n                historical_stats=historical_stats\n            ),\n            \"recommended_server\": self.recommend_server(\n                historical_stats=historical_stats\n            ),\n            \"resource_tracker\": {\n                \"version\": __version__,\n                \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n                \"integration\": integration,\n                \"integration_is\": {\n                    \"standalone\": integration == \"standalone\",\n                    \"not_standalone\": integration != \"standalone\",\n                    \"metaflow\": integration == \"Metaflow\",\n                    \"r\": integration == \"R\",\n                },\n                \"duration\": duration,\n                \"start_time\": self.start_time,\n                \"stop_time\": self.stop_time,\n                \"stopped\": self.stop_time is not None,\n                \"report_time\": time(),\n            },\n            \"status_failed\": status_failed,\n            \"csv\": {},\n        }\n\n        # comma-separated values\n        joined = self.get_combined_metrics(bytes=True, human_names=True)\n        for name, columns in REPORT_CSV_MAPPING.items():\n            csv_data = joined[columns]\n            # convert to JS milliseconds\n            csv_data[\"Timestamp\"] = [t * 1000 for t in csv_data[\"Timestamp\"]]\n            ctx[\"csv\"][name] = csv_data.to_csv(quote_strings=False)\n\n        # lookup instance prices\n        rec_server_cost = ctx[\"recommended_server\"][\"min_price_ondemand\"]\n        rec_run_cost = rec_server_cost / 60 / 60 * duration\n        ctx[\"recommended_server\"][\"best_ondemand_price_duration\"] = rec_run_cost\n        if ctx[\"cloud_info\"] and ctx[\"cloud_info\"][\"instance_type\"] != \"unknown\":\n            current_server_cost = get_instance_price(\n                ctx[\"cloud_info\"][\"vendor\"],\n                ctx[\"cloud_info\"][\"region\"],\n                ctx[\"cloud_info\"][\"instance_type\"],\n            )\n            if current_server_cost:\n                current_run_cost = round(current_server_cost / 60 / 60 * duration, 6)\n                ctx[\"cloud_info\"][\"run_costs\"] = current_run_cost\n                ctx[\"recommended_server\"][\"cost_savings\"] = {\n                    \"percent\": round(\n                        (current_run_cost - rec_run_cost) / current_run_cost * 100, 2\n                    ),\n                    \"amount\": round(current_run_cost - rec_run_cost, 6),\n                }\n\n        html_template_path = path.join(\n            path.dirname(__file__), \"report_template\", \"report.html\"\n        )\n        with open(html_template_path) as f:\n            html_template = f.read()\n        html = render_template(html_template, ctx)\n        return Report(html)\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.start","title":"start","text":"<pre><code>start()\n</code></pre> <p>Start the selected resource trackers in the background as subprocess(es).</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def start(self):\n    \"\"\"Start the selected resource trackers in the background as subprocess(es).\"\"\"\n    if self.running:\n        raise RuntimeError(\"Resource tracker already running, cannot start again.\")\n    if hasattr(self, \"stop_time\"):\n        raise RuntimeError(\n            \"Resource tracker already stopped. Create a new instance instead of trying to restart it.\"\n        )\n\n    self.start_time = time()\n    self.stop_time = None\n    # round to the nearest interval in the future\n    self.start_time = ceil(self.start_time / self.interval) * self.interval\n    # leave at least 50 ms for trackers to start\n    if self.start_time - time() &lt; 0.05:\n        self.start_time += self.interval\n\n    if \"process_tracker\" in self.trackers:\n        self.process_tracker_process = self.mpc.Process(\n            target=_run_tracker,\n            args=(\"process\", self.error_queue),\n            kwargs={\n                \"pid\": self.pid,\n                \"start_time\": self.start_time,\n                \"interval\": self.interval,\n                \"children\": self.children,\n                \"output_file\": self.process_tracker_filepath,\n            },\n            daemon=True,\n        )\n        self.process_tracker_process.start()\n\n    if \"system_tracker\" in self.trackers:\n        self.system_tracker_process = self.mpc.Process(\n            target=_run_tracker,\n            args=(\"system\", self.error_queue),\n            kwargs={\n                \"start_time\": self.start_time,\n                \"interval\": self.interval,\n                \"output_file\": self.system_tracker_filepath,\n            },\n            daemon=True,\n        )\n        self.system_tracker_process.start()\n\n    def collect_server_info():\n        \"\"\"Collect server info to be run in a background thread.\"\"\"\n        try:\n            self._server_info = get_server_info()\n        except Exception as e:\n            logger.warning(f\"Error fetching server info: {e}\")\n\n    def collect_cloud_info():\n        \"\"\"Collect cloud info to be run in a background thread.\"\"\"\n        try:\n            self._cloud_info = get_cloud_info()\n        except Exception as e:\n            logger.warning(f\"Error fetching cloud info: {e}\")\n\n    if self.discover_server:\n        server_thread = Thread(target=collect_server_info, daemon=True)\n        server_thread.start()\n    if self.discover_cloud:\n        cloud_thread = Thread(target=collect_cloud_info, daemon=True)\n        cloud_thread.start()\n\n    # make sure to cleanup the started subprocess(es)\n    finalize(\n        self,\n        cleanup_processes,\n        [\n            getattr(self, f\"{tracker_name}_process\")\n            for tracker_name in self.trackers\n        ],\n    )\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.cleanup","title":"cleanup","text":"<pre><code>cleanup()\n</code></pre> <p>Cleanup temp files and background processes.</p> <p>Note that there is no need to call this method manually, as it is automatically handled by the garbage collector, but in some cases it might be useful to call it manually to avoid waiting for the garbage collector to run.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def cleanup(self):\n    \"\"\"Cleanup temp files and background processes.\n\n    Note that there is no need to call this method manually, as it is\n    automatically handled by the garbage collector, but in some cases it\n    might be useful to call it manually to avoid waiting for the garbage\n    collector to run.\n    \"\"\"\n    with suppress(Exception):\n        self.stop()\n    with suppress(Exception):\n        cleanup_files(\n            [\n                getattr(self, f\"{tracker_name}_filepath\")\n                for tracker_name in self.trackers\n            ]\n        )\n    with suppress(Exception):\n        cleanup_processes(\n            [\n                getattr(self, f\"{tracker_name}_process\")\n                for tracker_name in self.trackers\n            ]\n        )\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.stop","title":"stop","text":"<pre><code>stop()\n</code></pre> <p>Stop the previously started resource trackers' background processes.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the previously started resource trackers' background processes.\"\"\"\n    self.stop_time = time()\n    # check for errors in the subprocesses\n    if not self.error_queue.empty():\n        error_data = self.error_queue.get()\n        logger.warning(\n            \"Resource tracker subprocess failed!\\n\"\n            f\"Error type: {error_data['name']} (from module {error_data['module']})\\n\"\n            f\"Error message: {error_data['message']}\\n\"\n            f\"Original traceback:\\n{error_data['traceback']}\"\n        )\n    # terminate tracker processes\n    for tracker_name in self.trackers:\n        process_attr = f\"{tracker_name}_process\"\n        if hasattr(self, process_attr):\n            cleanup_processes([getattr(self, process_attr)])\n    self.error_queue.close()\n    logger.debug(\n        \"Resource tracker stopped after %s seconds, logging %d process-level and %d system-wide records\",\n        self.stop_time - self.start_time,\n        len(self.process_metrics),\n        len(self.system_metrics),\n    )\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.n_samples","title":"n_samples  <code>property</code>","text":"<pre><code>n_samples\n</code></pre> <p>Number of samples collected by the resource tracker.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.server_info","title":"server_info  <code>property</code>","text":"<pre><code>server_info\n</code></pre> <p>High-level server info.</p> <p>Collected data from resource_tracker.get_server_info plus a guess for the allocation type of the server: if it's dedicated to the tracked process(es) or shared with other processes. The guess is based on the [resource_tracker.column_maps.SERVER_ALLOCATION_CHECKS][] checks.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.cloud_info","title":"cloud_info  <code>property</code>","text":"<pre><code>cloud_info\n</code></pre> <p>High-level cloud info.</p> <p>Collected data from resource_tracker.get_cloud_info.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.process_metrics","title":"process_metrics  <code>property</code>","text":"<pre><code>process_metrics\n</code></pre> <p>Collected data from resource_tracker.ProcessTracker.</p> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the collected data or an empty list if the resource_tracker.ProcessTracker is not running.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.system_metrics","title":"system_metrics  <code>property</code>","text":"<pre><code>system_metrics\n</code></pre> <p>Collected data from resource_tracker.SystemTracker.</p> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the collected data or an empty list if the resource_tracker.SystemTracker is not running.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.snapshot","title":"snapshot","text":"<pre><code>snapshot()\n</code></pre> <p>Collect the current state of the resource tracker.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the current state of the resource tracker.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def snapshot(self) -&gt; dict:\n    \"\"\"Collect the current state of the resource tracker.\n\n    Returns:\n        A dictionary containing the current state of the resource tracker.\n    \"\"\"\n    return {\n        \"metadata\": {\n            \"version\": 1,\n            \"resource_tracker\": {\n                \"version\": __version__,\n                \"implementation\": \"psutil\" if is_psutil_available() else \"procfs\",\n            },\n            \"pid\": self.pid,\n            \"children\": self.children,\n            \"interval\": self.interval,\n            \"method\": self.method,\n            \"autostart\": self.autostart,\n            \"track_processes\": \"process_tracker\" in self.trackers,\n            \"track_system\": \"system_tracker\" in self.trackers,\n            \"discover_server\": self.discover_server,\n            \"discover_cloud\": self.discover_cloud,\n            \"start_time\": self.start_time,\n            \"stop_time\": self.stop_time or time(),\n            \"duration\": round(\n                (self.stop_time or time()) - self.start_time + self.interval, 2\n            ),\n        },\n        \"server_info\": self.server_info,\n        \"cloud_info\": self.cloud_info,\n        \"process_metrics\": self.process_metrics.to_dict(),\n        \"system_metrics\": self.system_metrics.to_dict(),\n    }\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.from_snapshot","title":"from_snapshot  <code>classmethod</code>","text":"<pre><code>from_snapshot(snapshot)\n</code></pre> <p>Create a ResourceTracker from a snapshot.</p> <p>Parameters:</p> Name Type Description Default <code>snapshot</code> <code>dict</code> <p>A dictionary containing the current state of the resource tracker, created by resource_tracker.ResourceTracker.snapshot.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef from_snapshot(cls, snapshot: dict):\n    \"\"\"Create a ResourceTracker from a snapshot.\n\n    Args:\n        snapshot: A dictionary containing the current state of the resource tracker, created by [resource_tracker.ResourceTracker.snapshot][].\n    \"\"\"\n    tracker = cls(\n        pid=snapshot[\"metadata\"][\"pid\"],\n        children=snapshot[\"metadata\"][\"children\"],\n        interval=snapshot[\"metadata\"][\"interval\"],\n        method=snapshot[\"metadata\"][\"method\"],\n        autostart=False,\n        track_processes=snapshot[\"metadata\"][\"track_processes\"],\n        track_system=snapshot[\"metadata\"][\"track_system\"],\n        discover_server=snapshot[\"metadata\"][\"discover_server\"],\n        discover_cloud=snapshot[\"metadata\"][\"discover_cloud\"],\n    )\n    tracker.start_time = snapshot[\"metadata\"][\"start_time\"]\n    tracker.stop_time = snapshot[\"metadata\"][\"stop_time\"]\n    tracker._server_info = snapshot[\"server_info\"]\n    tracker._cloud_info = snapshot[\"cloud_info\"]\n    TinyDataFrame(data=snapshot[\"process_metrics\"]).to_csv(\n        tracker.process_tracker_filepath\n    )\n    TinyDataFrame(data=snapshot[\"system_metrics\"]).to_csv(\n        tracker.system_tracker_filepath\n    )\n    return tracker\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.dumps","title":"dumps","text":"<pre><code>dumps()\n</code></pre> <p>Serialize the resource tracker to a JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing the current state of the resource tracker.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def dumps(self) -&gt; str:\n    \"\"\"Serialize the resource tracker to a JSON string.\n\n    Returns:\n        A JSON string containing the current state of the resource tracker.\n    \"\"\"\n    return json_dumps(self.snapshot())\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.loads","title":"loads  <code>classmethod</code>","text":"<pre><code>loads(s)\n</code></pre> <p>Deserialize the resource tracker from a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The JSON string to deserialize the resource tracker from.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef loads(cls, s: str):\n    \"\"\"Deserialize the resource tracker from a JSON string.\n\n    Args:\n        s: The JSON string to deserialize the resource tracker from.\n    \"\"\"\n    return cls.from_snapshot(json_loads(s))\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.dump","title":"dump","text":"<pre><code>dump(file)\n</code></pre> <p>Serialize the resource tracker to a gzipped JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the file to write the serialized resource tracker to.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>def dump(self, file: str):\n    \"\"\"Serialize the resource tracker to a gzipped JSON file.\n\n    Args:\n        file: The path to the file to write the serialized resource tracker to.\n    \"\"\"\n    with gzip_open(file, \"wb\") as f:\n        f.write(self.dumps().encode())\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(file)\n</code></pre> <p>Deserialize the resource tracker from a gzipped JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str</code> <p>The path to the file to read the serialized resource tracker from.</p> required Source code in <code>resource_tracker/tracker.py</code> <pre><code>@classmethod\ndef load(cls, file: str):\n    \"\"\"Deserialize the resource tracker from a gzipped JSON file.\n\n    Args:\n        file: The path to the file to read the serialized resource tracker from.\n    \"\"\"\n    with gzip_open(file, \"rb\") as f:\n        return cls.loads(f.read().decode())\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.get_combined_metrics","title":"get_combined_metrics","text":"<pre><code>get_combined_metrics(bytes=False, human_names=False, system_prefix=None, process_prefix=None)\n</code></pre> <p>Collected data both from the resource_tracker.ProcessTracker and resource_tracker.SystemTracker.</p> <p>This is effectively binding the two dataframes together by timestamp, and adding a prefix to the column names to distinguish between the system and process metrics.</p> <p>Parameters:</p> Name Type Description Default <code>bytes</code> <code>bool</code> <p>Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at resource_tracker.ProcessTracker and resource_tracker.SystemTracker (kB, MiB, or GiB).</p> <code>False</code> <code>human_names</code> <code>bool</code> <p>Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at resource_tracker.ProcessTracker and resource_tracker.SystemTracker with prefixes.</p> <code>False</code> <code>system_prefix</code> <code>Optional[str]</code> <p>Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of <code>human_names</code>.</p> <code>None</code> <code>process_prefix</code> <code>Optional[str]</code> <p>Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of <code>human_names</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>TinyDataFrame</code> <p>A resource_tracker.tiny_data_frame.TinyDataFrame object containing the combined data or an empty list if tracker(s) not running.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def get_combined_metrics(\n    self,\n    bytes: bool = False,\n    human_names: bool = False,\n    system_prefix: Optional[str] = None,\n    process_prefix: Optional[str] = None,\n) -&gt; TinyDataFrame:\n    \"\"\"Collected data both from the [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][].\n\n    This is effectively binding the two dataframes together by timestamp,\n    and adding a prefix to the column names to distinguish between the system and process metrics.\n\n    Args:\n        bytes: Whether to convert all metrics (e.g. memory, VRAM, disk usage) to bytes. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] (kB, MiB, or GiB).\n        human_names: Whether to rename the columns to use human-friendly names. Defaults to False, reporting as documented at [resource_tracker.ProcessTracker][] and [resource_tracker.SystemTracker][] with prefixes.\n        system_prefix: Prefix to add to the system-level column names. Defaults to \"system_\" or \"System \" based on the value of `human_names`.\n        process_prefix: Prefix to add to the process-level column names. Defaults to \"process_\" or \"Process \" based on the value of `human_names`.\n\n    Returns:\n        A [resource_tracker.tiny_data_frame.TinyDataFrame][] object containing the combined data or an empty list if tracker(s) not running.\n    \"\"\"\n    try:\n        process_metrics = self.process_metrics\n        system_metrics = self.system_metrics\n\n        # ensure both have the same length\n        if len(process_metrics) &gt; len(system_metrics):\n            process_metrics = process_metrics[: len(system_metrics)]\n        elif len(system_metrics) &gt; len(process_metrics):\n            system_metrics = system_metrics[: len(process_metrics)]\n\n        # nothing to report on\n        if len(process_metrics) == 0:\n            return TinyDataFrame(data=[])\n\n        if bytes:\n            for col, factor in BYTE_MAPPING.items():\n                for metrics in (system_metrics, process_metrics):\n                    if col in metrics.columns:\n                        metrics[col] = [v * factor for v in metrics[col]]\n\n        if system_prefix is None:\n            system_prefix = \"system_\" if not human_names else \"System \"\n        if process_prefix is None:\n            process_prefix = \"process_\" if not human_names else \"Process \"\n\n        # cbind the two dataframes with column name prefixes and optional human-friendly names\n        combined = system_metrics.rename(\n            columns={\n                n: (\n                    (system_prefix if n != \"timestamp\" else \"\")\n                    + (n if not human_names else HUMAN_NAMES_MAPPING.get(n, n))\n                )\n                for n in system_metrics.columns\n            }\n        )\n        for col in process_metrics.columns[1:]:\n            combined[\n                process_prefix\n                + (col if not human_names else HUMAN_NAMES_MAPPING.get(col, col))\n            ] = process_metrics[col]\n\n        return combined\n    except Exception as e:\n        with suppress(Exception):\n            logger.warning(\n                f\"Kept {len(process_metrics) if 'process_metrics' in locals() else 'unknown'} records of process metrics out of {len(self.process_metrics)} collected records ({self.process_metrics.columns}), \"\n                f\"and {len(system_metrics) if 'system_metrics' in locals() else 'unknown'} records of system metrics out of {len(self.system_metrics)} collected records ({self.system_metrics.columns}), \"\n                f\"but creating the combined metrics dataframe failed with error: {e}\"\n            )\n            logger.warning(f\"Process metrics: {self.process_metrics.to_dict()}\")\n            logger.warning(f\"System metrics: {self.system_metrics.to_dict()}\")\n        raise\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.stats","title":"stats","text":"<pre><code>stats(specs=[StatSpec(column='process_cpu_usage', agg=mean, round=2), StatSpec(column='process_cpu_usage', agg=max, round=2), StatSpec(column='process_memory', agg=mean, round=2), StatSpec(column='process_memory', agg=max, round=2), StatSpec(column='process_gpu_usage', agg=mean, round=2), StatSpec(column='process_gpu_usage', agg=max, round=2), StatSpec(column='process_gpu_vram', agg=mean, round=2), StatSpec(column='process_gpu_vram', agg=max, round=2), StatSpec(column='process_gpu_utilized', agg=mean, round=2), StatSpec(column='process_gpu_utilized', agg=max, round=2), StatSpec(column='system_disk_space_used_gb', agg=max, round=2), StatSpec(column='system_net_recv_bytes', agg=sum), StatSpec(column='system_net_sent_bytes', agg=sum), StatSpec(column='timestamp', agg=lambda x: max(x) - min(x), agg_name='duration')])\n</code></pre> <p>Collect statistics from the resource tracker.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>List[StatSpec]</code> <p>A list of resource_tracker.tiny_data_frame.StatSpec objects specifying the statistics to collect.</p> <code>[StatSpec(column='process_cpu_usage', agg=mean, round=2), StatSpec(column='process_cpu_usage', agg=max, round=2), StatSpec(column='process_memory', agg=mean, round=2), StatSpec(column='process_memory', agg=max, round=2), StatSpec(column='process_gpu_usage', agg=mean, round=2), StatSpec(column='process_gpu_usage', agg=max, round=2), StatSpec(column='process_gpu_vram', agg=mean, round=2), StatSpec(column='process_gpu_vram', agg=max, round=2), StatSpec(column='process_gpu_utilized', agg=mean, round=2), StatSpec(column='process_gpu_utilized', agg=max, round=2), StatSpec(column='system_disk_space_used_gb', agg=max, round=2), StatSpec(column='system_net_recv_bytes', agg=sum), StatSpec(column='system_net_sent_bytes', agg=sum), StatSpec(column='timestamp', agg=lambda x: max(x) - min(x), agg_name='duration')]</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the collected statistics.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def stats(\n    self,\n    specs: List[StatSpec] = [\n        StatSpec(column=\"process_cpu_usage\", agg=mean, round=2),\n        StatSpec(column=\"process_cpu_usage\", agg=max, round=2),\n        StatSpec(column=\"process_memory\", agg=mean, round=2),\n        StatSpec(column=\"process_memory\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_usage\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_usage\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_vram\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_vram\", agg=max, round=2),\n        StatSpec(column=\"process_gpu_utilized\", agg=mean, round=2),\n        StatSpec(column=\"process_gpu_utilized\", agg=max, round=2),\n        StatSpec(column=\"system_disk_space_used_gb\", agg=max, round=2),\n        StatSpec(column=\"system_net_recv_bytes\", agg=sum),\n        StatSpec(column=\"system_net_sent_bytes\", agg=sum),\n        StatSpec(\n            column=\"timestamp\", agg=lambda x: max(x) - min(x), agg_name=\"duration\"\n        ),\n    ],\n) -&gt; dict:\n    \"\"\"Collect statistics from the resource tracker.\n\n    Args:\n        specs: A list of [resource_tracker.tiny_data_frame.StatSpec][] objects specifying the statistics to collect.\n\n    Returns:\n        A dictionary containing the collected statistics.\n    \"\"\"\n    if self.n_samples &gt; 0:\n        stats = self.get_combined_metrics().stats(specs)\n        stats[\"timestamp\"][\"duration\"] += self.interval\n        return stats\n    else:\n        raise RuntimeError(\"No metrics collected (yet)\")\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.running","title":"running  <code>property</code>","text":"<pre><code>running\n</code></pre> <p>Check if the resource tracker is running.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if the resource tracker is running, False if already stopped.</p>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.wait_for_samples","title":"wait_for_samples","text":"<pre><code>wait_for_samples(n=1, timeout=5)\n</code></pre> <p>Wait for at least one sample to be collected.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The minimum number of samples to collect. Defaults to 1.</p> <code>1</code> <code>timeout</code> <code>float</code> <p>The maximum time to wait for a sample. Defaults to 5 seconds.</p> <code>5</code> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def wait_for_samples(self, n: int = 1, timeout: float = 5):\n    \"\"\"Wait for at least one sample to be collected.\n\n    Args:\n        n: The minimum number of samples to collect. Defaults to 1.\n        timeout: The maximum time to wait for a sample. Defaults to 5 seconds.\n    \"\"\"\n    if self.running:\n        while self.n_samples &lt; n:\n            sleep(self.interval / 10)\n            if time() - self.start_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out waiting for resource tracker to collect {n} samples\"\n                )\n    else:\n        if self.n_samples &lt; n:\n            raise RuntimeError(\n                f\"Resource tracker has been already stopped with {self.n_samples} sample(s), \"\n                f\"cannot wait to collect the requested {n} sample(s).\"\n            )\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.recommend_resources","title":"recommend_resources","text":"<pre><code>recommend_resources(historical_stats=[])\n</code></pre> <p>Recommend optimal resource allocation based on the measured resource tracker data.</p> <p>The recommended resources are based on the following rules:</p> <ul> <li>target average CPU usage of the process(es)</li> <li>target maximum memory usage of the process(es) with a 20% buffer</li> <li>target maximum number of GPUs used by the process(es)</li> <li>target maximum VRAM usage of the process(es) with a 20% buffer</li> </ul> <p>Parameters:</p> Name Type Description Default <code>historical_stats</code> <code>List[dict]</code> <p>Optional list of historical statistics (as returned by resource_tracker.ResourceTracker.stats)               to consider when making recommendations. These will be combined with the current stats.</p> <code>[]</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the recommended resources (cpu, memory, gpu, vram).</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def recommend_resources(self, historical_stats: List[dict] = []) -&gt; dict:\n    \"\"\"Recommend optimal resource allocation based on the measured resource tracker data.\n\n    The recommended resources are based on the following rules:\n\n    - target average CPU usage of the process(es)\n    - target maximum memory usage of the process(es) with a 20% buffer\n    - target maximum number of GPUs used by the process(es)\n    - target maximum VRAM usage of the process(es) with a 20% buffer\n\n    Args:\n        historical_stats: Optional list of historical statistics (as returned by [resource_tracker.ResourceTracker.stats][])\n                          to consider when making recommendations. These will be combined with the current stats.\n\n    Returns:\n        A dictionary containing the recommended resources (cpu, memory, gpu, vram).\n    \"\"\"\n    self.wait_for_samples(n=1, timeout=self.interval * 5)\n\n    current_stats = self.stats()\n    if historical_stats:\n        stats = aggregate_stats([current_stats] + historical_stats)\n    else:\n        stats = current_stats\n\n    rec = {}\n    # target average CPU usage\n    rec[\"cpu\"] = max(1, round(stats[\"process_cpu_usage\"][\"mean\"]))\n    # target maximum memory usage (kB-&gt;MB) with a 20% buffer\n    rec[\"memory\"] = round_memory(mb=stats[\"process_memory\"][\"max\"] * 1.2 / 1024)\n    # target maximum GPU number of GPUs used\n    rec[\"gpu\"] = (\n        max(1, round(stats[\"process_gpu_usage\"][\"max\"]))\n        if stats[\"process_gpu_usage\"][\"mean\"] &gt; 0\n        else 0\n    )\n    # target maximum VRAM usage (MiB) with a 20% buffer\n    rec[\"vram\"] = (\n        round_memory(mb=stats[\"process_gpu_vram\"][\"max\"] * 1.2)\n        if stats[\"process_gpu_vram\"][\"max\"] &gt; 0\n        else 0\n    )\n    return rec\n</code></pre>"},{"location":"reference/resource_tracker/tracker/#resource_tracker.tracker.ResourceTracker.recommend_server","title":"recommend_server","text":"<pre><code>recommend_server(**kwargs)\n</code></pre> <p>Recommend the cheapest cloud server matching the recommended resources.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the recommended cloud server. Response format is described at https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get.</p> Source code in <code>resource_tracker/tracker.py</code> <pre><code>def recommend_server(self, **kwargs) -&gt; dict:\n    \"\"\"Recommend the cheapest cloud server matching the recommended resources.\n\n    Args:\n        **kwargs: Additional filtering arguments (e.g. vendor_id or compliance_framework_id) to pass to the Spare Cores Keeper API.\n\n    Returns:\n        A dictionary containing the recommended cloud server. Response format is described at &lt;https://keeper.sparecores.net/redoc#tag/Query-Resources/operation/search_servers_servers_get&gt;.\n    \"\"\"\n    historical_stats = kwargs.pop(\"historical_stats\", [])\n    rec = self.recommend_resources(historical_stats=historical_stats)\n    return get_recommended_cloud_servers(**rec, **kwargs, n=1)[0]\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/","title":"resource_tracker.tracker_procfs","text":""},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs","title":"resource_tracker.tracker_procfs","text":"<p>Helpers to track resource usage via <code>procfs</code>.</p> <p>Note that <code>procfs</code> is specific to Linux, and these helpers rely on modern (2017+) kernel features, such as <code>smaps_rollup</code>, which is much faster than iterating over all <code>smaps</code> files.</p> <p>Functions:</p> Name Description <code>get_sector_sizes</code> <p>Get the sector size of all disks.</p> <code>get_process_children</code> <p>Get all descendant processes recursively.</p> <code>get_process_rss</code> <p>Get the current resident set size of a process.</p> <code>get_process_pss_rollup</code> <p>Reads the total PSS from <code>/proc/{pid}/smaps_rollup</code>.</p> <code>get_process_proc_times</code> <p>Get the current user and system times of a process from <code>/proc/{pid}/stat</code>.</p> <code>get_process_proc_io</code> <p>Get the total bytes read and written by a process from <code>/proc/{pid}/io</code>.</p> <code>get_process_stats</code> <p>Collect current/cumulative stats of a process from procfs.</p> <code>get_system_stats</code> <p>Collect current system-wide stats from procfs.</p>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_sector_sizes","title":"get_sector_sizes  <code>cached</code>","text":"<pre><code>get_sector_sizes()\n</code></pre> <p>Get the sector size of all disks.</p> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>A dictionary mapping disk names to their sector sizes.</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>@cache\ndef get_sector_sizes() -&gt; Dict[str, int]:\n    \"\"\"Get the sector size of all disks.\n\n    Returns:\n        A dictionary mapping disk names to their sector sizes.\n    \"\"\"\n    sector_sizes = {}\n    with suppress(FileNotFoundError):\n        for disk_path in glob(\"/sys/block/*/\"):\n            disk_name = disk_path.split(\"/\")[-2]\n            if is_partition(disk_name):\n                continue\n            try:\n                with open(f\"{disk_path}queue/hw_sector_size\", \"r\") as f:\n                    sector_sizes[disk_name] = int(f.read().strip())\n            except (FileNotFoundError, ValueError):\n                sector_sizes[disk_name] = 512\n    return sector_sizes\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_children","title":"get_process_children","text":"<pre><code>get_process_children(pid)\n</code></pre> <p>Get all descendant processes recursively.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to get descendant processes for.</p> required <p>Returns:</p> Type Description <code>Set[int]</code> <p>All descendant process ids.</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_children(pid: int) -&gt; Set[int]:\n    \"\"\"Get all descendant processes recursively.\n\n    Args:\n        pid: The process ID to get descendant processes for.\n\n    Returns:\n        All descendant process ids.\n    \"\"\"\n    try:\n        with open(f\"/proc/{pid}/task/{pid}/children\", \"r\") as f:\n            children = {int(child) for child in f.read().strip().split()}\n            descendants = set()\n            for child in children:\n                descendants.update(get_process_children(child))\n            return children | descendants\n    except (ProcessLookupError, FileNotFoundError):\n        return set()\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_rss","title":"get_process_rss","text":"<pre><code>get_process_rss(pid)\n</code></pre> <p>Get the current resident set size of a process.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to get the resident set size for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The current resident set size of the process in kB.</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_rss(pid: int) -&gt; int:\n    \"\"\"Get the current resident set size of a process.\n\n    Args:\n        pid: The process ID to get the resident set size for.\n\n    Returns:\n        The current resident set size of the process in kB.\n    \"\"\"\n    try:\n        with open(f\"/proc/{pid}/status\", \"r\") as f:\n            for line in f:\n                if line.startswith(\"VmRSS\"):\n                    return int(line.split()[1])\n    except (ProcessLookupError, FileNotFoundError):\n        return 0\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_pss_rollup","title":"get_process_pss_rollup","text":"<pre><code>get_process_pss_rollup(pid)\n</code></pre> <p>Reads the total PSS from <code>/proc/{pid}/smaps_rollup</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to get the total PSS for.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The total PSS in kB.</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_pss_rollup(pid: int) -&gt; int:\n    \"\"\"Reads the total PSS from `/proc/{pid}/smaps_rollup`.\n\n    Args:\n        pid: The process ID to get the total PSS for.\n\n    Returns:\n        The total PSS in kB.\n    \"\"\"\n    with suppress(ProcessLookupError, FileNotFoundError):\n        with open(f\"/proc/{pid}/smaps_rollup\", \"r\") as f:\n            for line in f:\n                if line.startswith(\"Pss:\"):\n                    return int(line.split()[1])\n    return 0\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_proc_times","title":"get_process_proc_times","text":"<pre><code>get_process_proc_times(pid, children=True)\n</code></pre> <p>Get the current user and system times of a process from <code>/proc/{pid}/stat</code>.</p> <p>Note that cannot use <code>cutime</code>/<code>cstime</code> for real-time monitoring, as they need to wait for the children to exit.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>Process ID to track</p> required <code>children</code> <code>bool</code> <p>Whether to include stats from exited child processes</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>A dictionary containing process time information: - <code>utime</code> (int): User mode CPU time in clock ticks - <code>stime</code> (int): System mode CPU time in clock ticks</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_proc_times(pid: int, children: bool = True) -&gt; Dict[str, int]:\n    \"\"\"Get the current user and system times of a process from `/proc/{pid}/stat`.\n\n    Note that cannot use `cutime`/`cstime` for real-time monitoring,\n    as they need to wait for the children to exit.\n\n    Args:\n        pid: Process ID to track\n        children: Whether to include stats from exited child processes\n\n    Returns:\n        A dictionary containing process time information:\n            - `utime` (int): User mode CPU time in clock ticks\n            - `stime` (int): System mode CPU time in clock ticks\n    \"\"\"\n    try:\n        with open(f\"/proc/{pid}/stat\", \"r\") as f:\n            values = f.read().split()\n            # https://docs.kernel.org/filesystems/proc.html\n            return {\n                \"utime\": int(values[13]) + (int(values[15]) if children else 0),\n                \"stime\": int(values[14]) + (int(values[16]) if children else 0),\n            }\n    except (ProcessLookupError, FileNotFoundError):\n        return {\"utime\": 0, \"stime\": 0}\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_proc_io","title":"get_process_proc_io","text":"<pre><code>get_process_proc_io(pid)\n</code></pre> <p>Get the total bytes read and written by a process from <code>/proc/{pid}/io</code>.</p> <p>Note that it is not tracking reading from memory-mapped objects, and is fairly limited in what it can track. E.g. the process might not even have permissions to read its own <code>/proc/self/io</code>.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to get the total bytes read and written for.</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>A dictionary containing the total bytes read and written by the process.</p> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_proc_io(pid: int) -&gt; Dict[str, int]:\n    \"\"\"Get the total bytes read and written by a process from `/proc/{pid}/io`.\n\n    Note that it is not tracking reading from memory-mapped objects,\n    and is fairly limited in what it can track. E.g. the process might\n    not even have permissions to read its own `/proc/self/io`.\n\n    Args:\n        pid: The process ID to get the total bytes read and written for.\n\n    Returns:\n        A dictionary containing the total bytes read and written by the process.\n    \"\"\"\n    try:\n        with open(f\"/proc/{pid}/io\", \"r\") as f:\n            return {\n                parts[0]: int(parts[1]) for line in f if (parts := line.split(\": \"))\n            }\n    except (ProcessLookupError, FileNotFoundError, PermissionError):\n        return {\"read_bytes\": 0, \"write_bytes\": 0}\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_process_stats","title":"get_process_stats","text":"<pre><code>get_process_stats(pid, children=True)\n</code></pre> <p>Collect current/cumulative stats of a process from procfs.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to track.</p> required <code>children</code> <code>bool</code> <p>Whether to include child processes.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float, None, Set[int]]]</code> <p>A dictionary containing process stats:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>pid (int): The process ID.</li> <li>children (int | None): The current number of child processes.</li> <li>utime (int): The total user mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>memory (int): The current PSS (Proportional Set Size) in kB.</li> <li>read_bytes (int): The total number of bytes read.</li> <li>write_bytes (int): The total number of bytes written.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_process_stats(\n    pid: int, children: bool = True\n) -&gt; Dict[str, Union[int, float, None, Set[int]]]:\n    \"\"\"Collect current/cumulative stats of a process from procfs.\n\n    Args:\n        pid: The process ID to track.\n        children: Whether to include child processes.\n\n    Returns:\n        A dictionary containing process stats:\n\n            - timestamp (float): The current timestamp.\n            - pid (int): The process ID.\n            - children (int | None): The current number of child processes.\n            - utime (int): The total user mode CPU time in seconds.\n            - stime (int): The total system mode CPU time in seconds.\n            - memory (int): The current PSS (Proportional Set Size) in kB.\n            - read_bytes (int): The total number of bytes read.\n            - write_bytes (int): The total number of bytes written.\n            - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n            - gpu_vram (float): The current GPU memory used in MiB.\n            - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n    \"\"\"\n    current_time = time()\n\n    nvidia_process = start_nvidia_smi_pmon()\n\n    current_children = get_process_children(pid)\n    current_pss = get_process_pss_rollup(pid)\n    if children:\n        for child in current_children:\n            current_pss += get_process_pss_rollup(child)\n    current_proc_times = get_process_proc_times(pid, children)\n    if children:\n        for child in current_children:\n            current_proc_times[\"utime\"] += get_process_proc_times(child, True)[\"utime\"]\n            current_proc_times[\"stime\"] += get_process_proc_times(child, True)[\"stime\"]\n    current_io = get_process_proc_io(pid)\n    if children:\n        for child in current_children:\n            child_io = get_process_proc_io(child)\n            for key in set(current_io) &amp; set(child_io):\n                current_io[key] += child_io[key]\n\n    gpu_stats = process_nvidia_smi_pmon(nvidia_process, {pid} | current_children)\n\n    return {\n        \"timestamp\": current_time,\n        \"pid\": pid,\n        \"children\": len(current_children) if children else None,\n        \"utime\": current_proc_times[\"utime\"] / sysconf(\"SC_CLK_TCK\"),\n        \"stime\": current_proc_times[\"stime\"] / sysconf(\"SC_CLK_TCK\"),\n        \"memory\": current_pss,\n        \"read_bytes\": current_io[\"read_bytes\"],\n        \"write_bytes\": current_io[\"write_bytes\"],\n        **gpu_stats,\n    }\n</code></pre>"},{"location":"reference/resource_tracker/tracker_procfs/#resource_tracker.tracker_procfs.get_system_stats","title":"get_system_stats","text":"<pre><code>get_system_stats()\n</code></pre> <p>Collect current system-wide stats from procfs.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float, Dict]]</code> <p>A dictionary containing system stats:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>processes (int): Number of running processes.</li> <li>utime (int): Total user mode CPU time in seconds.</li> <li>stime (int): Total system mode CPU time in seconds.</li> <li>memory_free (int): Free physical memory in kB.</li> <li>memory_used (int): Used physical memory in kB (excluding buffers/cache).</li> <li>memory_buffers (int): Memory used for buffers in kB.</li> <li>memory_cached (int): Memory used for cache in kB.</li> <li>memory_active (int): Memory used for active pages in kB.</li> <li>memory_inactive (int): Memory used for inactive pages in kB.</li> <li> <p>disk_stats (dict): Dictionary mapping disk names to their stats:</p> <ul> <li>read_bytes (int): Bytes read from this disk.</li> <li>write_bytes (int): Bytes written to this disk.</li> </ul> </li> <li> <p>disk_spaces (dict): Dictionary mapping mount points to their space stats:</p> <ul> <li>total (int): Total space in bytes.</li> <li>used (int): Used space in bytes.</li> <li>free (int): Free space in bytes.</li> </ul> </li> <li> <p>net_recv_bytes (int): Total bytes received over network.</p> </li> <li>net_sent_bytes (int): Total bytes sent over network.</li> </ul> Source code in <code>resource_tracker/tracker_procfs.py</code> <pre><code>def get_system_stats() -&gt; Dict[str, Union[int, float, Dict]]:\n    \"\"\"Collect current system-wide stats from procfs.\n\n    Returns:\n        A dictionary containing system stats:\n\n            - timestamp (float): The current timestamp.\n            - processes (int): Number of running processes.\n            - utime (int): Total user mode CPU time in seconds.\n            - stime (int): Total system mode CPU time in seconds.\n            - memory_free (int): Free physical memory in kB.\n            - memory_used (int): Used physical memory in kB (excluding buffers/cache).\n            - memory_buffers (int): Memory used for buffers in kB.\n            - memory_cached (int): Memory used for cache in kB.\n            - memory_active (int): Memory used for active pages in kB.\n            - memory_inactive (int): Memory used for inactive pages in kB.\n            - disk_stats (dict): Dictionary mapping disk names to their stats:\n\n                - read_bytes (int): Bytes read from this disk.\n                - write_bytes (int): Bytes written to this disk.\n\n            - disk_spaces (dict): Dictionary mapping mount points to their space stats:\n\n                - total (int): Total space in bytes.\n                - used (int): Used space in bytes.\n                - free (int): Free space in bytes.\n\n            - net_recv_bytes (int): Total bytes received over network.\n            - net_sent_bytes (int): Total bytes sent over network.\n    \"\"\"\n    stats = {\n        \"timestamp\": time(),\n        \"processes\": 0,\n        \"utime\": 0,\n        \"stime\": 0,\n        \"memory_free\": 0,\n        \"memory_used\": 0,\n        \"memory_buffers\": 0,\n        \"memory_cached\": 0,\n        \"memory_active\": 0,\n        \"memory_inactive\": 0,\n        \"disk_stats\": {},\n        \"disk_spaces\": {},\n        \"net_recv_bytes\": 0,\n        \"net_sent_bytes\": 0,\n    }\n\n    nvidia_process = start_nvidia_smi()\n\n    with suppress(FileNotFoundError):\n        with open(\"/proc/stat\", \"r\") as f:\n            for line in f:\n                if line.startswith(\"cpu \"):\n                    cpu_stats = line.split()\n                    tps = sysconf(\"SC_CLK_TCK\")\n                    # user + nice\n                    stats[\"utime\"] = (int(cpu_stats[1]) + int(cpu_stats[2])) / tps\n                    stats[\"stime\"] = int(cpu_stats[3]) / tps\n        stats[\"processes\"] = len([x for x in listdir(\"/proc\") if x.isdigit()])\n\n    # memory stats reported in kB\n    with suppress(FileNotFoundError):\n        with open(\"/proc/meminfo\", \"r\") as f:\n            mem_info = {}\n            for line in f:\n                parts = line.split(\":\")\n                if len(parts) == 2:\n                    key = parts[0].strip()\n                    value_parts = parts[1].strip().split()\n                    if len(value_parts) &gt; 0:\n                        try:\n                            mem_info[key] = int(value_parts[0])\n                        except ValueError:\n                            pass\n\n            total = mem_info.get(\"MemTotal\", 0)\n            stats[\"memory_free\"] = mem_info.get(\"MemFree\", 0)\n            stats[\"memory_buffers\"] = mem_info.get(\"Buffers\", 0)\n            stats[\"memory_cached\"] = mem_info.get(\"Cached\", 0)\n            stats[\"memory_cached\"] += mem_info.get(\"SReclaimable\", 0)\n            stats[\"memory_used\"] = (\n                total\n                - stats[\"memory_free\"]\n                - stats[\"memory_buffers\"]\n                - stats[\"memory_cached\"]\n            )\n            stats[\"memory_active\"] = mem_info.get(\"Active\", 0)\n            stats[\"memory_inactive\"] = mem_info.get(\"Inactive\", 0)\n\n    with suppress(FileNotFoundError):\n        with open(\"/proc/diskstats\", \"r\") as f:\n            for line in f:\n                parts = line.split()\n                if len(parts) &gt;= 14:\n                    disk_name = parts[2]\n                    if not is_partition(disk_name):\n                        sector_size = get_sector_sizes().get(disk_name, 512)\n                        stats[\"disk_stats\"][disk_name] = {\n                            \"read_bytes\": int(parts[5]) * sector_size,\n                            \"write_bytes\": int(parts[9]) * sector_size,\n                        }\n\n    with suppress(FileNotFoundError):\n        with open(\"/proc/net/dev\", \"r\") as f:\n            # skip header lines\n            next(f)\n            next(f)\n            for line in f:\n                parts = line.split(\":\")\n                if len(parts) == 2:\n                    interface = parts[0].strip()\n                    if interface != \"lo\":\n                        values = parts[1].strip().split()\n                        stats[\"net_recv_bytes\"] += int(values[0])\n                        stats[\"net_sent_bytes\"] += int(values[8])\n\n    check_zfs = False\n    with suppress(FileNotFoundError):\n        with open(\"/proc/mounts\", \"r\") as f:\n            for line in f:\n                parts = line.split()\n                if len(parts) &gt;= 2:\n                    mount_point = parts[1]\n                    filesystem = parts[2]\n                    # skip known virtual filesystems\n                    if mount_point.startswith((\"/proc\", \"/sys\", \"/dev\", \"/run\")):\n                        continue\n                    # skip zfs, will count later due to overlapping partitions\n                    if filesystem == \"zfs\":\n                        check_zfs = True\n                        continue\n                    try:\n                        fs_stats = statvfs(mount_point)\n                        # skip pseudo filesystems\n                        if fs_stats.f_blocks == 0:\n                            continue\n                        block_size = fs_stats.f_frsize\n                        total_space = fs_stats.f_blocks * block_size\n                        free_space = fs_stats.f_bavail * block_size\n                        used_space = total_space - free_space\n                        stats[\"disk_spaces\"][mount_point] = {\n                            \"total\": total_space,\n                            \"used\": used_space,\n                            \"free\": free_space,\n                        }\n                    except (OSError, PermissionError):\n                        pass\n    if check_zfs:\n        stats[\"disk_spaces\"].update(get_zfs_pools_space())\n\n    stats.update(process_nvidia_smi(nvidia_process))\n\n    return stats\n</code></pre>"},{"location":"reference/resource_tracker/tracker_psutil/","title":"resource_tracker.tracker_psutil","text":""},{"location":"reference/resource_tracker/tracker_psutil/#resource_tracker.tracker_psutil","title":"resource_tracker.tracker_psutil","text":"<p>Helpers to track resource usage via <code>psutil</code>.</p> <p>Functions:</p> Name Description <code>get_process_stats</code> <p>Collect current/cumulative stats of a process via psutil.</p> <code>get_system_stats</code> <p>Collect current system-wide stats via psutil.</p>"},{"location":"reference/resource_tracker/tracker_psutil/#resource_tracker.tracker_psutil.get_process_stats","title":"get_process_stats","text":"<pre><code>get_process_stats(pid, children=True)\n</code></pre> <p>Collect current/cumulative stats of a process via psutil.</p> <p>Parameters:</p> Name Type Description Default <code>pid</code> <code>int</code> <p>The process ID to track.</p> required <code>children</code> <code>bool</code> <p>Whether to include child processes.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float, None, Set[int]]]</code> <p>A dictionary containing process stats:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>pid (int): The process ID.</li> <li>children (int | None): The current number of child processes.</li> <li>utime (int): The total user mode CPU time in seconds.</li> <li>stime (int): The total system mode CPU time in seconds.</li> <li>memory (int): The current PSS (Proportional Set Size) on Linux,   USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on   other OSs where neither PSS nor USS are available in kB. See more details at   https://gmpy.dev/blog/2016/real-process-memory-and-environ-in-python.</li> <li>read_bytes (int): The total number of bytes read.</li> <li>write_bytes (int): The total number of bytes written.</li> <li>gpu_usage (float): The current GPU utilization between 0 and GPU count.</li> <li>gpu_vram (float): The current GPU memory used in MiB.</li> <li>gpu_utilized (int): The number of GPUs with utilization &gt; 0.</li> </ul> Source code in <code>resource_tracker/tracker_psutil.py</code> <pre><code>def get_process_stats(\n    pid: int, children: bool = True\n) -&gt; Dict[str, Union[int, float, None, Set[int]]]:\n    \"\"\"Collect current/cumulative stats of a process via psutil.\n\n    Args:\n        pid: The process ID to track.\n        children: Whether to include child processes.\n\n    Returns:\n        A dictionary containing process stats:\n\n            - timestamp (float): The current timestamp.\n            - pid (int): The process ID.\n            - children (int | None): The current number of child processes.\n            - utime (int): The total user mode CPU time in seconds.\n            - stime (int): The total system mode CPU time in seconds.\n            - memory (int): The current PSS (Proportional Set Size) on Linux,\n              USS (Unique Set Size) on macOS and Windows, and RSS (Resident Set Size) on\n              other OSs where neither PSS nor USS are available in kB. See more details at\n              &lt;https://gmpy.dev/blog/2016/real-process-memory-and-environ-in-python&gt;.\n            - read_bytes (int): The total number of bytes read.\n            - write_bytes (int): The total number of bytes written.\n            - gpu_usage (float): The current GPU utilization between 0 and GPU count.\n            - gpu_vram (float): The current GPU memory used in MiB.\n            - gpu_utilized (int): The number of GPUs with utilization &gt; 0.\n    \"\"\"\n    current_time = time()\n    nvidia_process = start_nvidia_smi_pmon()\n\n    processes = [Process(pid)]\n    if children:\n        current_children = processes[0].children(recursive=True)\n        processes = processes + list(current_children)\n\n    stats = {\n        \"timestamp\": current_time,\n        \"pid\": pid,\n        \"children\": len(current_children) if children else None,\n        \"utime\": 0,\n        \"stime\": 0,\n        \"memory\": 0,\n        \"read_bytes\": 0,\n        \"write_bytes\": 0,\n    }\n\n    for process in processes:\n        # process might have been terminated, so silently skip if not found\n        with suppress(Exception):\n            cpu_times = process.cpu_times()\n            stats[\"utime\"] += cpu_times.user + cpu_times.children_user\n            stats[\"stime\"] += cpu_times.system + cpu_times.children_system\n            memory_info = process.memory_full_info()\n            for attr in (\"pss\", \"uss\", \"rss\"):\n                if (\n                    hasattr(memory_info, attr)\n                    and getattr(memory_info, attr) is not None\n                    and getattr(memory_info, attr) != 0\n                ):\n                    stats[\"memory\"] += getattr(memory_info, attr) / 1024  # kB\n                    break\n            io_counters = process.io_counters()\n            stats[\"read_bytes\"] += io_counters.read_bytes\n            stats[\"write_bytes\"] += io_counters.write_bytes\n\n    stats.update(process_nvidia_smi_pmon(nvidia_process, [p.pid for p in processes]))\n\n    return stats\n</code></pre>"},{"location":"reference/resource_tracker/tracker_psutil/#resource_tracker.tracker_psutil.get_system_stats","title":"get_system_stats","text":"<pre><code>get_system_stats()\n</code></pre> <p>Collect current system-wide stats via psutil.</p> <p>Note that some fields are not available on all platforms, e.g. memory buffers/cache are specific to Linux and BSD, and active/inactive anonymous pages are specific to Linux, so <code>0</code> is returned for these fields on other platforms.</p> <p>Returns:</p> Type Description <code>Dict[str, Union[int, float, Dict]]</code> <p>A dictionary containing system stats:</p> <ul> <li>timestamp (float): The current timestamp.</li> <li>processes (int): Number of running processes.</li> <li>utime (int): Total user mode CPU time in seconds.</li> <li>stime (int): Total system mode CPU time in seconds.</li> <li>memory_free (int): Free physical memory in kB.</li> <li>memory_used (int): Used physical memory in kB (excluding buffers/cache).</li> <li>memory_buffers (int): Memory used for buffers in kB.</li> <li>memory_cached (int): Memory used for cache in kB.</li> <li>memory_active (int): Memory used for active pages in kB.</li> <li>memory_inactive (int): Memory used for inactive pages in kB.</li> <li> <p>disk_stats (dict): Dictionary mapping disk names to their stats:</p> <ul> <li>read_bytes (int): Bytes read from this disk.</li> <li>write_bytes (int): Bytes written to this disk.</li> </ul> </li> <li> <p>disk_spaces (dict): Dictionary mapping mount points to their space stats:</p> <ul> <li>total (int): Total space in bytes.</li> <li>used (int): Used space in bytes.</li> <li>free (int): Free space in bytes.</li> </ul> </li> <li> <p>net_recv_bytes (int): Total bytes received over network.</p> </li> <li>net_sent_bytes (int): Total bytes sent over network.</li> </ul> Source code in <code>resource_tracker/tracker_psutil.py</code> <pre><code>def get_system_stats() -&gt; Dict[str, Union[int, float, Dict]]:\n    \"\"\"Collect current system-wide stats via psutil.\n\n    Note that some fields are not available on all platforms,\n    e.g. memory buffers/cache are specific to Linux and BSD,\n    and active/inactive anonymous pages are specific to Linux,\n    so `0` is returned for these fields on other platforms.\n\n    Returns:\n        A dictionary containing system stats:\n\n            - timestamp (float): The current timestamp.\n            - processes (int): Number of running processes.\n            - utime (int): Total user mode CPU time in seconds.\n            - stime (int): Total system mode CPU time in seconds.\n            - memory_free (int): Free physical memory in kB.\n            - memory_used (int): Used physical memory in kB (excluding buffers/cache).\n            - memory_buffers (int): Memory used for buffers in kB.\n            - memory_cached (int): Memory used for cache in kB.\n            - memory_active (int): Memory used for active pages in kB.\n            - memory_inactive (int): Memory used for inactive pages in kB.\n            - disk_stats (dict): Dictionary mapping disk names to their stats:\n\n                - read_bytes (int): Bytes read from this disk.\n                - write_bytes (int): Bytes written to this disk.\n\n            - disk_spaces (dict): Dictionary mapping mount points to their space stats:\n\n                - total (int): Total space in bytes.\n                - used (int): Used space in bytes.\n                - free (int): Free space in bytes.\n\n            - net_recv_bytes (int): Total bytes received over network.\n            - net_sent_bytes (int): Total bytes sent over network.\n    \"\"\"\n    stats = {\n        \"timestamp\": time(),\n        \"processes\": 0,\n        \"utime\": 0,\n        \"stime\": 0,\n        \"memory_free\": 0,\n        \"memory_used\": 0,\n        \"memory_buffers\": 0,\n        \"memory_cached\": 0,\n        \"memory_active\": 0,\n        \"memory_inactive\": 0,\n        \"disk_stats\": {},\n        \"disk_spaces\": {},\n        \"net_recv_bytes\": 0,\n        \"net_sent_bytes\": 0,\n    }\n\n    nvidia_process = start_nvidia_smi()\n\n    cpu = cpu_times()\n    stats[\"utime\"] = cpu.user\n    if hasattr(cpu, \"nice\"):\n        stats[\"utime\"] += cpu.nice\n    stats[\"stime\"] = cpu.system\n    stats[\"processes\"] = len(pids())\n\n    # store memory stats in kB\n    memory = virtual_memory()\n    stats[\"memory_free\"] = memory.free / 1024\n    if hasattr(memory, \"buffers\"):\n        stats[\"memory_buffers\"] = memory.buffers / 1024\n    if hasattr(memory, \"cached\"):\n        stats[\"memory_cached\"] = memory.cached / 1024\n    if hasattr(memory, \"active\"):\n        stats[\"memory_active\"] = memory.active / 1024\n    if hasattr(memory, \"inactive\"):\n        stats[\"memory_inactive\"] = memory.inactive / 1024\n    stats[\"memory_used\"] = memory.used / 1024\n\n    disk_io = disk_io_counters(perdisk=True)\n    stats[\"disk_stats\"] = {\n        disk_name: {\n            \"read_bytes\": disk_io[disk_name].read_bytes,\n            \"write_bytes\": disk_io[disk_name].write_bytes,\n        }\n        for disk_name in disk_io\n        if not is_partition(disk_name)\n    }\n\n    net_io = net_io_counters()\n    stats[\"net_recv_bytes\"] = net_io.bytes_recv\n    stats[\"net_sent_bytes\"] = net_io.bytes_sent\n\n    check_zfs = False\n    disks = disk_partitions()\n    for disk in disks:\n        if disk.fstype == \"zfs\":\n            check_zfs = True\n            continue\n        usage = disk_usage(disk.mountpoint)\n        stats[\"disk_spaces\"][disk.mountpoint] = {\n            \"total\": usage.total,\n            \"used\": usage.used,\n            \"free\": usage.free,\n        }\n    if check_zfs:\n        stats[\"disk_spaces\"].update(get_zfs_pools_space())\n\n    stats.update(process_nvidia_smi(nvidia_process))\n\n    return stats\n</code></pre>"}]}